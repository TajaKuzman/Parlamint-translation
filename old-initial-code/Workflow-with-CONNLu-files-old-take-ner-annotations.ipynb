{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Workflow:\n",
    "1. Extract information from the CONLL-U\n",
    "2. Translate\n",
    "3. Tokenize English translations with Stanza\n",
    "4. Word alignment, substitute English NE translations with lemmas from the source, get information on NE annotations for each translated word from the source annotations\n",
    "5. Linguistically process English translation with Stanza (lemmas, POS)\n",
    "6. Parse CONLL-u file and add additional information (sentence ids, alignments, NER annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conllu import parse\n",
    "import pandas as pd\n",
    "import os\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip the folder with the files\n",
    "#with zipfile.ZipFile(\"/home/tajak/Parlamint-translation/ParlaMint-CZ/ParlaMint-CZ.conllu.zip\", 'r') as zip_ref:\n",
    "#    zip_ref.extractall(\"/home/tajak/Parlamint-translation/ParlaMint-CZ/ParlaMint-CZ.conllu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the main information\n",
    "lang_code = \"CZ\"\n",
    "path = \"/home/tajak/Parlamint-translation/ParlaMint-CZ/ParlaMint-CZ.conllu/ParlaMint-CZ.conllu\"\n",
    "opus_lang_code = \"cs\"\n",
    "\n",
    "# Create a folder named as the lang_code under results beforehand defining the following paths\n",
    "extracted_dataframe_path = \"/home/tajak/Parlamint-translation/results/{}/ParlaMint-{}-extracted-source-data.csv\".format(lang_code, lang_code)\n",
    "\n",
    "translated_dataframe_path = \"/home/tajak/Parlamint-translation/results/{}/ParlaMint-{}-translated.csv\".format(lang_code, lang_code)\n",
    "translated_tokenized_dataframe_path = \"/home/tajak/Parlamint-translation/results/{}/ParlaMint-{}-translated.csv\".format(lang_code, lang_code)\n",
    "final_dataframe = \"/home/tajak/Parlamint-translation/results/{}/ParlaMint-{}-final-dataframe.csv\".format(lang_code, lang_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/tajak/Parlamint-translation/ParlaMint-CZ/ParlaMint-CZ.conllu/ParlaMint-CZ.conllu/2013/ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003.conllu', '/home/tajak/Parlamint-translation/ParlaMint-CZ/ParlaMint-CZ.conllu/ParlaMint-CZ.conllu/2013/ParlaMint-CZ_2013-12-10-ps2013-004-01-004-003.conllu']\n",
      "['ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003.conllu', 'ParlaMint-CZ_2013-12-10-ps2013-004-01-004-003.conllu']\n"
     ]
    }
   ],
   "source": [
    "parl_list = []\n",
    "file_name_list = []\n",
    "\n",
    "for dir1 in os.listdir(path):\n",
    "    full_path = os.path.join(path, dir1)\n",
    "    if os.path.isdir(full_path):\n",
    "        current = os.listdir(full_path)\n",
    "        # Keep only files with parliamentary sessions:\n",
    "        for file in current:\n",
    "            if \"ParlaMint-{}_\".format(lang_code) in file:\n",
    "                if \".conllu\" in file:\n",
    "                    final_path = \"{}/{}\".format(full_path, file)\n",
    "                    parl_list.append(final_path)\n",
    "                    file_name_list.append(file)\n",
    "\n",
    "print(parl_list[:2])\n",
    "print(file_name_list[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6328"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See how many files we have:\n",
    "len(parl_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract information from CONLL-U files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Format:\n",
    "\n",
    "# sent_id = ParlaMint-CZ_2013-11-25-ps2013-001-01-000-000.u1.p4.s3\n",
    "# text = Dovolte mi tedy, abych vás seznámila s omluvami, které předložili členové vlády.\n",
    "1\tDovolte\tdovolit\tVERB\t_\tAspect=Perf|Mood=Imp|Number=Plur|Person=2|Polarity=Pos|VerbForm=Fin\t0\troot\t_\tNER=O\n",
    "2\tmi\tjá\tPRON\t_\tCase=Dat|Number=Sing|Person=1|PronType=Prs|Variant=Short\t1\tobl:arg\t_\tNER=O\n",
    "3\ttedy\ttedy\tADV\t_\t_\t1\tadvmod\t_\tNER=O|SpaceAfter=No\n",
    "4\t,\t,\tPUNCT\t_\t_\t8\tpunct\t_\tNER=O\n",
    "5-6\tabych\t_\t_\t_\t_\t_\t_\t_\tNER=O\n",
    "5\taby\taby\tSCONJ\t_\t_\t8\tmark\t_\t_\n",
    "6\tbych\tbýt\tAUX\t_\tMood=Cnd|Number=Sing|Person=1|VerbForm=Fin\t8\taux\t_\t_\n",
    "7\tvás\tty\tPRON\t_\tCase=Acc|Number=Plur|Person=2|PronType=Prs\t8\tobj\t_\tNER=O\n",
    "8\tseznámila\tseznámit\tVERB\t_\tAspect=Perf|Gender=Fem,Neut|Number=Plur,Sing|Polarity=Pos|Tense=Past|VerbForm=Part|Voice=Act\t1\tccomp\t_\tNER=O\n",
    "9\ts\ts\tADP\t_\tAdpType=Prep|Case=Ins\t10\tcase\t_\tNER=O\n",
    "10\tomluvami\tomluva\tNOUN\t_\tCase=Ins|Gender=Fem|Number=Plur|Polarity=Pos\t8\tobl:arg\t_\tNER=O|SpaceAfter=No\n",
    "11\t,\t,\tPUNCT\t_\t_\t13\tpunct\t_\tNER=O\n",
    "12\tkteré\tkterý\tDET\t_\tCase=Acc|Gender=Fem|Number=Plur|PronType=Int,Rel\t13\tobj\t_\tNER=O\n",
    "13\tpředložili\tpředložit\tVERB\t_\tAnimacy=Anim|Aspect=Perf|Gender=Masc|Number=Plur|Polarity=Pos|Tense=Past|VerbForm=Part|Voice=Act\t10\tacl:relcl\t_\tNER=O\n",
    "14\tčlenové\tčlen\tNOUN\t_\tAnimacy=Anim|Case=Nom|Gender=Masc|Number=Plur|Polarity=Pos\t13\tnsubj\t_\tNER=O\n",
    "15\tvlády\tvláda\tNOUN\t_\tCase=Gen|Gender=Fem|Number=Sing|Polarity=Pos\t14\tnmod\t_\tNER=O|SpaceAfter=No\n",
    "16\t.\t.\tPUNCT\t_\t_\t1\tpunct\t_\tNER=O\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Format of the parsed info (with conll-u parser):\n",
    "\n",
    "token inside a token list:\n",
    "\n",
    "{'id': 1,\n",
    " 'form': '3',\n",
    " 'lemma': '3',\n",
    " 'upos': 'NUM',\n",
    " 'xpos': None,\n",
    " 'feats': {'NumForm': 'Digit', 'NumType': 'Card'},\n",
    " 'head': 0,\n",
    " 'deprel': 'root',\n",
    " 'deps': None,\n",
    " 'misc': {'NER': 'O', 'SpaceAfter': 'No'}}\n",
    "\n",
    "\n",
    " metadata:\n",
    " \n",
    "metadata={newdoc id: \"ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003.u1\",\n",
    "newpar id: \"ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003.u1.p1\",\n",
    "sent_id: \"ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003.u1.p1.s1\",\n",
    "text: \"3.\"}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with this language is that we have multiword parts where one word is segmented into two syntactic units, as we can see in the case 5-6.  I need to transfer the NER annotation to the appropriate words. I will do this by applying the following rule: if a word does not have NER annotation, it should take the annotation from the word above it (the number-number word.) Then I will discard words that appear under indices in a form \"number-number\" from the tokenized text (to assure proper alignment)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found out that the parts of the multiword token do not have misc and NER annotation (misc is None), while the annotation is in the multiword token (which does not have annotations for lemmas, pos tags and all other annotations). We can find multiword token by searching for the type of the token id - in all other cases, the type is integer, while here it is a tuple (it is printed as (5, -, 6))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONLLU parser cheatsheet\n",
    "\"\"\"\n",
    "# Find which words are proper names with the filtering function\n",
    "sentence.filter(misc__NER=lambda x: x != \"O\")\n",
    "\n",
    "# Adding new metadata to the file\n",
    "sentences[0].metadata[\"alignment\"] =  \"1-1\"\n",
    "\n",
    "# To turn back into conll-u\n",
    "print(sentences[1].serialize())\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract from each sentence in the CONLL-u file:\n",
    "- sent_id (in metadata) (# sent_id = ParlaMint-SI_2014-08-01-SDZ7-Redna-01.seg1.1)\n",
    "- \"text\" (in metadata): to be feed into the MT system (# text = Spoštovani, prosim, da zasedete svoja mesta.)\n",
    "- tokenized text (punctuation separated from words by space): by iterating through the tokens in the sentence - create a list of tokens and join them into a string ([\"Spoštovani\", \"prosim\", \",\", \"da\"] -> \"Spoštovani prosim , da). In case of multiword tokens, we will add the subword tokens to the tokenized text and skip the multiword token. We will also get all necessary information about the ids and lemmas from the subword tokens. The subword tokens do not have the NER annotation, so we will use the multiword annotation for all of its subparts.\n",
    "- information on the proper nouns: if the word is annotated as a proper noun (has \"PER\" in ner attribute), take its index, form and lemma and save it into a dictionary for each sentence ({0: (Taje, Taja), 1: (Kuzman, Kuzman)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty df\n",
    "df = pd.DataFrame({\"file_path\": [\"\"],\"file\": [\"\"], \"sentence_id\": [\"\"], \"text\": [\"\"], \"tokenized_text\": [\"\"], \"NER\": [\"\"], \"proper_nouns\": [\"\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Check whether there are any problems with parsing the documents\n",
    "\"\"\"\n",
    "error_count = 0\n",
    "problematic_doc_list = []\n",
    "\n",
    "for doc in parl_list:\n",
    "\ttry:\n",
    "\t\t# Open the file\n",
    "\t\tdata = open(\"{}\".format(doc), \"r\").read()\n",
    "\n",
    "\t\tsentences = parse(data)\n",
    "\texcept:\n",
    "\t\terror_count += 1\n",
    "\t\tproblematic_doc_list.append(doc)\n",
    "\n",
    "print(error_count)\n",
    "print(problematic_doc_list)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the data with CONLL-u parser - to check if everything works, I will parse 100 files for now\n",
    "\n",
    "for doc in parl_list[:100]:\n",
    "\t# Open the file\n",
    "\tdata = open(\"{}\".format(doc), \"r\").read()\n",
    "\t\n",
    "\tsentences = parse(data)\n",
    "\n",
    "\tsentence_id_list = []\n",
    "\ttext_list = []\n",
    "\ttokenized_text_list = []\n",
    "\tproper_noun_list = []\n",
    "\n",
    "\tfor sentence in sentences:\n",
    "\t\t# Find sentence ids\n",
    "\t\tcurrent_sentence_id = sentence.metadata[\"sent_id\"]\n",
    "\t\tsentence_id_list.append(current_sentence_id)\n",
    "\n",
    "\t\t# Find text - if texts consists of multiword tokens, these tokens will appear as they are,\n",
    "\t\t# not separated into subwords\n",
    "\t\tcurrent_text = sentence.metadata[\"text\"]\n",
    "\t\ttext_list.append(current_text)\n",
    "\n",
    "\t\t# Create a string out of tokens\n",
    "\t\tcurrent_token_list = []\n",
    "\t\tword_dict = {}\n",
    "\n",
    "\t\tfor token in sentence:\n",
    "\t\t\t# Find multiword tokens and take their NER\n",
    "\t\t\tif type(token[\"id\"]) != int:\n",
    "\t\t\t\tmultiword_ner = token[\"misc\"][\"NER\"]\n",
    "\t\t\t\n",
    "\t\t\telse:\n",
    "\t\t\t# Append to the tokenized text tokens that are not multiword tokens\n",
    "\t\t\t# (we append subtokens to the tokenized texts, not multiword tokens)\n",
    "\t\t\t\tcurrent_token_list.append(token[\"form\"])\n",
    "\n",
    "\t\t\t\t# Create a list of NE annotations with word indices.\n",
    "\t\t\t\t# I'll substract one from the word index,\n",
    "\t\t\t\t# because indexing in the CONLLU file starts with 1, not 0\n",
    "\t\t\t\tcurrent_index = int(token[\"id\"]) - 1\n",
    "\n",
    "\t\t\t\t# If the word does not have NER annotation,\n",
    "\t\t\t\t# take the annotation from the multiword token\n",
    "\t\t\t\tif token[\"misc\"] is None:\n",
    "\t\t\t\t\tcurrent_ner = multiword_ner\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tcurrent_ner = token[\"misc\"][\"NER\"]\n",
    "\n",
    "\t\t\t\t# Add information on the lemma if the NE is personal name\n",
    "\t\t\t\t# and if the word is a PROPN\n",
    "\t\t\t\tif token[\"upos\"] == \"PROPN\":\n",
    "\t\t\t\t\tif \"PER\" in current_ner:\n",
    "\t\t\t\t\t\tword_dict[current_index] = [token[\"form\"], token[\"lemma\"]]\n",
    "\n",
    "\t\tproper_noun_list.append(word_dict)\n",
    "\n",
    "\t\tcurrent_string = \" \".join(current_token_list)\n",
    "\n",
    "\t\ttokenized_text_list.append(current_string)\n",
    "\t\n",
    "\tnew_df = pd.DataFrame({\"sentence_id\": sentence_id_list, \"text\": text_list, \"tokenized_text\": tokenized_text_list, \"proper_nouns\": proper_noun_list})\n",
    "\n",
    "\tnew_df[\"file_path\"] = doc\n",
    "\n",
    "\t# Get the file name\n",
    "\tfile_name = file_name_list[parl_list.index(doc)]\n",
    "\tnew_df[\"file\"] = file_name\n",
    "\n",
    "\t# Merge df to the previous df\n",
    "\tdf = pd.concat([df, new_df])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting text from all 6000+ files of the CZ corpora took 25 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Parse the data with CONLL-u parser - code for parsing for NER annotations as well\n",
    "\n",
    "for doc in parl_list:\n",
    "\t# Open the file\n",
    "\tdata = open(\"{}\".format(doc), \"r\").read()\n",
    "\t\n",
    "\tsentences = parse(data)\n",
    "\n",
    "\tsentence_id_list = []\n",
    "\ttext_list = []\n",
    "\ttokenized_text_list = []\n",
    "\tNER_list = []\n",
    "\tproper_noun_list = []\n",
    "\n",
    "\tfor sentence in sentences[:10]:\n",
    "\t\t# Find sentence ids\n",
    "\t\tcurrent_sentence_id = sentence.metadata[\"sent_id\"]\n",
    "\t\tsentence_id_list.append(current_sentence_id)\n",
    "\n",
    "\t\t# Find text\n",
    "\t\tcurrent_text = sentence.metadata[\"text\"]\n",
    "\t\ttext_list.append(current_text)\n",
    "\n",
    "\t\t# Create a string out of tokens\n",
    "\t\tcurrent_token_list = []\n",
    "\t\tcurrent_ner_dict = {}\n",
    "\t\tword_dict = {}\n",
    "\n",
    "\t\tfor token in sentence:\n",
    "\t\t\t# Find multiword tokens and take their NER\n",
    "\t\t\tif type(token[\"id\"]) != int:\n",
    "\t\t\t\tmultiword_ner = token[\"misc\"][\"NER\"]\n",
    "\t\t\t\n",
    "\t\t\telse:\n",
    "\t\t\t\tcurrent_token_list.append(token[\"form\"])\n",
    "\n",
    "\t\t\t\t# Create a list of NE annotations with word indices.\n",
    "\t\t\t\t# I'll substract one from the word index, because indexing in the CONLLU file starts with 1, not 0\n",
    "\t\t\t\tcurrent_index = int(token[\"id\"]) - 1\n",
    "\n",
    "\t\t\t\t# If the word does not have NER annotation, take the annotation from the multiword token\n",
    "\t\t\t\tif token[\"misc\"] is None:\n",
    "\t\t\t\t\tcurrent_ner = multiword_ner\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tcurrent_ner = token[\"misc\"][\"NER\"]\n",
    "\t\t\t\t\n",
    "\t\t\t\tcurrent_ner_dict[current_index] = current_ner\n",
    "\n",
    "\t\t\t\t# Add information on the lemma if the NE is personal name\n",
    "\t\t\t\t# if there will be a case where a multiword token is annotated with PER, this will break, but I assume this won't happen\n",
    "\t\t\t\tif \"PER\" in current_ner:\n",
    "\t\t\t\t\tword_dict[current_index] = [token[\"form\"], token[\"lemma\"]]\n",
    "\n",
    "\t\tproper_noun_list.append(word_dict)\n",
    "\n",
    "\t\tcurrent_string = \" \".join(current_token_list)\n",
    "\n",
    "\t\ttokenized_text_list.append(current_string)\n",
    "\t\tNER_list.append(current_ner_dict)\n",
    "\t\n",
    "\tnew_df = pd.DataFrame({\"sentence_id\": sentence_id_list, \"text\": text_list, \"tokenized_text\": tokenized_text_list, \"NER\": NER_list, \"proper_nouns\": proper_noun_list})\n",
    "\n",
    "\tnew_df[\"file_path\"] = doc\n",
    "\n",
    "\t# Get the file name\n",
    "\tfile_name = file_name_list[parl_list.index(doc)]\n",
    "\tnew_df[\"file\"] = file_name\n",
    "\n",
    "\t# Merge df to the previous df\n",
    "\tdf = pd.concat([df, new_df])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>file</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>NER</th>\n",
       "      <th>proper_nouns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>15117</td>\n",
       "      <td>15117</td>\n",
       "      <td>15117</td>\n",
       "      <td>15117</td>\n",
       "      <td>15117</td>\n",
       "      <td>0</td>\n",
       "      <td>15117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>15117</td>\n",
       "      <td>12796</td>\n",
       "      <td>12796</td>\n",
       "      <td>0</td>\n",
       "      <td>1610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>/home/tajak/Parlamint-translation/ParlaMint-CZ...</td>\n",
       "      <td>ParlaMint-CZ_2013-12-16-ps2013-004-03-003-018....</td>\n",
       "      <td>ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....</td>\n",
       "      <td>Děkuji.</td>\n",
       "      <td>Děkuji .</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1506</td>\n",
       "      <td>1506</td>\n",
       "      <td>1</td>\n",
       "      <td>516</td>\n",
       "      <td>516</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                file_path  \\\n",
       "count                                               15117   \n",
       "unique                                                100   \n",
       "top     /home/tajak/Parlamint-translation/ParlaMint-CZ...   \n",
       "freq                                                 1506   \n",
       "\n",
       "                                                     file  \\\n",
       "count                                               15117   \n",
       "unique                                                100   \n",
       "top     ParlaMint-CZ_2013-12-16-ps2013-004-03-003-018....   \n",
       "freq                                                 1506   \n",
       "\n",
       "                                              sentence_id     text  \\\n",
       "count                                               15117    15117   \n",
       "unique                                              15117    12796   \n",
       "top     ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....  Děkuji.   \n",
       "freq                                                    1      516   \n",
       "\n",
       "       tokenized_text  NER proper_nouns  \n",
       "count           15117    0        15117  \n",
       "unique          12796    0         1610  \n",
       "top          Děkuji .  NaN           {}  \n",
       "freq              516  NaN        13281  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reset index\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Remove the first row\n",
    "df = df.drop([0], axis=\"index\")\n",
    "\n",
    "# Reset index\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Show the results\n",
    "df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_path': {10147: '/home/tajak/Parlamint-translation/ParlaMint-CZ/ParlaMint-CZ.conllu/ParlaMint-CZ.conllu/2013/ParlaMint-CZ_2013-11-25-ps2013-001-01-000-000.conllu'},\n",
       " 'file': {10147: 'ParlaMint-CZ_2013-11-25-ps2013-001-01-000-000.conllu'},\n",
       " 'sentence_id': {10147: 'ParlaMint-CZ_2013-11-25-ps2013-001-01-000-000.u1.p4.s3'},\n",
       " 'text': {10147: 'Dovolte mi tedy, abych vás seznámila s omluvami, které předložili členové vlády.'},\n",
       " 'tokenized_text': {10147: 'Dovolte mi tedy , aby bych vás seznámila s omluvami , které předložili členové vlády .'},\n",
       " 'NER': {10147: nan},\n",
       " 'proper_nouns': {10147: {}}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect what happened with sentences that contain multiword tokens\n",
    "df[df[\"sentence_id\"] == \"ParlaMint-CZ_2013-11-25-ps2013-001-01-000-000.u1.p4.s3\"].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>file</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>NER</th>\n",
       "      <th>proper_nouns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/tajak/Parlamint-translation/ParlaMint-CZ...</td>\n",
       "      <td>ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....</td>\n",
       "      <td>ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....</td>\n",
       "      <td>3.</td>\n",
       "      <td>3 .</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/tajak/Parlamint-translation/ParlaMint-CZ...</td>\n",
       "      <td>ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....</td>\n",
       "      <td>ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....</td>\n",
       "      <td>Návrh zasedacího pořádku poslanců v jednacím s...</td>\n",
       "      <td>Návrh zasedacího pořádku poslanců v jednacím s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/tajak/Parlamint-translation/ParlaMint-CZ...</td>\n",
       "      <td>ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....</td>\n",
       "      <td>ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....</td>\n",
       "      <td>Podle § 52 odst. 1 našeho jednacího řádu Posla...</td>\n",
       "      <td>Podle § 52 odst . 1 našeho jednacího řádu Posl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/tajak/Parlamint-translation/ParlaMint-CZ...</td>\n",
       "      <td>ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....</td>\n",
       "      <td>ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....</td>\n",
       "      <td>Já bych k této věci otevřel rozpravu.</td>\n",
       "      <td>Já bych k této věci otevřel rozpravu .</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/tajak/Parlamint-translation/ParlaMint-CZ...</td>\n",
       "      <td>ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....</td>\n",
       "      <td>ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....</td>\n",
       "      <td>Pan poslanec a předseda klubu ODS Zbyněk Stanj...</td>\n",
       "      <td>Pan poslanec a předseda klubu ODS Zbyněk Stanj...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{6: ['Zbyněk', 'Zbyněk'], 7: ['Stanjura', 'Sta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_path  \\\n",
       "0  /home/tajak/Parlamint-translation/ParlaMint-CZ...   \n",
       "1  /home/tajak/Parlamint-translation/ParlaMint-CZ...   \n",
       "2  /home/tajak/Parlamint-translation/ParlaMint-CZ...   \n",
       "3  /home/tajak/Parlamint-translation/ParlaMint-CZ...   \n",
       "4  /home/tajak/Parlamint-translation/ParlaMint-CZ...   \n",
       "\n",
       "                                                file  \\\n",
       "0  ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....   \n",
       "1  ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....   \n",
       "2  ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....   \n",
       "3  ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....   \n",
       "4  ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....   \n",
       "\n",
       "                                         sentence_id  \\\n",
       "0  ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....   \n",
       "1  ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....   \n",
       "2  ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....   \n",
       "3  ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....   \n",
       "4  ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....   \n",
       "\n",
       "                                                text  \\\n",
       "0                                                 3.   \n",
       "1  Návrh zasedacího pořádku poslanců v jednacím s...   \n",
       "2  Podle § 52 odst. 1 našeho jednacího řádu Posla...   \n",
       "3              Já bych k této věci otevřel rozpravu.   \n",
       "4  Pan poslanec a předseda klubu ODS Zbyněk Stanj...   \n",
       "\n",
       "                                      tokenized_text  NER  \\\n",
       "0                                                3 .  NaN   \n",
       "1  Návrh zasedacího pořádku poslanců v jednacím s...  NaN   \n",
       "2  Podle § 52 odst . 1 našeho jednacího řádu Posl...  NaN   \n",
       "3             Já bych k této věci otevřel rozpravu .  NaN   \n",
       "4  Pan poslanec a předseda klubu ODS Zbyněk Stanj...  NaN   \n",
       "\n",
       "                                        proper_nouns  \n",
       "0                                                 {}  \n",
       "1                                                 {}  \n",
       "2                                                 {}  \n",
       "3                                                 {}  \n",
       "4  {6: ['Zbyněk', 'Zbyněk'], 7: ['Stanjura', 'Sta...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.map_locations\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'dict'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'pandas._libs.index.IndexEngine._call_map_locations'\n",
      "Traceback (most recent call last):\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5231, in pandas._libs.hashtable.PyObjectHashTable.map_locations\n",
      "TypeError: unhashable type: 'dict'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}                                                                                  13281\n",
       "{2: ['Kalousek', 'Kalousek']}                                                          10\n",
       "{5: ['Martina', 'Martin'], 6: ['Kolovratníka', 'Kolovratník']}                          8\n",
       "{7: ['Kalousek', 'Kalousek']}                                                           6\n",
       "{6: ['Martina', 'Martin'], 7: ['Kolovratníka', 'Kolovratník']}                          6\n",
       "                                                                                    ...  \n",
       "{4: ['Helena', 'Helena'], 5: ['Válková', 'Válková']}                                    1\n",
       "{3: ['Stanjurovi', 'Stanjur'], 28: ['Jeroným', 'Jeroným'], 29: ['Tejc', 'Tejc']}        1\n",
       "{2: ['Stanjura', 'Stanjura'], 17: ['Tejc', 'Tejc']}                                     1\n",
       "{5: ['Milan', 'Milan'], 6: ['Štěch', 'Štěch']}                                          1\n",
       "{14: ['Sklenáka', 'Sklenák']}                                                           1\n",
       "Name: proper_nouns, Length: 1610, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.proper_nouns.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the corpora: 228433\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>15117.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>15.111001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.674160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>21.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>193.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             length\n",
       "count  15117.000000\n",
       "mean      15.111001\n",
       "std       13.674160\n",
       "min        1.000000\n",
       "25%        5.000000\n",
       "50%       11.000000\n",
       "75%       21.000000\n",
       "max      193.000000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add information on length\n",
    "df[\"length\"] = df[\"text\"].str.split().str.len()\n",
    "\n",
    "print(\"Number of words in the corpora: {}\".format(df[\"length\"].sum()))\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe\n",
    "df.to_csv(\"{}\".format(extracted_dataframe_path), sep=\"\\t\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>file</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>NER</th>\n",
       "      <th>proper_nouns</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/tajak/Parlamint-translation/ParlaMint-CZ...</td>\n",
       "      <td>ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....</td>\n",
       "      <td>ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....</td>\n",
       "      <td>3.</td>\n",
       "      <td>3 .</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/tajak/Parlamint-translation/ParlaMint-CZ...</td>\n",
       "      <td>ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....</td>\n",
       "      <td>ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....</td>\n",
       "      <td>Návrh zasedacího pořádku poslanců v jednacím s...</td>\n",
       "      <td>Návrh zasedacího pořádku poslanců v jednacím s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{}</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_path  \\\n",
       "0  /home/tajak/Parlamint-translation/ParlaMint-CZ...   \n",
       "1  /home/tajak/Parlamint-translation/ParlaMint-CZ...   \n",
       "\n",
       "                                                file  \\\n",
       "0  ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....   \n",
       "1  ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....   \n",
       "\n",
       "                                         sentence_id  \\\n",
       "0  ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....   \n",
       "1  ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....   \n",
       "\n",
       "                                                text  \\\n",
       "0                                                 3.   \n",
       "1  Návrh zasedacího pořádku poslanců v jednacím s...   \n",
       "\n",
       "                                      tokenized_text  NER proper_nouns  length  \n",
       "0                                                3 .  NaN           {}       1  \n",
       "1  Návrh zasedacího pořádku poslanců v jednacím s...  NaN           {}       9  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open the df\n",
    "df = pd.read_csv(\"{}\".format(extracted_dataframe_path), sep=\"\\t\", index_col = 0)\n",
    "df.head(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to translate the following corpora into English:\n",
    "- Belgian (BE) - which language??\n",
    "- Bulgarian (BG)\n",
    "- Croatian (HR) - We will use \"South Slavic MT\" based on the manual analysis\n",
    "- Czech (CZ)\n",
    "- Danish (DK)\n",
    "- Dutch (NL)\n",
    "- French (FR)\n",
    "- Hungarian (HU) - multilingual model only\n",
    "- Icelandic (IS)\n",
    "- Italian (IT)\n",
    "- Latvian (LV)\n",
    "- Lithuanian (LT)\n",
    "- Polish (PL)\n",
    "- Slovenian (SI) - We will use \"Slavic MT\" based on the results of the manual analysis\n",
    "- Spanish? (ES)\n",
    "- Turkish (TR)\n",
    "- Austrian (AT)\n",
    "- Basque (ES-PV)\n",
    "- Bosnian (BA)\n",
    "- Catalan (ES-CT)\n",
    "- Estonian (EE)\n",
    "- Finnish (FI)\n",
    "- Galician (ES-GA)\n",
    "- Greek (GR)\n",
    "- Norwegian (NO) - NO OPUS-MT model (!) - we can use GT or eTranslation\n",
    "- Portuguese (PT)\n",
    "- Romanian (RO)\n",
    "- Serbian (RS)\n",
    "- Swedish (SE)\n",
    "- Ukrainian (UA)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of language codes:\n",
    "- sla = Slavic\n",
    "- zls = South Slavic\n",
    "- zlw = West Slavic\n",
    "- zle = East Slavic\n",
    "- gmq = North Germanic\n",
    "- gem = Germanic\n",
    "- gmw = West Germanic\n",
    "- roa = Romance\n",
    "- itc = Italic\n",
    "- bat = Baltic\n",
    "- trk = Turkic\n",
    "- urj = Uralic\n",
    "- fiu = Finno-Ugrian"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "choose_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_model(lang_code):\n",
    "\t\"\"\"\n",
    "\tCompare a small sample of translations of all OPUS-MT models that are available\n",
    "\tfor the language, to decide which one to use. The function prints out a dataframe with all translations of the sample and saves it as ParlaMint-{lang_code}-sample-model-comparison.csv.\n",
    "\n",
    "\tArgs:\n",
    "\t- lang_code: the lang code that is used in the names of the files, it should be the same as for extract_text()\n",
    "\t\"\"\"\n",
    "\timport pandas as pd\n",
    "\timport regex as re\n",
    "\tfrom easynmt import EasyNMT\n",
    "\tfrom IPython.display import display\n",
    "\t\n",
    "\tlang_models_dict = {\"BG\": [\"bg\", \"sla\", \"zls\"], \"HR\": [\"zls\"], \"CZ\": [\"cs\", \"sla\", \"zlw\" ], \"DK\": [\"da\", \"gmq\", \"gem\"], \"NL\": [\"nl\", \"gem\", \"gmw\"], \"FR\": [\"fr\", \"itc\",\"roa\"], \"HU\": [\"mul\"], \"IS\": [\"is\",\"gmq\", \"gem\"], \"IT\": [\"it\", \"roa\", \"itc\"], \"LV\": [\"lv\",\"bat\"], \"LT\": [\"bat\"], \"PL\": [\"pl\", \"sla\", \"zlw\"], \"SI\": [\"sla\", \"zls\"], \"ES\": [\"es\", \"roa\", \"itc\"], \"TR\": [\"tr\", \"trk\" ], \"AT\": [\"de\", \"gem\", \"gmw\"], \"ES-PV\": [\"eu\", \"mul\"], \"BA\": [\"sla\", \"zls\"], \"ES-CT\": [\"ca\", \"roa\", \"itc\"], \"EE\": [\"et\", \"urj\", \"fiu\"], \"FI\": [\"fi\", \"urj\", \"fiu\"], \"ES-GA\": [\"gl\", \"roa\", \"itc\"], \"GR\": [\"grk\"], \"PT\": [\"roa\", \"itc\"], \"RO\":[\"roa\", \"itc\"], \"RS\": [\"zls\", \"sla\"], \"SE\": [\"sv\", \"gmq\", \"gem\"], \"UA\":[\"uk\", \"sla\", \"zle\"]}\n",
    "\n",
    "\t# Open the file, created in the previous step\n",
    "\tdf = pd.read_csv(\"{}\".format(extracted_dataframe_path), sep=\"\\t\", index_col=0)\n",
    "\n",
    "\t# Define the model\n",
    "\tmodel = EasyNMT('opus-mt')\n",
    "\n",
    "\tprint(\"Entire corpus has {} sentences and {} words.\".format(df[\"text\"].count(), df[\"length\"].sum()))\n",
    "\n",
    "\t# Create a smaller sample - just a couple of sentences from one file\n",
    "\tdf = df[df.file == list(df[\"file\"].unique())[0]][:20]\n",
    "\n",
    "\tprint(\"Sample files has {} sentences and {} words.\".format(df[\"text\"].count(), df[\"length\"].sum()))\n",
    "\n",
    "\t# Create a list of sentences from the df\n",
    "\tsentence_list = df.text.to_list()\n",
    "\n",
    "\t# Translate the sample using all available models for this language\n",
    "\tfor opus_lang_code in lang_models_dict[lang_code]:\n",
    "\t\ttranslation_list = model.translate(sentence_list, source_lang = \"{}\".format(opus_lang_code), target_lang='en')\n",
    "\n",
    "\t\t# Add the translations to the df\n",
    "\t\tdf[\"translation-{}\".format(opus_lang_code)] = translation_list\n",
    "\t\n",
    "\tdf = df.drop(columns=[\"file\", \"sentence_id\", \"tokenized_text\", \"NER\", \"proper_nouns\", \"length\"])\n",
    "\n",
    "\t# Save the df\n",
    "\tdf.to_csv(\"/home/tajak/Parlamint-translation/results/{}/ParlaMint-{}-sample-model-comparison.csv\".format(lang_code, lang_code))\n",
    "\n",
    "\tprint(\"The file is saved as/home/tajak/Parlamint-translation/ results/{}/ParlaMint-{}-sample-model-comparison.csv. \".format(lang_code, lang_code))\n",
    "\n",
    "\treturn df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entire corpus has 15117 sentences and 228433 words.\n",
      "Sample files has 20 sentences and 245 words.\n",
      "The file is saved as/home/tajak/Parlamint-translation/ results/CZ/ParlaMint-CZ-sample-model-comparison.csv. \n"
     ]
    }
   ],
   "source": [
    "df = choose_model(lang_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>text</th>\n",
       "      <th>translation-cs</th>\n",
       "      <th>translation-sla</th>\n",
       "      <th>translation-zlw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/tajak/Parlamint-translation/ParlaMint-CZ...</td>\n",
       "      <td>3.</td>\n",
       "      <td>3.</td>\n",
       "      <td>3.</td>\n",
       "      <td>3.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/tajak/Parlamint-translation/ParlaMint-CZ...</td>\n",
       "      <td>Návrh zasedacího pořádku poslanců v jednacím s...</td>\n",
       "      <td>Proposal for a sitting order of Members in the...</td>\n",
       "      <td>Draft meeting order of Members in the Chamber ...</td>\n",
       "      <td>Proposal for a meeting order of Members in the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_path  \\\n",
       "0  /home/tajak/Parlamint-translation/ParlaMint-CZ...   \n",
       "1  /home/tajak/Parlamint-translation/ParlaMint-CZ...   \n",
       "\n",
       "                                                text  \\\n",
       "0                                                 3.   \n",
       "1  Návrh zasedacího pořádku poslanců v jednacím s...   \n",
       "\n",
       "                                      translation-cs  \\\n",
       "0                                                 3.   \n",
       "1  Proposal for a sitting order of Members in the...   \n",
       "\n",
       "                                     translation-sla  \\\n",
       "0                                                 3.   \n",
       "1  Draft meeting order of Members in the Chamber ...   \n",
       "\n",
       "                                     translation-zlw  \n",
       "0                                                 3.  \n",
       "1  Proposal for a meeting order of Members in the...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open the analysed sample\n",
    "\n",
    "sample = pd.read_csv(\"/home/tajak/Parlamint-translation/results/{}/ParlaMint-{}-sample-model-comparison.csv\".format(lang_code, lang_code), index_col = 0)\n",
    "sample.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.comparison.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model for Czech was shown to be cs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "translate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(lang_code, opus_lang_code):\n",
    "\t\"\"\"\n",
    "\tThis function translates the text from the dataframe, created with the extract_text() function\n",
    "\twith OPUS-MT models using EasyNMT. It returns a dataframe with the translation.\n",
    "\n",
    "\tArgs:\n",
    "\t- lang_code: the lang code that is used in the names of the files, it should be the same as for extract_text()\n",
    "\t- opus_lang_code: the lang code to be used in the OPUS-MT model - use the one that performed the best in the comparison (see function choose_model())\n",
    "\t\"\"\"\n",
    "\timport pandas as pd\n",
    "\timport regex as re\n",
    "\tfrom easynmt import EasyNMT\n",
    "\tfrom IPython.display import display\n",
    "\n",
    "\t# Open the file, created in the previous step\n",
    "\tdf = pd.read_csv(\"{}\".format(extracted_dataframe_path), sep=\"\\t\", index_col=0)\n",
    "\n",
    "\t# Define the model\n",
    "\tmodel = EasyNMT('opus-mt')\n",
    "\n",
    "\tprint(\"Entire corpus has {} sentences and {} words.\".format(df[\"text\"].count(), df[\"length\"].sum()))\n",
    "\n",
    "\t# Create a list of sentences from the df\n",
    "\tsentence_list = df.text.to_list()\n",
    "\n",
    "\t#Translate the list of sentences - you need to provide the source language as it is in the name of the model - the opus_lang_code\n",
    "\ttranslation_list = model.translate(sentence_list, source_lang = \"{}\".format(opus_lang_code), target_lang='en')\n",
    "\n",
    "\t# Add the translations to the df\n",
    "\tdf[\"translation\"] = translation_list\n",
    "\n",
    "\t# Display the df\n",
    "\tdisplay(df[:3])\n",
    "\n",
    "\t# Save the df\n",
    "\tdf.to_csv(\"{}\".format(translated_dataframe_path), sep=\"\\t\")\n",
    "\n",
    "\treturn df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entire corpus has 15117 sentences and 228433 words.\n"
     ]
    }
   ],
   "source": [
    "df = translate(lang_code, opus_lang_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>file</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>NER</th>\n",
       "      <th>proper_nouns</th>\n",
       "      <th>length</th>\n",
       "      <th>translation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15115</th>\n",
       "      <td>/home/tajak/Parlamint-translation/ParlaMint-CZ...</td>\n",
       "      <td>ParlaMint-CZ_2016-07-01-ps2013-048-04-000-000....</td>\n",
       "      <td>ParlaMint-CZ_2016-07-01-ps2013-048-04-000-000....</td>\n",
       "      <td>Tento návrh byl přijat.</td>\n",
       "      <td>Tento návrh byl přijat .</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{}</td>\n",
       "      <td>4</td>\n",
       "      <td>This proposal was adopted.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15116</th>\n",
       "      <td>/home/tajak/Parlamint-translation/ParlaMint-CZ...</td>\n",
       "      <td>ParlaMint-CZ_2016-07-01-ps2013-048-04-000-000....</td>\n",
       "      <td>ParlaMint-CZ_2016-07-01-ps2013-048-04-000-000....</td>\n",
       "      <td>Tím jsme se vypořádali s pořadem schůze a může...</td>\n",
       "      <td>Tím jsme se vypořádali s pořadem schůze a může...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{}</td>\n",
       "      <td>12</td>\n",
       "      <td>We've dealt with the agenda of the meeting and...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               file_path  \\\n",
       "15115  /home/tajak/Parlamint-translation/ParlaMint-CZ...   \n",
       "15116  /home/tajak/Parlamint-translation/ParlaMint-CZ...   \n",
       "\n",
       "                                                    file  \\\n",
       "15115  ParlaMint-CZ_2016-07-01-ps2013-048-04-000-000....   \n",
       "15116  ParlaMint-CZ_2016-07-01-ps2013-048-04-000-000....   \n",
       "\n",
       "                                             sentence_id  \\\n",
       "15115  ParlaMint-CZ_2016-07-01-ps2013-048-04-000-000....   \n",
       "15116  ParlaMint-CZ_2016-07-01-ps2013-048-04-000-000....   \n",
       "\n",
       "                                                    text  \\\n",
       "15115                            Tento návrh byl přijat.   \n",
       "15116  Tím jsme se vypořádali s pořadem schůze a může...   \n",
       "\n",
       "                                          tokenized_text  NER proper_nouns  \\\n",
       "15115                           Tento návrh byl přijat .  NaN           {}   \n",
       "15116  Tím jsme se vypořádali s pořadem schůze a může...  NaN           {}   \n",
       "\n",
       "       length                                        translation  \n",
       "15115       4                         This proposal was adopted.  \n",
       "15116      12  We've dealt with the agenda of the meeting and...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3.',\n",
       " \"Proposal for a sitting order of Members in the Chamber of Deputies' Chamber of Deputies\",\n",
       " \"According to Article 52 (2) (a) of the basic Regulation, the Commission considers that the aid is compatible with the internal market under Article 107 (3) (c) of the Treaty on the Functioning of the European Union and Article 108 (3) of the Treaty on the Functioning of the European Union. 1 of our Rules of Procedure The Chamber of Deputies approves the sitting arrangements of Members in the Chamber of Deputies' Chamber of Deputies' Chamber of Deputies' Chamber of Deputies' Chamber of Deputies' Chamber of Deputies' Chamber of Deputies, and a proposal has been given to you as agreed upon by the various Members' Clubs.\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.translation.to_list()[:3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word alignment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization with Stanza"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We apply the stanza tokenization over the translation; use tokenize_no_ssplit to avoid splitting sentences in multiple sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>file</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>NER</th>\n",
       "      <th>proper_nouns</th>\n",
       "      <th>length</th>\n",
       "      <th>translation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/tajak/Parlamint-translation/ParlaMint-CZ...</td>\n",
       "      <td>ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....</td>\n",
       "      <td>ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....</td>\n",
       "      <td>3.</td>\n",
       "      <td>3 .</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{}</td>\n",
       "      <td>1</td>\n",
       "      <td>3.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/tajak/Parlamint-translation/ParlaMint-CZ...</td>\n",
       "      <td>ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....</td>\n",
       "      <td>ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....</td>\n",
       "      <td>Návrh zasedacího pořádku poslanců v jednacím s...</td>\n",
       "      <td>Návrh zasedacího pořádku poslanců v jednacím s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{}</td>\n",
       "      <td>9</td>\n",
       "      <td>Proposal for a sitting order of Members in the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_path  \\\n",
       "0  /home/tajak/Parlamint-translation/ParlaMint-CZ...   \n",
       "1  /home/tajak/Parlamint-translation/ParlaMint-CZ...   \n",
       "\n",
       "                                                file  \\\n",
       "0  ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....   \n",
       "1  ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....   \n",
       "\n",
       "                                         sentence_id  \\\n",
       "0  ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....   \n",
       "1  ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....   \n",
       "\n",
       "                                                text  \\\n",
       "0                                                 3.   \n",
       "1  Návrh zasedacího pořádku poslanců v jednacím s...   \n",
       "\n",
       "                                      tokenized_text  NER proper_nouns  \\\n",
       "0                                                3 .  NaN           {}   \n",
       "1  Návrh zasedacího pořádku poslanců v jednacím s...  NaN           {}   \n",
       "\n",
       "   length                                        translation  \n",
       "0       1                                                 3.  \n",
       "1       9  Proposal for a sitting order of Members in the...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open the translated df\n",
    "\n",
    "df = pd.read_csv(\"{}\".format(translated_dataframe_path), sep=\"\\t\", index_col = 0)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-17 13:15:10 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10fbadd0f4314c348d6dbf718cac1dfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-17 13:15:11 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "========================\n",
      "\n",
      "2023-01-17 13:15:11 INFO: Use device: gpu\n",
      "2023-01-17 13:15:11 INFO: Loading: tokenize\n",
      "2023-01-17 13:15:13 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize', tokenize_no_ssplit = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'id': 1, 'text': 'Proposal', 'start_char': 0, 'end_char': 8},\n",
       "  {'id': 2, 'text': 'for', 'start_char': 9, 'end_char': 12},\n",
       "  {'id': 3, 'text': 'a', 'start_char': 13, 'end_char': 14},\n",
       "  {'id': 4, 'text': 'sitting', 'start_char': 15, 'end_char': 22},\n",
       "  {'id': 5, 'text': 'order', 'start_char': 23, 'end_char': 28},\n",
       "  {'id': 6, 'text': 'of', 'start_char': 29, 'end_char': 31},\n",
       "  {'id': 7, 'text': 'Members', 'start_char': 32, 'end_char': 39},\n",
       "  {'id': 8, 'text': 'in', 'start_char': 40, 'end_char': 42},\n",
       "  {'id': 9, 'text': 'the', 'start_char': 43, 'end_char': 46},\n",
       "  {'id': 10, 'text': 'Chamber', 'start_char': 47, 'end_char': 54},\n",
       "  {'id': 11, 'text': 'of', 'start_char': 55, 'end_char': 57},\n",
       "  {'id': 12, 'text': 'Deputies', 'start_char': 58, 'end_char': 66},\n",
       "  {'id': 13, 'text': \"'\", 'start_char': 66, 'end_char': 67},\n",
       "  {'id': 14, 'text': 'Chamber', 'start_char': 68, 'end_char': 75},\n",
       "  {'id': 15, 'text': 'of', 'start_char': 76, 'end_char': 78},\n",
       "  {'id': 16, 'text': 'Deputies', 'start_char': 79, 'end_char': 87}]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect how the output of Stanza pipeline looks like\n",
    "En_sentences = df.translation.to_list()\n",
    "\n",
    "tokenized_sentences = []\n",
    "space_after_list = []\n",
    "\n",
    "for i in En_sentences[:2]:\n",
    "\tdoc = nlp(i).to_dict()\n",
    "\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>file_path</th>\n",
       "      <th>file</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>NER</th>\n",
       "      <th>proper_nouns</th>\n",
       "      <th>length</th>\n",
       "      <th>translation</th>\n",
       "      <th>translation-tokenized</th>\n",
       "      <th>space-after-information</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>/home/tajak/Parlamint-translation/ParlaMint-CZ...</td>\n",
       "      <td>ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....</td>\n",
       "      <td>ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....</td>\n",
       "      <td>3.</td>\n",
       "      <td>3 .</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{}</td>\n",
       "      <td>1</td>\n",
       "      <td>3.</td>\n",
       "      <td>3 .</td>\n",
       "      <td>[No, Last]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>/home/tajak/Parlamint-translation/ParlaMint-CZ...</td>\n",
       "      <td>ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....</td>\n",
       "      <td>ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....</td>\n",
       "      <td>Návrh zasedacího pořádku poslanců v jednacím s...</td>\n",
       "      <td>Návrh zasedacího pořádku poslanců v jednacím s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{}</td>\n",
       "      <td>9</td>\n",
       "      <td>Proposal for a sitting order of Members in the...</td>\n",
       "      <td>Proposal for a sitting order of Members in the...</td>\n",
       "      <td>[Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>/home/tajak/Parlamint-translation/ParlaMint-CZ...</td>\n",
       "      <td>ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....</td>\n",
       "      <td>ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....</td>\n",
       "      <td>Podle § 52 odst. 1 našeho jednacího řádu Posla...</td>\n",
       "      <td>Podle § 52 odst . 1 našeho jednacího řádu Posl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{}</td>\n",
       "      <td>33</td>\n",
       "      <td>According to Article 52 (2) (a) of the basic R...</td>\n",
       "      <td>According to Article 52 ( 2 ) ( a ) of the bas...</td>\n",
       "      <td>[Yes, Yes, Yes, Yes, No, No, Yes, No, No, Yes,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>/home/tajak/Parlamint-translation/ParlaMint-CZ...</td>\n",
       "      <td>ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....</td>\n",
       "      <td>ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....</td>\n",
       "      <td>Já bych k této věci otevřel rozpravu.</td>\n",
       "      <td>Já bych k této věci otevřel rozpravu .</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{}</td>\n",
       "      <td>7</td>\n",
       "      <td>I would like to open a debate on this matter.</td>\n",
       "      <td>I would like to open a debate on this matter .</td>\n",
       "      <td>[Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>/home/tajak/Parlamint-translation/ParlaMint-CZ...</td>\n",
       "      <td>ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....</td>\n",
       "      <td>ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....</td>\n",
       "      <td>Pan poslanec a předseda klubu ODS Zbyněk Stanj...</td>\n",
       "      <td>Pan poslanec a předseda klubu ODS Zbyněk Stanj...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{6: ['Zbyněk', 'Zbyněk'], 7: ['Stanjura', 'Sta...</td>\n",
       "      <td>8</td>\n",
       "      <td>Member and Chairman of the ODS Club Zbyněk Sta...</td>\n",
       "      <td>Member and Chairman of the ODS Club Zbyněk Sta...</td>\n",
       "      <td>[Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, No, L...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                          file_path  \\\n",
       "0           0  /home/tajak/Parlamint-translation/ParlaMint-CZ...   \n",
       "1           1  /home/tajak/Parlamint-translation/ParlaMint-CZ...   \n",
       "2           2  /home/tajak/Parlamint-translation/ParlaMint-CZ...   \n",
       "3           3  /home/tajak/Parlamint-translation/ParlaMint-CZ...   \n",
       "4           4  /home/tajak/Parlamint-translation/ParlaMint-CZ...   \n",
       "\n",
       "                                                file  \\\n",
       "0  ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....   \n",
       "1  ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....   \n",
       "2  ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....   \n",
       "3  ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....   \n",
       "4  ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....   \n",
       "\n",
       "                                         sentence_id  \\\n",
       "0  ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....   \n",
       "1  ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....   \n",
       "2  ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....   \n",
       "3  ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....   \n",
       "4  ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....   \n",
       "\n",
       "                                                text  \\\n",
       "0                                                 3.   \n",
       "1  Návrh zasedacího pořádku poslanců v jednacím s...   \n",
       "2  Podle § 52 odst. 1 našeho jednacího řádu Posla...   \n",
       "3              Já bych k této věci otevřel rozpravu.   \n",
       "4  Pan poslanec a předseda klubu ODS Zbyněk Stanj...   \n",
       "\n",
       "                                      tokenized_text  NER  \\\n",
       "0                                                3 .  NaN   \n",
       "1  Návrh zasedacího pořádku poslanců v jednacím s...  NaN   \n",
       "2  Podle § 52 odst . 1 našeho jednacího řádu Posl...  NaN   \n",
       "3             Já bych k této věci otevřel rozpravu .  NaN   \n",
       "4  Pan poslanec a předseda klubu ODS Zbyněk Stanj...  NaN   \n",
       "\n",
       "                                        proper_nouns  length  \\\n",
       "0                                                 {}       1   \n",
       "1                                                 {}       9   \n",
       "2                                                 {}      33   \n",
       "3                                                 {}       7   \n",
       "4  {6: ['Zbyněk', 'Zbyněk'], 7: ['Stanjura', 'Sta...       8   \n",
       "\n",
       "                                         translation  \\\n",
       "0                                                 3.   \n",
       "1  Proposal for a sitting order of Members in the...   \n",
       "2  According to Article 52 (2) (a) of the basic R...   \n",
       "3      I would like to open a debate on this matter.   \n",
       "4  Member and Chairman of the ODS Club Zbyněk Sta...   \n",
       "\n",
       "                               translation-tokenized  \\\n",
       "0                                                3 .   \n",
       "1  Proposal for a sitting order of Members in the...   \n",
       "2  According to Article 52 ( 2 ) ( a ) of the bas...   \n",
       "3     I would like to open a debate on this matter .   \n",
       "4  Member and Chairman of the ODS Club Zbyněk Sta...   \n",
       "\n",
       "                             space-after-information  \n",
       "0                                         [No, Last]  \n",
       "1  [Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, ...  \n",
       "2  [Yes, Yes, Yes, Yes, No, No, Yes, No, No, Yes,...  \n",
       "3  [Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, ...  \n",
       "4  [Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, No, L...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply tokenization to English translation and add the sentences to the df\n",
    "# Open the df\n",
    "df = pd.read_csv(\"{}\".format(translated_dataframe_path), sep=\"\\t\")\n",
    "\n",
    "\n",
    "# Save also the information on whether there is a space after or before punctuation\n",
    "# which we will need later, to remove unnecessary spaces\n",
    "\n",
    "En_sentences = df.translation.to_list()\n",
    "\n",
    "tokenized_sentences = []\n",
    "space_after_list = []\n",
    "\n",
    "for i in En_sentences:\n",
    "\tdoc = nlp(i).to_dict()\n",
    "\tcurrent_sentence_list = []\n",
    "\tcurrent_space_after_list = []\n",
    "\n",
    "\t# Define a list of start_char and end_char\n",
    "\tstart_chars = []\n",
    "\tend_chars = []\n",
    "\n",
    "\t# Loop through the tokens in the sentence and add them to a current sentence list\n",
    "\tfor sentence in doc:\n",
    "\t\tfor word in sentence:\n",
    "\t\t\tcurrent_sentence_list.append(word[\"text\"])\n",
    "\n",
    "\t\t\t# Add information on start and end chars to the list\n",
    "\t\t\tstart_chars.append(word[\"start_char\"])\n",
    "\t\t\tend_chars.append(word[\"end_char\"])\n",
    "\t\t\n",
    "\t# Now loop through the start_char and end_char lists and find instances\n",
    "\t# where the end_char of one word is the same as the start_char of the next one\n",
    "\t# this means there is no space between them\n",
    "\tfor char_index in range(len(start_chars)-1):\n",
    "\t\tif end_chars[char_index] == start_chars[(char_index+1)]:\n",
    "\t\t\tcurrent_space_after_list.append(\"No\")\n",
    "\t\telse:\n",
    "\t\t\tcurrent_space_after_list.append(\"Yes\")\n",
    "\n",
    "\t# This loop is not possible for the end token, so let's add information for the last token\n",
    "\t# just to avoid errors due to different lengths of lists\n",
    "\tcurrent_space_after_list.append(\"Last\")\n",
    "\n",
    "\t# Join the list into a space-separated string\n",
    "\tcurrent_string = \" \".join(current_sentence_list)\n",
    "\n",
    "\ttokenized_sentences.append(current_string)\n",
    "\n",
    "\tspace_after_list.append(current_space_after_list)\n",
    "\n",
    "# Add the result to the df\n",
    "df[\"translation-tokenized\"] = tokenized_sentences\n",
    "df[\"space-after-information\"] = space_after_list\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the df\n",
    "df.to_csv(\"{}\".format(translated_tokenized_dataframe_path), sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>file_path</th>\n",
       "      <th>file</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>NER</th>\n",
       "      <th>proper_nouns</th>\n",
       "      <th>length</th>\n",
       "      <th>translation</th>\n",
       "      <th>translation-tokenized</th>\n",
       "      <th>space-after-information</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>/home/tajak/Parlamint-translation/ParlaMint-CZ...</td>\n",
       "      <td>ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....</td>\n",
       "      <td>ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....</td>\n",
       "      <td>3.</td>\n",
       "      <td>3 .</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{}</td>\n",
       "      <td>1</td>\n",
       "      <td>3.</td>\n",
       "      <td>3 .</td>\n",
       "      <td>[No, Last]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                          file_path  \\\n",
       "0           0  /home/tajak/Parlamint-translation/ParlaMint-CZ...   \n",
       "\n",
       "                                                file  \\\n",
       "0  ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....   \n",
       "\n",
       "                                         sentence_id text tokenized_text  NER  \\\n",
       "0  ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....   3.            3 .  NaN   \n",
       "\n",
       "  proper_nouns  length translation translation-tokenized  \\\n",
       "0           {}       1          3.                   3 .   \n",
       "\n",
       "  space-after-information  \n",
       "0              [No, Last]  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alignment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Perform word alignment.\n",
    "- Save forward and reverse alignment information for each sentence (2 additional columns).\n",
    "- Substitute translated NE words with lemmas based on the annotation, save new translation to a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # Code if we would add the NER information from the source to target\n",
    "# Substitute words in the translation based on alignments\n",
    "intermediate_list = list(zip(df[\"translation-tokenized\"], df[\"proper_nouns\"], df[\"alignments\"], df[\"NER\"]))\n",
    "\n",
    "new_translations = []\n",
    "substituted_all_info = []\n",
    "substituted_only = []\n",
    "NER_alignments = []\n",
    "\n",
    "# Add information whether an error occurred\n",
    "error_list = []\n",
    "\n",
    "for i in intermediate_list[:3]:\n",
    "\tcurrent_substituted_list = []\n",
    "\tcurrent_substituted_only = []\n",
    "\tcurrent_error = \"No\"\n",
    "\n",
    "\t# Create a list of NER alignments - at first, let all elements be \"not NE\", then we will substitute elements with appropriate tags\n",
    "\t# \"O\" is repeated as many times as there are tokens in the translation\n",
    "\tcurrent_NER_list = [\"O\"] * len(intermediate_list[0].split())\n",
    "\n",
    "\t# Loop through the NER list for the source of this sentence\n",
    "\tfor NER_pair in list(i[3].items()):\n",
    "\t\t# If the pair is not \"O\", get the word index\n",
    "\t\tif NER_pair[1] != \"O\":\n",
    "\t\t\tsource_NE_index = NER_pair[0]\n",
    "\n",
    "\t\t\t# Find to which target index it corresponds:\n",
    "\t\t\tsubstituted_word_index = i[2][source_NE_index]\n",
    "\n",
    "\t\t\t# Substitute the element it the NER list under this index with the NE tag\n",
    "\t\t\tcurrent_NER_list[substituted_word_index] = NER_pair[1]\n",
    "\n",
    "\t# If no proper names were detected, do not change the translation\n",
    "\tif i[1] == 0:\n",
    "\t\tnew_translations.append(i[0])\n",
    "\t\n",
    "\telse:\n",
    "\t\tcurrent_translation = i[0]\n",
    "\n",
    "\t\t# Substitute the word with the Slovene lemma based on the index - loop through the proper nouns to be changed\n",
    "\t\tfor word_index in list(i[1].keys()):\n",
    "\t\t\ttry:\n",
    "\t\t\t\t# split the translation into list of words\n",
    "\t\t\t\tword_list = current_translation.split()\n",
    "\n",
    "\t\t\t\t# Get index of the substituted word\n",
    "\t\t\t\tsubstituted_word_index = i[2][word_index]\n",
    "\n",
    "\t\t\t\t# Get the lemma to substitute the word with\n",
    "\t\t\t\tcorrect_lemma = i[1][word_index][1]\n",
    "\n",
    "\t\t\t\t# If the substitute word and lemma are not the same, get substituted word and its match\n",
    "\t\t\t\tif word_list[substituted_word_index] != correct_lemma:\n",
    "\t\t\t\t\tcurrent_substituted_list.append((word_list[substituted_word_index], correct_lemma))\n",
    "\t\t\t\t\tcurrent_substituted_only.append((word_list[substituted_word_index], correct_lemma))\n",
    "\n",
    "\t\t\t\t\t# Substitute the word in the word list\n",
    "\t\t\t\t\tword_list[substituted_word_index] = correct_lemma\n",
    "\t\t\t\t\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\t# Add information that substitution was not performed\n",
    "\t\t\t\t\tcurrent_substituted_list.append(f\"No substitution: {word_list[substituted_word_index], correct_lemma}\")\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Change the translation by merging the words back into a string\n",
    "\t\t\t\tcurrent_translation = \" \".join(word_list)\n",
    "\n",
    "\t\t\texcept:\n",
    "\t\t\t\tprint(f\"Issue: index {word_index}: {i[1][word_index]}\")\n",
    "\t\t\t\tcurrent_error = f\"Issue: index {word_index}: {i[1][word_index]}\"\n",
    "\n",
    "\t\t# After the loop through proper nouns, save the new translation\n",
    "\t\tnew_translations.append(current_translation)\n",
    "\t\n",
    "\t# Add information on what was substituted\n",
    "\tsubstituted_all_info.append(current_substituted_list)\n",
    "\tsubstituted_only.append(current_substituted_only)\n",
    "\terror_list.append(current_error)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_proper_nouns(lang_code):\n",
    "\t\"\"\"\n",
    "\tThis function takes the translated text and the source text, aligns words with eflomal and corrects proper nouns.\n",
    "\tIt takes the dataframe that was created in the function extract_text() and to which the translation was added\n",
    "\tin the function translate().\n",
    "\n",
    "\tTo use eflomal, you need to install it first:\n",
    "\t!git clone https://github.com/robertostling/eflomal\n",
    "\t%cd eflomal\n",
    "\t!make\n",
    "\t!sudo make install\n",
    "\t!python3 setup.py install\n",
    "\n",
    "\tArgs:\n",
    "\t- lang_code: the lang code that is used in the names of the files, it should be the same as for extract_text()\n",
    "\t\"\"\"\n",
    "\timport pandas as pd\n",
    "\timport re\n",
    "\timport ast\n",
    "\tfrom IPython.display import display\n",
    "\n",
    "\t# Open the file, created in the previous step\n",
    "\tdf = pd.read_csv(\"{}\".format(translated_dataframe_path), sep=\"\\t\", index_col=0)\n",
    "\n",
    "\t# Move into the eflomal folder\n",
    "\t%cd /home/tajak/Parlamint-translation/eflomal\n",
    "\n",
    "\t# Then we need to create files for all texts and all translations\n",
    "\tsource_sentences = open(\"source_sentences.txt\", \"w\")\n",
    "\tEnglish_sentences = open(\"English_sentences.txt\", \"w\")\n",
    "\n",
    "\tfor i in df[\"tokenized_text\"].to_list():\n",
    "\t\tsource_sentences.write(i)\n",
    "\t\tsource_sentences.write(\"\\n\")\n",
    "\n",
    "\tfor i in df[\"translation-tokenized\"].to_list():\n",
    "\t\tEnglish_sentences.write(i)\n",
    "\t\tEnglish_sentences.write(\"\\n\")\n",
    "\n",
    "\tsource_sentences.close()\n",
    "\tEnglish_sentences.close()\n",
    "\n",
    "\t# Align sentences with eflomal and get out a file with alignments\n",
    "\t!python3 align.py -s source_sentences.txt -t English_sentences.txt --model 3 -r source-en.rev -f source-en.fwd\n",
    "\n",
    "\t# Create a list of alignments from the returned files which will be added to the final conllu\n",
    "\n",
    "\t# Create target alignments from the source alignment direction (by changing the direction in the fwd file)\n",
    "\taligns_list_target = open(\"source-en.fwd\", \"r\").readlines()\n",
    "\taligns_list_target = [i.replace(\"\\n\", \"\") for i in aligns_list_target]\n",
    "\taligns_list_target = [i.split(\" \") for i in aligns_list_target]\n",
    "\n",
    "\taligns_list_target_final = []\n",
    "\n",
    "\tfor i in aligns_list_target:\n",
    "\t\tcurrent_sentence_align = \"\"\n",
    "\t\tfor pair in i:\n",
    "\t\t\tcurrent_pair = pair.split(\"-\")\n",
    "\t\t\tcurrent_sentence_align += \"{}-{}\".format(current_pair[1], current_pair[0])\n",
    "\t\t\tcurrent_sentence_align += \" \"\n",
    "\t\n",
    "\t\taligns_list_target_final.append(current_sentence_align)\n",
    "\t\n",
    "\t# Add aligns_list to the df\n",
    "\tdf[\"aligns-target\"] = aligns_list_target_final\n",
    "\n",
    "\t# Create a list of alignments for the source file\n",
    "\taligns_list = open(\"source-en.rev\", \"r\").readlines()\n",
    "\taligns_list = [i.replace(\"\\n\", \"\") for i in aligns_list]\n",
    "\n",
    "\t# Add information to be added to the conllu\n",
    "\tdf[\"aligns-source\"] = aligns_list\n",
    "\n",
    "\t# Continue with processing the list to create the final alignments format which I'll use to correct proper names\n",
    "\taligns_list = [i.split(\" \") for i in aligns_list]\n",
    "\n",
    "\tfor i in aligns_list:\n",
    "\t\tfor pair in i:\n",
    "\t\t\tcurrent_pair = pair.split(\"-\")\n",
    "\t\t\ti[i.index(pair)] = {int(current_pair[0]): int(current_pair[1])}\n",
    "\t\n",
    "\tfinal_aligns = []\n",
    "\n",
    "\t# Create a dictionary out of the rev alignments\n",
    "\tfor i in aligns_list:\n",
    "\t\tcurrent_line = {}\n",
    "\n",
    "\t\ttry:\n",
    "\t\t\tfor element in i:\n",
    "\t\t\t\ta = list(element.items())[0][0]\n",
    "\t\t\t\tb = list(element.items())[0][1]\n",
    "\t\t\t\tcurrent_line[a] = b\n",
    "\t\t\n",
    "\t\t\t# Check whether the number of pairs in the list is the same as number of items\n",
    "\t\t\tif len(i) != len(list(current_line.items())):\n",
    "\t\t\t\tprint(\"Not okay:\")\n",
    "\t\t\t\tprint(i)\n",
    "\t\t\t\tprint(current_line)\n",
    "\n",
    "\t\t\tfinal_aligns.append(current_line)\n",
    "\t\t\n",
    "\t\texcept:\n",
    "\t\t\tprint(\"error\")\n",
    "\t\t\tprint(aligns_list.index(i))\n",
    "\t\t\tprint(i)\n",
    "\t\t\tfinal_aligns.append(\"Error\")\n",
    "\t\t\n",
    "\tprint(\"Number of aligned sentences: {}\".format(len(final_aligns)))\n",
    "\n",
    "\t# Add a to the df\n",
    "\tdf[\"alignments\"] = final_aligns\n",
    "\n",
    "\t# Remove the rev and fwd file\n",
    "\t%rm source-en.rev\n",
    "\t%rm source-en.fwd\n",
    "\n",
    "\t# When we open the dataframe file, the dictionaries with proper names changed into strings - Change strings in the column proper_nouns into dictionaries\n",
    "\n",
    "\tdf[\"proper_nouns\"] = df.proper_nouns.astype(\"str\")\n",
    "\tdf[\"proper_nouns\"] = df.proper_nouns.apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "\t# Change nan values in the proper_nouns columns\n",
    "\tdf = df.fillna(0)\n",
    "\n",
    "\t# Substitute words in the translation based on alignments\n",
    "\tintermediate_list = list(zip(df[\"translation-tokenized\"], df[\"proper_nouns\"], df[\"alignments\"]))\n",
    "\n",
    "\tnew_translations = []\n",
    "\tsubstituted_all_info = []\n",
    "\tsubstituted_only = []\n",
    "\n",
    "\t# Add information whether an error occurred\n",
    "\terror_list = []\n",
    "\n",
    "\tfor i in intermediate_list:\n",
    "\t\tcurrent_substituted_list = []\n",
    "\t\tcurrent_substituted_only = []\n",
    "\t\tcurrent_error = \"No\"\n",
    "\n",
    "\t\t# If no proper names were detected, do not change the translation\n",
    "\t\tif i[1] == 0:\n",
    "\t\t\tnew_translations.append(i[0])\n",
    "\t\t\n",
    "\t\telse:\n",
    "\t\t\tcurrent_translation = i[0]\n",
    "\n",
    "\t\t\t# Substitute the word with the source lemma based on the index - loop through the proper nouns to be changed\n",
    "\t\t\tfor word_index in list(i[1].keys()):\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\t# split the translation into list of words\n",
    "\t\t\t\t\tword_list = current_translation.split()\n",
    "\n",
    "\t\t\t\t\t# Get index of the substituted word\n",
    "\t\t\t\t\tsubstituted_word_index = i[2][word_index]\n",
    "\n",
    "\t\t\t\t\t# Get the lemma to substitute the word with\n",
    "\t\t\t\t\tcorrect_lemma = i[1][word_index][1]\n",
    "\n",
    "\t\t\t\t\t# If the substitute word and lemma are not the same, get substituted word and its match\n",
    "\t\t\t\t\tif word_list[substituted_word_index] != correct_lemma:\n",
    "\t\t\t\t\t\tcurrent_substituted_list.append((word_list[substituted_word_index], correct_lemma))\n",
    "\t\t\t\t\t\tcurrent_substituted_only.append((word_list[substituted_word_index], correct_lemma))\n",
    "\n",
    "\t\t\t\t\t\t# Substitute the word in the word list\n",
    "\t\t\t\t\t\tword_list[substituted_word_index] = correct_lemma\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t# Add information that substitution was not performed\n",
    "\t\t\t\t\t\tcurrent_substituted_list.append(f\"No substitution: {word_list[substituted_word_index], correct_lemma}\")\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t# Change the translation by merging the words back into a string\n",
    "\t\t\t\t\tcurrent_translation = \" \".join(word_list)\n",
    "\n",
    "\t\t\t\texcept:\n",
    "\t\t\t\t\tprint(f\"Issue: index {word_index}: {i[1][word_index]}\")\n",
    "\t\t\t\t\tcurrent_error = f\"Issue: index {word_index}: {i[1][word_index]}\"\n",
    "\n",
    "\t\t\t# After the loop through proper nouns, save the new translation\n",
    "\t\t\tnew_translations.append(current_translation)\n",
    "\t\t\n",
    "\t\t# Add information on what was substituted\n",
    "\t\tif len(substituted_all_info) != 0:\n",
    "\t\t\tsubstituted_all_info.append(current_substituted_list)\n",
    "\t\telse:\n",
    "\t\t\tsubstituted_all_info.append(0)\n",
    "\n",
    "\t\tif len(current_substituted_only) != 0:\n",
    "\t\t\tsubstituted_only.append(current_substituted_only)\n",
    "\t\telse:\n",
    "\t\t\tsubstituted_only.append(0)\n",
    "\n",
    "\t\terror_list.append(current_error)\n",
    "\n",
    "\n",
    "\t# Add to the df\n",
    "\tdf[\"new_translations\"] = new_translations\n",
    "\tdf[\"substitution_info\"] = substituted_all_info\n",
    "\tdf[\"substituted_words\"] = substituted_only\n",
    "\tdf[\"errors\"] = error_list\n",
    "\n",
    "\t# Change the working directory once again\n",
    "\t%cd ..\n",
    "\n",
    "\t# Save the df\n",
    "\tdf.to_csv(\"{}\".format(final_dataframe), sep=\"\\t\")\n",
    "\n",
    "\t# Display most common substitutions\n",
    "\tdf_substituted = df[df[\"proper_nouns\"] != \"0\"]\n",
    "\tdisplay(df_substituted.substituted_words.value_counts()[:20])\n",
    "\n",
    "\treturn df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tajak/Parlamint-translation/eflomal\n",
      "Number of aligned sentences: 15117\n",
      "/home/tajak/Parlamint-translation\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.map_locations\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'pandas._libs.index.IndexEngine._call_map_locations'\n",
      "Traceback (most recent call last):\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5231, in pandas._libs.hashtable.PyObjectHashTable.map_locations\n",
      "TypeError: unhashable type: 'list'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                               14527\n",
       "[(Faltynek, Faltýnek)]             26\n",
       "[(Laudat, Laudát)]                 15\n",
       "[(Vladimir, Vladimír)]             12\n",
       "[(Philip, Filip)]                  11\n",
       "[(Shincl, Šincl)]                   9\n",
       "[(Peter, Petr)]                     8\n",
       "[(Semel, Semelová)]                 8\n",
       "[(Jerome, Jeroným)]                 7\n",
       "[(Mark, Marková)]                   7\n",
       "[(Mark, Marek)]                     7\n",
       "[(Wark, Válková)]                   7\n",
       "[(Zaoralek, Zaorálek)]              6\n",
       "[(Zlatuska, Zlatuška)]              6\n",
       "[(Putn, Putnová)]                   6\n",
       "[(Free, Volný)]                     5\n",
       "[(Sedya, Seďa)]                     5\n",
       "[(Jerman, Jermanová)]               5\n",
       "[(Vanya, Váňa)]                     5\n",
       "[(Kolorodětík, Kolovratník)]        5\n",
       "Name: substituted_words, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = correct_proper_nouns(lang_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>file_path</th>\n",
       "      <th>file</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>NER</th>\n",
       "      <th>proper_nouns</th>\n",
       "      <th>length</th>\n",
       "      <th>translation</th>\n",
       "      <th>translation-tokenized</th>\n",
       "      <th>space-after-information</th>\n",
       "      <th>aligns-target</th>\n",
       "      <th>aligns-source</th>\n",
       "      <th>alignments</th>\n",
       "      <th>new_translations</th>\n",
       "      <th>substitution_info</th>\n",
       "      <th>substituted_words</th>\n",
       "      <th>errors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>/home/tajak/Parlamint-translation/ParlaMint-CZ...</td>\n",
       "      <td>ParlaMint-CZ_2013-12-10-ps2013-004-01-015-024....</td>\n",
       "      <td>ParlaMint-CZ_2013-12-10-ps2013-004-01-015-024....</td>\n",
       "      <td>Děkuji panu ministru Martinu Pecinovi a nyní p...</td>\n",
       "      <td>Děkuji panu ministru Martinu Pecinovi a nyní p...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{3: ['Martinu', 'Martin'], 4: ['Pecinovi', 'Pe...</td>\n",
       "      <td>22</td>\n",
       "      <td>Thank you to Minister Martin Pecin and I will ...</td>\n",
       "      <td>Thank you to Minister Martin Pecin and I will ...</td>\n",
       "      <td>['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Ye...</td>\n",
       "      <td>0-0 1-0 2-1 3-2 4-3 5-4 6-5 9-6 10-7 12-8 15-9...</td>\n",
       "      <td>0-0 1-1 2-3 3-4 4-5 5-6 6-9 7-10 8-12 9-15 10-...</td>\n",
       "      <td>{0: 0, 1: 1, 2: 3, 3: 4, 4: 5, 5: 6, 6: 9, 7: ...</td>\n",
       "      <td>Thank you to Minister Martin Pecin and I will ...</td>\n",
       "      <td>[No substitution: ('Martin', 'Martin'), No sub...</td>\n",
       "      <td>[(Koniček, Koníček)]</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>/home/tajak/Parlamint-translation/ParlaMint-CZ...</td>\n",
       "      <td>ParlaMint-CZ_2013-12-10-ps2013-004-01-015-024....</td>\n",
       "      <td>ParlaMint-CZ_2013-12-10-ps2013-004-01-015-024....</td>\n",
       "      <td>Vážený pane místopředsedo, dovoluji si navrhno...</td>\n",
       "      <td>Vážený pane místopředsedo , dovoluji si navrhn...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{9: ['Vladimíra', 'Vladimír'], 10: ['Koníčka',...</td>\n",
       "      <td>10</td>\n",
       "      <td>Mr Vice-President, I would like to propose Mr ...</td>\n",
       "      <td>Mr Vice - President , I would like to propose ...</td>\n",
       "      <td>['Yes', 'No', 'No', 'No', 'Yes', 'Yes', 'Yes',...</td>\n",
       "      <td>0-1 1-2 2-2 3-2 4-3 6-4 7-5 9-6 10-8 11-9 12-1...</td>\n",
       "      <td>1-0 2-1 3-4 4-7 5-8 6-9 7-10 9-11 10-12 11-13</td>\n",
       "      <td>{1: 0, 2: 1, 3: 4, 4: 7, 5: 8, 6: 9, 7: 10, 9:...</td>\n",
       "      <td>Mr Vice - President , I would like to propose ...</td>\n",
       "      <td>[No substitution: ('Vladimír', 'Vladimír'), (K...</td>\n",
       "      <td>[(Koniček, Koníček)]</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>101</td>\n",
       "      <td>/home/tajak/Parlamint-translation/ParlaMint-CZ...</td>\n",
       "      <td>ParlaMint-CZ_2013-12-10-ps2013-004-01-015-024....</td>\n",
       "      <td>ParlaMint-CZ_2013-12-10-ps2013-004-01-015-024....</td>\n",
       "      <td>Ptám se, kdo je pro to, aby zpravodajem pro pr...</td>\n",
       "      <td>Ptám se , kdo je pro to , aby by zpravodajem p...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{17: ['Vladimír', 'Vladimír'], 18: ['Koníček',...</td>\n",
       "      <td>16</td>\n",
       "      <td>I ask who is in favour of the rapporteur for f...</td>\n",
       "      <td>I ask who is in favour of the rapporteur for f...</td>\n",
       "      <td>['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Ye...</td>\n",
       "      <td>0-0 1-1 2-3 3-4 5-5 7-9 8-10 9-11 10-12 11-13 ...</td>\n",
       "      <td>0-1 3-2 4-3 5-5 9-7 10-8 11-9 12-10 13-11 14-1...</td>\n",
       "      <td>{0: 1, 3: 2, 4: 3, 5: 5, 9: 7, 10: 8, 11: 9, 1...</td>\n",
       "      <td>I ask who is in favour of the rapporteur for f...</td>\n",
       "      <td>[(Vladimir, Vladimír), No substitution: ('Koní...</td>\n",
       "      <td>[(Vladimir, Vladimír)]</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>106</td>\n",
       "      <td>/home/tajak/Parlamint-translation/ParlaMint-CZ...</td>\n",
       "      <td>ParlaMint-CZ_2013-12-10-ps2013-004-01-015-024....</td>\n",
       "      <td>ParlaMint-CZ_2013-12-10-ps2013-004-01-015-024....</td>\n",
       "      <td>Pan Vladimír Koníček se stal zpravodajem pro p...</td>\n",
       "      <td>Pan Vladimír Koníček se stal zpravodajem pro p...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{1: ['Vladimír', 'Vladimír'], 2: ['Koníček', '...</td>\n",
       "      <td>9</td>\n",
       "      <td>Mr Vladimir Koníček became rapporteur for firs...</td>\n",
       "      <td>Mr Vladimir Koníček became rapporteur for firs...</td>\n",
       "      <td>['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Ye...</td>\n",
       "      <td>0-0 1-1 2-2 3-4 4-5 5-6 6-7 7-8 8-9</td>\n",
       "      <td>0-0 1-1 2-2 4-3 5-4 6-5 7-6 8-7 9-8</td>\n",
       "      <td>{0: 0, 1: 1, 2: 2, 4: 3, 5: 4, 6: 5, 7: 6, 8: ...</td>\n",
       "      <td>Mr Vladimír Koníček became rapporteur for firs...</td>\n",
       "      <td>[(Vladimir, Vladimír), No substitution: ('Koní...</td>\n",
       "      <td>[(Vladimir, Vladimír)]</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>174</td>\n",
       "      <td>/home/tajak/Parlamint-translation/ParlaMint-CZ...</td>\n",
       "      <td>ParlaMint-CZ_2013-12-10-ps2013-004-01-019-021....</td>\n",
       "      <td>ParlaMint-CZ_2013-12-10-ps2013-004-01-019-021....</td>\n",
       "      <td>Nyní prosím předsedu výboru pro zdravotnictví ...</td>\n",
       "      <td>Nyní prosím předsedu výboru pro zdravotnictví ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{7: ['Rostislava', 'Rostislav'], 8: ['Vyzulu',...</td>\n",
       "      <td>16</td>\n",
       "      <td>I now ask the chairman of the Committee on Hea...</td>\n",
       "      <td>I now ask the chairman of the Committee on Hea...</td>\n",
       "      <td>['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Ye...</td>\n",
       "      <td>1-0 2-1 4-2 7-3 8-4 9-5 11-6 12-7 13-8 14-9 15...</td>\n",
       "      <td>0-1 1-2 2-4 3-7 4-8 5-9 6-11 7-12 8-13 9-14 10...</td>\n",
       "      <td>{0: 1, 1: 2, 2: 4, 3: 7, 4: 8, 5: 9, 6: 11, 7:...</td>\n",
       "      <td>I now ask the chairman of the Committee on Hea...</td>\n",
       "      <td>[No substitution: ('Rostislav', 'Rostislav'), ...</td>\n",
       "      <td>[(Vyzulu, Vyzula)]</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0.1                                          file_path  \\\n",
       "96             96  /home/tajak/Parlamint-translation/ParlaMint-CZ...   \n",
       "98             98  /home/tajak/Parlamint-translation/ParlaMint-CZ...   \n",
       "101           101  /home/tajak/Parlamint-translation/ParlaMint-CZ...   \n",
       "106           106  /home/tajak/Parlamint-translation/ParlaMint-CZ...   \n",
       "174           174  /home/tajak/Parlamint-translation/ParlaMint-CZ...   \n",
       "\n",
       "                                                  file  \\\n",
       "96   ParlaMint-CZ_2013-12-10-ps2013-004-01-015-024....   \n",
       "98   ParlaMint-CZ_2013-12-10-ps2013-004-01-015-024....   \n",
       "101  ParlaMint-CZ_2013-12-10-ps2013-004-01-015-024....   \n",
       "106  ParlaMint-CZ_2013-12-10-ps2013-004-01-015-024....   \n",
       "174  ParlaMint-CZ_2013-12-10-ps2013-004-01-019-021....   \n",
       "\n",
       "                                           sentence_id  \\\n",
       "96   ParlaMint-CZ_2013-12-10-ps2013-004-01-015-024....   \n",
       "98   ParlaMint-CZ_2013-12-10-ps2013-004-01-015-024....   \n",
       "101  ParlaMint-CZ_2013-12-10-ps2013-004-01-015-024....   \n",
       "106  ParlaMint-CZ_2013-12-10-ps2013-004-01-015-024....   \n",
       "174  ParlaMint-CZ_2013-12-10-ps2013-004-01-019-021....   \n",
       "\n",
       "                                                  text  \\\n",
       "96   Děkuji panu ministru Martinu Pecinovi a nyní p...   \n",
       "98   Vážený pane místopředsedo, dovoluji si navrhno...   \n",
       "101  Ptám se, kdo je pro to, aby zpravodajem pro pr...   \n",
       "106  Pan Vladimír Koníček se stal zpravodajem pro p...   \n",
       "174  Nyní prosím předsedu výboru pro zdravotnictví ...   \n",
       "\n",
       "                                        tokenized_text  NER  \\\n",
       "96   Děkuji panu ministru Martinu Pecinovi a nyní p...  0.0   \n",
       "98   Vážený pane místopředsedo , dovoluji si navrhn...  0.0   \n",
       "101  Ptám se , kdo je pro to , aby by zpravodajem p...  0.0   \n",
       "106  Pan Vladimír Koníček se stal zpravodajem pro p...  0.0   \n",
       "174  Nyní prosím předsedu výboru pro zdravotnictví ...  0.0   \n",
       "\n",
       "                                          proper_nouns  length  \\\n",
       "96   {3: ['Martinu', 'Martin'], 4: ['Pecinovi', 'Pe...      22   \n",
       "98   {9: ['Vladimíra', 'Vladimír'], 10: ['Koníčka',...      10   \n",
       "101  {17: ['Vladimír', 'Vladimír'], 18: ['Koníček',...      16   \n",
       "106  {1: ['Vladimír', 'Vladimír'], 2: ['Koníček', '...       9   \n",
       "174  {7: ['Rostislava', 'Rostislav'], 8: ['Vyzulu',...      16   \n",
       "\n",
       "                                           translation  \\\n",
       "96   Thank you to Minister Martin Pecin and I will ...   \n",
       "98   Mr Vice-President, I would like to propose Mr ...   \n",
       "101  I ask who is in favour of the rapporteur for f...   \n",
       "106  Mr Vladimir Koníček became rapporteur for firs...   \n",
       "174  I now ask the chairman of the Committee on Hea...   \n",
       "\n",
       "                                 translation-tokenized  \\\n",
       "96   Thank you to Minister Martin Pecin and I will ...   \n",
       "98   Mr Vice - President , I would like to propose ...   \n",
       "101  I ask who is in favour of the rapporteur for f...   \n",
       "106  Mr Vladimir Koníček became rapporteur for firs...   \n",
       "174  I now ask the chairman of the Committee on Hea...   \n",
       "\n",
       "                               space-after-information  \\\n",
       "96   ['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Ye...   \n",
       "98   ['Yes', 'No', 'No', 'No', 'Yes', 'Yes', 'Yes',...   \n",
       "101  ['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Ye...   \n",
       "106  ['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Ye...   \n",
       "174  ['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Ye...   \n",
       "\n",
       "                                         aligns-target  \\\n",
       "96   0-0 1-0 2-1 3-2 4-3 5-4 6-5 9-6 10-7 12-8 15-9...   \n",
       "98   0-1 1-2 2-2 3-2 4-3 6-4 7-5 9-6 10-8 11-9 12-1...   \n",
       "101  0-0 1-1 2-3 3-4 5-5 7-9 8-10 9-11 10-12 11-13 ...   \n",
       "106               0-0 1-1 2-2 3-4 4-5 5-6 6-7 7-8 8-9    \n",
       "174  1-0 2-1 4-2 7-3 8-4 9-5 11-6 12-7 13-8 14-9 15...   \n",
       "\n",
       "                                         aligns-source  \\\n",
       "96   0-0 1-1 2-3 3-4 4-5 5-6 6-9 7-10 8-12 9-15 10-...   \n",
       "98       1-0 2-1 3-4 4-7 5-8 6-9 7-10 9-11 10-12 11-13   \n",
       "101  0-1 3-2 4-3 5-5 9-7 10-8 11-9 12-10 13-11 14-1...   \n",
       "106                0-0 1-1 2-2 4-3 5-4 6-5 7-6 8-7 9-8   \n",
       "174  0-1 1-2 2-4 3-7 4-8 5-9 6-11 7-12 8-13 9-14 10...   \n",
       "\n",
       "                                            alignments  \\\n",
       "96   {0: 0, 1: 1, 2: 3, 3: 4, 4: 5, 5: 6, 6: 9, 7: ...   \n",
       "98   {1: 0, 2: 1, 3: 4, 4: 7, 5: 8, 6: 9, 7: 10, 9:...   \n",
       "101  {0: 1, 3: 2, 4: 3, 5: 5, 9: 7, 10: 8, 11: 9, 1...   \n",
       "106  {0: 0, 1: 1, 2: 2, 4: 3, 5: 4, 6: 5, 7: 6, 8: ...   \n",
       "174  {0: 1, 1: 2, 2: 4, 3: 7, 4: 8, 5: 9, 6: 11, 7:...   \n",
       "\n",
       "                                      new_translations  \\\n",
       "96   Thank you to Minister Martin Pecin and I will ...   \n",
       "98   Mr Vice - President , I would like to propose ...   \n",
       "101  I ask who is in favour of the rapporteur for f...   \n",
       "106  Mr Vladimír Koníček became rapporteur for firs...   \n",
       "174  I now ask the chairman of the Committee on Hea...   \n",
       "\n",
       "                                     substitution_info  \\\n",
       "96   [No substitution: ('Martin', 'Martin'), No sub...   \n",
       "98   [No substitution: ('Vladimír', 'Vladimír'), (K...   \n",
       "101  [(Vladimir, Vladimír), No substitution: ('Koní...   \n",
       "106  [(Vladimir, Vladimír), No substitution: ('Koní...   \n",
       "174  [No substitution: ('Rostislav', 'Rostislav'), ...   \n",
       "\n",
       "          substituted_words errors  \n",
       "96     [(Koniček, Koníček)]     No  \n",
       "98     [(Koniček, Koníček)]     No  \n",
       "101  [(Vladimir, Vladimír)]     No  \n",
       "106  [(Vladimir, Vladimír)]     No  \n",
       "174      [(Vyzulu, Vyzula)]     No  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"substituted_words\"]!= 0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 19)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"errors\"]!=\"No\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0.1': 11481,\n",
       " 'file_path': '/home/tajak/Parlamint-translation/ParlaMint-CZ/ParlaMint-CZ.conllu/ParlaMint-CZ.conllu/2013/ParlaMint-CZ_2013-12-10-ps2013-004-01-001-001.conllu',\n",
       " 'file': 'ParlaMint-CZ_2013-12-10-ps2013-004-01-001-001.conllu',\n",
       " 'sentence_id': 'ParlaMint-CZ_2013-12-10-ps2013-004-01-001-001.u2.p2.s3',\n",
       " 'text': 'Význam Nelsona R. Mandely přesáhl hranice Jihoafrické republiky a oslovil globální společenství svým nezaměnitelným poselstvím lidské důstojnosti, svobody, statečnosti a státnické rozvahy.',\n",
       " 'tokenized_text': 'Význam Nelsona R . Mandely přesáhl hranice Jihoafrické republiky a oslovil globální společenství svým nezaměnitelným poselstvím lidské důstojnosti , svobody , statečnosti a státnické rozvahy .',\n",
       " 'NER': 0.0,\n",
       " 'proper_nouns': {1: ['Nelsona', 'Nelson'],\n",
       "  2: ['R', 'R'],\n",
       "  3: ['.', '.'],\n",
       "  4: ['Mandely', 'Mandela']},\n",
       " 'length': 22,\n",
       " 'translation': 'The importance of Nelson R. Mandela exceeded the borders of South Africa and addressed the global community with its unmistakable message of human dignity, freedom, bravery and state balance sheet.',\n",
       " 'translation-tokenized': 'The importance of Nelson R. Mandela exceeded the borders of South Africa and addressed the global community with its unmistakable message of human dignity , freedom , bravery and state balance sheet .',\n",
       " 'space-after-information': \"['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Last']\",\n",
       " 'aligns-target': '1-0 3-1 4-2 5-4 6-5 8-6 10-7 11-8 12-9 13-10 15-11 16-12 18-13 19-14 20-15 22-16 23-17 24-18 25-19 26-20 27-21 28-22 29-23 30-23 31-23 32-25 ',\n",
       " 'aligns-source': '0-1 1-3 2-4 4-5 5-6 6-8 7-10 8-11 9-12 10-13 11-15 12-16 13-18 14-19 15-20 16-22 17-23 18-24 19-25 20-26 21-27 22-28 23-29 24-30 25-32',\n",
       " 'alignments': {0: 1,\n",
       "  1: 3,\n",
       "  2: 4,\n",
       "  4: 5,\n",
       "  5: 6,\n",
       "  6: 8,\n",
       "  7: 10,\n",
       "  8: 11,\n",
       "  9: 12,\n",
       "  10: 13,\n",
       "  11: 15,\n",
       "  12: 16,\n",
       "  13: 18,\n",
       "  14: 19,\n",
       "  15: 20,\n",
       "  16: 22,\n",
       "  17: 23,\n",
       "  18: 24,\n",
       "  19: 25,\n",
       "  20: 26,\n",
       "  21: 27,\n",
       "  22: 28,\n",
       "  23: 29,\n",
       "  24: 30,\n",
       "  25: 32},\n",
       " 'new_translations': 'The importance of Nelson R Mandela exceeded the borders of South Africa and addressed the global community with its unmistakable message of human dignity , freedom , bravery and state balance sheet .',\n",
       " 'substitution_info': [\"No substitution: ('Nelson', 'Nelson')\",\n",
       "  ('R.', 'R'),\n",
       "  \"No substitution: ('Mandela', 'Mandela')\"],\n",
       " 'substituted_words': [('R.', 'R')],\n",
       " 'errors': \"Issue: index 3: ['.', '.']\"}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyse errors\n",
    "df[df[\"errors\"] != \"No\"].iloc[9].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>file_path</th>\n",
       "      <th>file</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>NER</th>\n",
       "      <th>proper_nouns</th>\n",
       "      <th>length</th>\n",
       "      <th>translation</th>\n",
       "      <th>translation-tokenized</th>\n",
       "      <th>space-after-information</th>\n",
       "      <th>aligns-target</th>\n",
       "      <th>aligns-source</th>\n",
       "      <th>alignments</th>\n",
       "      <th>new_translations</th>\n",
       "      <th>substitution_info</th>\n",
       "      <th>substituted_words</th>\n",
       "      <th>errors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>15117.000000</td>\n",
       "      <td>15117</td>\n",
       "      <td>15117</td>\n",
       "      <td>15117</td>\n",
       "      <td>15117</td>\n",
       "      <td>15117</td>\n",
       "      <td>15117.0</td>\n",
       "      <td>15117</td>\n",
       "      <td>15117.000000</td>\n",
       "      <td>15117</td>\n",
       "      <td>15117</td>\n",
       "      <td>15117</td>\n",
       "      <td>15117</td>\n",
       "      <td>15117</td>\n",
       "      <td>15117</td>\n",
       "      <td>15117</td>\n",
       "      <td>15117</td>\n",
       "      <td>15117.0</td>\n",
       "      <td>15117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>15117</td>\n",
       "      <td>12796</td>\n",
       "      <td>12796</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1610</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12660</td>\n",
       "      <td>12660</td>\n",
       "      <td>5826</td>\n",
       "      <td>11276</td>\n",
       "      <td>11221</td>\n",
       "      <td>11221</td>\n",
       "      <td>12659</td>\n",
       "      <td>894</td>\n",
       "      <td>330.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>/home/tajak/Parlamint-translation/ParlaMint-CZ...</td>\n",
       "      <td>ParlaMint-CZ_2013-12-16-ps2013-004-03-003-018....</td>\n",
       "      <td>ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....</td>\n",
       "      <td>Děkuji.</td>\n",
       "      <td>Děkuji .</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thank you.</td>\n",
       "      <td>Thank you .</td>\n",
       "      <td>['Yes', 'No', 'Last']</td>\n",
       "      <td>0-0 1-0 2-1</td>\n",
       "      <td>0-0 1-2</td>\n",
       "      <td>{0: 0, 1: 2}</td>\n",
       "      <td>Thank you .</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1506</td>\n",
       "      <td>1506</td>\n",
       "      <td>1</td>\n",
       "      <td>516</td>\n",
       "      <td>516</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13281</td>\n",
       "      <td>NaN</td>\n",
       "      <td>601</td>\n",
       "      <td>601</td>\n",
       "      <td>825</td>\n",
       "      <td>524</td>\n",
       "      <td>538</td>\n",
       "      <td>538</td>\n",
       "      <td>601</td>\n",
       "      <td>13280</td>\n",
       "      <td>14527.0</td>\n",
       "      <td>15117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7558.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.111001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4364.046345</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.674160</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3779.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7558.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>11337.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>15116.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>193.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0.1                                          file_path  \\\n",
       "count   15117.000000                                              15117   \n",
       "unique           NaN                                                100   \n",
       "top              NaN  /home/tajak/Parlamint-translation/ParlaMint-CZ...   \n",
       "freq             NaN                                               1506   \n",
       "mean     7558.000000                                                NaN   \n",
       "std      4364.046345                                                NaN   \n",
       "min         0.000000                                                NaN   \n",
       "25%      3779.000000                                                NaN   \n",
       "50%      7558.000000                                                NaN   \n",
       "75%     11337.000000                                                NaN   \n",
       "max     15116.000000                                                NaN   \n",
       "\n",
       "                                                     file  \\\n",
       "count                                               15117   \n",
       "unique                                                100   \n",
       "top     ParlaMint-CZ_2013-12-16-ps2013-004-03-003-018....   \n",
       "freq                                                 1506   \n",
       "mean                                                  NaN   \n",
       "std                                                   NaN   \n",
       "min                                                   NaN   \n",
       "25%                                                   NaN   \n",
       "50%                                                   NaN   \n",
       "75%                                                   NaN   \n",
       "max                                                   NaN   \n",
       "\n",
       "                                              sentence_id     text  \\\n",
       "count                                               15117    15117   \n",
       "unique                                              15117    12796   \n",
       "top     ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....  Děkuji.   \n",
       "freq                                                    1      516   \n",
       "mean                                                  NaN      NaN   \n",
       "std                                                   NaN      NaN   \n",
       "min                                                   NaN      NaN   \n",
       "25%                                                   NaN      NaN   \n",
       "50%                                                   NaN      NaN   \n",
       "75%                                                   NaN      NaN   \n",
       "max                                                   NaN      NaN   \n",
       "\n",
       "       tokenized_text      NER proper_nouns        length translation  \\\n",
       "count           15117  15117.0        15117  15117.000000       15117   \n",
       "unique          12796      NaN         1610           NaN       12660   \n",
       "top          Děkuji .      NaN           {}           NaN  Thank you.   \n",
       "freq              516      NaN        13281           NaN         601   \n",
       "mean              NaN      0.0          NaN     15.111001         NaN   \n",
       "std               NaN      0.0          NaN     13.674160         NaN   \n",
       "min               NaN      0.0          NaN      1.000000         NaN   \n",
       "25%               NaN      0.0          NaN      5.000000         NaN   \n",
       "50%               NaN      0.0          NaN     11.000000         NaN   \n",
       "75%               NaN      0.0          NaN     21.000000         NaN   \n",
       "max               NaN      0.0          NaN    193.000000         NaN   \n",
       "\n",
       "       translation-tokenized space-after-information aligns-target  \\\n",
       "count                  15117                   15117         15117   \n",
       "unique                 12660                    5826         11276   \n",
       "top              Thank you .   ['Yes', 'No', 'Last']  0-0 1-0 2-1    \n",
       "freq                     601                     825           524   \n",
       "mean                     NaN                     NaN           NaN   \n",
       "std                      NaN                     NaN           NaN   \n",
       "min                      NaN                     NaN           NaN   \n",
       "25%                      NaN                     NaN           NaN   \n",
       "50%                      NaN                     NaN           NaN   \n",
       "75%                      NaN                     NaN           NaN   \n",
       "max                      NaN                     NaN           NaN   \n",
       "\n",
       "       aligns-source    alignments new_translations substitution_info  \\\n",
       "count          15117         15117            15117             15117   \n",
       "unique         11221         11221            12659               894   \n",
       "top          0-0 1-2  {0: 0, 1: 2}      Thank you .                []   \n",
       "freq             538           538              601             13280   \n",
       "mean             NaN           NaN              NaN               NaN   \n",
       "std              NaN           NaN              NaN               NaN   \n",
       "min              NaN           NaN              NaN               NaN   \n",
       "25%              NaN           NaN              NaN               NaN   \n",
       "50%              NaN           NaN              NaN               NaN   \n",
       "75%              NaN           NaN              NaN               NaN   \n",
       "max              NaN           NaN              NaN               NaN   \n",
       "\n",
       "        substituted_words errors  \n",
       "count             15117.0  15117  \n",
       "unique              330.0      1  \n",
       "top                   0.0     No  \n",
       "freq              14527.0  15117  \n",
       "mean                  NaN    NaN  \n",
       "std                   NaN    NaN  \n",
       "min                   NaN    NaN  \n",
       "25%                   NaN    NaN  \n",
       "50%                   NaN    NaN  \n",
       "75%                   NaN    NaN  \n",
       "max                   NaN    NaN  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "228433"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.length.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.map_locations\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'pandas._libs.index.IndexEngine._call_map_locations'\n",
      "Traceback (most recent call last):\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5231, in pandas._libs.hashtable.PyObjectHashTable.map_locations\n",
      "TypeError: unhashable type: 'list'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                               14527\n",
       "[(Faltynek, Faltýnek)]             26\n",
       "[(Laudat, Laudát)]                 15\n",
       "[(Vladimir, Vladimír)]             12\n",
       "[(Philip, Filip)]                  11\n",
       "[(Shincl, Šincl)]                   9\n",
       "[(Peter, Petr)]                     8\n",
       "[(Semel, Semelová)]                 8\n",
       "[(Jerome, Jeroným)]                 7\n",
       "[(Mark, Marková)]                   7\n",
       "[(Mark, Marek)]                     7\n",
       "[(Wark, Válková)]                   7\n",
       "[(Zaoralek, Zaorálek)]              6\n",
       "[(Zlatuska, Zlatuška)]              6\n",
       "[(Putn, Putnová)]                   6\n",
       "[(Free, Volný)]                     5\n",
       "[(Sedya, Seďa)]                     5\n",
       "[(Jerman, Jermanová)]               5\n",
       "[(Vanya, Váňa)]                     5\n",
       "[(Kolorodětík, Kolovratník)]        5\n",
       "Name: substituted_words, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.substituted_words.value_counts()[:20]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linguistic processing of translated text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will have to be done for each file separately - from now onwards, we need to separate the df into files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open df\n",
    "df = pd.read_csv(\"{}\".format(final_dataframe), sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003.conllu',\n",
       " 'ParlaMint-CZ_2013-12-10-ps2013-004-01-004-003.conllu',\n",
       " 'ParlaMint-CZ_2013-12-10-ps2013-004-01-015-024.conllu',\n",
       " 'ParlaMint-CZ_2013-12-10-ps2013-004-01-019-021.conllu',\n",
       " 'ParlaMint-CZ_2013-11-27-ps2013-001-02-015-020.conllu',\n",
       " 'ParlaMint-CZ_2013-12-12-ps2013-004-02-002-029.conllu',\n",
       " 'ParlaMint-CZ_2013-11-27-ps2013-001-02-001-006.conllu',\n",
       " 'ParlaMint-CZ_2013-12-10-ps2013-004-01-013-028.conllu',\n",
       " 'ParlaMint-CZ_2013-12-19-ps2013-004-04-008-038.conllu',\n",
       " 'ParlaMint-CZ_2013-12-19-ps2013-004-04-009-042.conllu',\n",
       " 'ParlaMint-CZ_2013-12-04-ps2013-002-01-006-006.conllu',\n",
       " 'ParlaMint-CZ_2013-12-06-ps2013-002-02-001-007.conllu',\n",
       " 'ParlaMint-CZ_2013-12-10-ps2013-004-01-006-005.conllu',\n",
       " 'ParlaMint-CZ_2013-12-19-ps2013-004-04-004-034.conllu',\n",
       " 'ParlaMint-CZ_2013-11-25-ps2013-001-01-001-001.conllu',\n",
       " 'ParlaMint-CZ_2013-11-27-ps2013-001-02-010-015.conllu',\n",
       " 'ParlaMint-CZ_2013-12-12-ps2013-004-02-005-045.conllu',\n",
       " 'ParlaMint-CZ_2013-12-10-ps2013-004-01-012-023.conllu',\n",
       " 'ParlaMint-CZ_2013-12-06-ps2013-002-02-002-008.conllu',\n",
       " 'ParlaMint-CZ_2013-12-06-ps2013-003-01-000-000.conllu',\n",
       " 'ParlaMint-CZ_2013-11-27-ps2013-001-02-007-012.conllu',\n",
       " 'ParlaMint-CZ_2013-11-27-ps2013-001-02-014-017.conllu',\n",
       " 'ParlaMint-CZ_2013-12-12-ps2013-004-02-006-010.conllu',\n",
       " 'ParlaMint-CZ_2013-12-16-ps2013-004-03-000-000.conllu',\n",
       " 'ParlaMint-CZ_2013-12-16-ps2013-004-03-003-018.conllu',\n",
       " 'ParlaMint-CZ_2013-12-10-ps2013-004-01-010-009.conllu',\n",
       " 'ParlaMint-CZ_2013-12-19-ps2013-004-04-005-035.conllu',\n",
       " 'ParlaMint-CZ_2013-11-25-ps2013-001-01-005-005.conllu',\n",
       " 'ParlaMint-CZ_2013-12-12-ps2013-004-02-014-032.conllu',\n",
       " 'ParlaMint-CZ_2013-11-27-ps2013-001-02-012-018.conllu',\n",
       " 'ParlaMint-CZ_2013-12-04-ps2013-002-01-002-002.conllu',\n",
       " 'ParlaMint-CZ_2013-12-12-ps2013-004-02-008-012.conllu',\n",
       " 'ParlaMint-CZ_2013-12-19-ps2013-004-04-011-050.conllu',\n",
       " 'ParlaMint-CZ_2013-12-12-ps2013-004-02-011-015.conllu',\n",
       " 'ParlaMint-CZ_2013-12-10-ps2013-004-01-008-007.conllu',\n",
       " 'ParlaMint-CZ_2013-11-27-ps2013-001-02-000-000.conllu',\n",
       " 'ParlaMint-CZ_2013-12-10-ps2013-004-01-003-002.conllu',\n",
       " 'ParlaMint-CZ_2013-12-19-ps2013-004-04-000-000.conllu',\n",
       " 'ParlaMint-CZ_2013-12-12-ps2013-004-02-010-014.conllu',\n",
       " 'ParlaMint-CZ_2013-12-10-ps2013-004-01-014-022.conllu',\n",
       " 'ParlaMint-CZ_2013-11-25-ps2013-001-01-003-003.conllu',\n",
       " 'ParlaMint-CZ_2013-11-27-ps2013-001-02-016-021.conllu',\n",
       " 'ParlaMint-CZ_2013-12-06-ps2013-002-02-003-009.conllu',\n",
       " 'ParlaMint-CZ_2013-11-27-ps2013-001-02-004-009.conllu',\n",
       " 'ParlaMint-CZ_2013-12-06-ps2013-002-02-000-000.conllu',\n",
       " 'ParlaMint-CZ_2013-12-10-ps2013-004-01-011-019.conllu',\n",
       " 'ParlaMint-CZ_2013-12-16-ps2013-004-03-002-046.conllu',\n",
       " 'ParlaMint-CZ_2013-12-12-ps2013-004-02-007-011.conllu',\n",
       " 'ParlaMint-CZ_2013-12-06-ps2013-003-01-001-001.conllu',\n",
       " 'ParlaMint-CZ_2013-12-19-ps2013-004-04-002-041.conllu',\n",
       " 'ParlaMint-CZ_2013-12-10-ps2013-004-01-007-006.conllu',\n",
       " 'ParlaMint-CZ_2013-11-27-ps2013-001-02-003-008.conllu',\n",
       " 'ParlaMint-CZ_2013-12-12-ps2013-004-02-009-013.conllu',\n",
       " 'ParlaMint-CZ_2013-12-04-ps2013-002-01-000-000.conllu',\n",
       " 'ParlaMint-CZ_2013-11-27-ps2013-001-02-009-014.conllu',\n",
       " 'ParlaMint-CZ_2013-11-27-ps2013-001-02-013-019.conllu',\n",
       " 'ParlaMint-CZ_2013-12-19-ps2013-004-04-003-047.conllu',\n",
       " 'ParlaMint-CZ_2013-11-27-ps2013-001-02-008-013.conllu',\n",
       " 'ParlaMint-CZ_2013-12-12-ps2013-004-02-003-030.conllu',\n",
       " 'ParlaMint-CZ_2013-12-19-ps2013-004-04-006-036.conllu',\n",
       " 'ParlaMint-CZ_2013-11-27-ps2013-001-02-005-010.conllu',\n",
       " 'ParlaMint-CZ_2013-12-10-ps2013-004-01-016-025.conllu',\n",
       " 'ParlaMint-CZ_2013-11-25-ps2013-001-01-004-004.conllu',\n",
       " 'ParlaMint-CZ_2013-11-27-ps2013-001-02-006-011.conllu',\n",
       " 'ParlaMint-CZ_2013-12-04-ps2013-002-01-001-001.conllu',\n",
       " 'ParlaMint-CZ_2013-12-12-ps2013-004-02-004-031.conllu',\n",
       " 'ParlaMint-CZ_2013-12-19-ps2013-004-04-007-037.conllu',\n",
       " 'ParlaMint-CZ_2013-12-10-ps2013-004-01-017-026.conllu',\n",
       " 'ParlaMint-CZ_2013-11-27-ps2013-001-02-011-016.conllu',\n",
       " 'ParlaMint-CZ_2013-12-04-ps2013-002-01-004-004.conllu',\n",
       " 'ParlaMint-CZ_2013-12-16-ps2013-004-03-004-039.conllu',\n",
       " 'ParlaMint-CZ_2013-11-25-ps2013-001-01-000-000.conllu',\n",
       " 'ParlaMint-CZ_2013-12-19-ps2013-004-04-001-048.conllu',\n",
       " 'ParlaMint-CZ_2013-12-10-ps2013-004-01-002-020.conllu',\n",
       " 'ParlaMint-CZ_2013-12-10-ps2013-004-01-009-008.conllu',\n",
       " 'ParlaMint-CZ_2013-11-25-ps2013-001-01-002-002.conllu',\n",
       " 'ParlaMint-CZ_2013-12-19-ps2013-004-04-010-049.conllu',\n",
       " 'ParlaMint-CZ_2013-12-10-ps2013-004-01-005-004.conllu',\n",
       " 'ParlaMint-CZ_2013-12-12-ps2013-004-02-000-000.conllu',\n",
       " 'ParlaMint-CZ_2013-12-16-ps2013-004-03-001-040.conllu',\n",
       " 'ParlaMint-CZ_2013-12-12-ps2013-004-02-015-044.conllu',\n",
       " 'ParlaMint-CZ_2013-12-12-ps2013-004-02-013-017.conllu',\n",
       " 'ParlaMint-CZ_2013-12-12-ps2013-004-02-001-033.conllu',\n",
       " 'ParlaMint-CZ_2013-12-10-ps2013-004-01-018-027.conllu',\n",
       " 'ParlaMint-CZ_2013-12-04-ps2013-002-01-005-005.conllu',\n",
       " 'ParlaMint-CZ_2013-11-27-ps2013-001-02-002-007.conllu',\n",
       " 'ParlaMint-CZ_2013-12-10-ps2013-004-01-001-001.conllu',\n",
       " 'ParlaMint-CZ_2013-12-10-ps2013-004-01-000-000.conllu',\n",
       " 'ParlaMint-CZ_2013-12-12-ps2013-004-02-012-016.conllu',\n",
       " 'ParlaMint-CZ_2016-03-23-ps2013-042-10-007-052.conllu',\n",
       " 'ParlaMint-CZ_2016-06-29-ps2013-046-02-000-000.conllu',\n",
       " 'ParlaMint-CZ_2016-01-28-ps2013-039-06-008-202.conllu',\n",
       " 'ParlaMint-CZ_2016-12-06-ps2013-053-05-010-038.conllu',\n",
       " 'ParlaMint-CZ_2016-03-02-ps2013-042-02-013-062.conllu',\n",
       " 'ParlaMint-CZ_2016-02-09-ps2013-039-07-001-016.conllu',\n",
       " 'ParlaMint-CZ_2016-10-27-ps2013-050-07-000-000.conllu',\n",
       " 'ParlaMint-CZ_2016-12-08-ps2013-053-07-005-216.conllu',\n",
       " 'ParlaMint-CZ_2016-06-29-ps2013-048-02-005-151.conllu',\n",
       " 'ParlaMint-CZ_2016-04-19-ps2013-044-05-006-020.conllu',\n",
       " 'ParlaMint-CZ_2016-07-01-ps2013-048-04-000-000.conllu']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a list of files\n",
    "files = list(df.file.unique())\n",
    "files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linguistically process with Stanza"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We use Stanza to get POS, lemmas and ner. Send in the \"pre-tokenized text\" (created in previous steps).\n",
    "- Transform the result into CONLL-u (which should contain tokens, lemmas, pos)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Parse the CONLL-u file and add:\n",
    "\t1) sentence_id as metadata\n",
    "\t2) forward and reverse alignment as metadata (# align_s = 1-1 2-2... and #align_t = 1-1 2-2...),\n",
    "\t3) based on alignment, add SpaceAfter information to each token\n",
    "\t4) source text (\"source\")\n",
    "\t5) initial translation (#initial_translation metadata)\n",
    "\t6) improved translated text (#text metadata): based on SpaceAfter information, remove spaces around punctuation\n",
    "\t7) Delete startchar and endchar information from [\"misc\"] metadata element\n",
    "- Save the file as CONLLU with the same name as the source CONLLU file (so each file will be saved separately). The number of sentences should be the same as in the source CONLLU and ANA file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conllu(file, lang_code):\n",
    "\t\"\"\"\n",
    "\tThe function takes the dataframe (df), created in previous steps and takes only the instances from the df that belong\n",
    "\tto the file that is in the argument. It linguistically processes the translated sentences from the file and saves the file.\n",
    "\tThen we add additional information (metadata and NER annotations) to it with the conllu parser and save the final conllu file.\n",
    "\n",
    "\tArgs:\n",
    "\t\t- file (str): file name from the files list (see above)\n",
    "\t\t- lang_code (str): the lang code that is used in the names of the files, it should be the same as for extract_text()\n",
    "\t\"\"\"\n",
    "\n",
    "\t# Process all sentences in the dataframe and save them to a conllu file\n",
    "\tfrom stanza.utils.conll import CoNLL\n",
    "\tfrom conllu import parse\n",
    "\timport ast\n",
    "\timport regex as re\n",
    "\n",
    "\t# Use the dataframe, created in previous steps\n",
    "\tdf = pd.read_csv(\"{}\".format(final_dataframe), sep=\"\\t\")\n",
    "\n",
    "\t# Filter out only instances from the file in question\n",
    "\tdf = df[df[\"file\"] == file]\n",
    "\n",
    "\t# When we open the dataframe file, the list of strings in \"space-after-information\"\n",
    "\t# is a string - change it back to a list\n",
    "\tdf[\"space-after-information\"] = df[\"space-after-information\"].astype(\"str\")\n",
    "\tdf[\"space-after-information\"] = df[\"space-after-information\"].apply(lambda x: ast.literal_eval(x))\n",
    "\t\n",
    "\t# Create lists of information that we need to add to the conllu file\n",
    "\tids_list = df.sentence_id.to_list()\n",
    "\tsource_text = df.text.to_list()\n",
    "\tinitial_translation = df.translation.to_list()\n",
    "\taligns_source = df[\"aligns-source\"].to_list()\n",
    "\taligns_target = df[\"aligns-target\"].to_list()\n",
    "\tspace_after_list = df[\"space-after-information\"].to_list()\n",
    "\t\n",
    "\tsentence_list = df.new_translations.to_list()\n",
    "\n",
    "\t# To feed the entire list into the pipeline, we need to create lists of tokens, split by space\n",
    "\tsentence_list = [x.split(\" \") for x in sentence_list]\n",
    "\n",
    "\t# Linguistically process the list\n",
    "\tdoc = nlp(sentence_list)\n",
    "\n",
    "\t# Save the conllu file\n",
    "\tCoNLL.write_doc2conll(doc, \"/home/tajak/Parlamint-translation/results/{}/temp/{}\".format(lang_code, file))\n",
    "\n",
    "\tprint(\"{} processed and saved.\".format(file))\n",
    "\n",
    "\t# Open the CONLL-u file with the CONLL-u parser\n",
    "\n",
    "\tdata = open(\"/home/tajak/Parlamint-translation/results/{}/temp/{}\".format(lang_code, file), \"r\").read()\n",
    "\n",
    "\tsentences = parse(data)\n",
    "\n",
    "\t# Adding additional information to the conllu\n",
    "\tfor sentence in sentences:\n",
    "\t\t# Get the sentence index\n",
    "\t\tsentence_index = sentences.index(sentence)\n",
    "\n",
    "\t\t# Add metadata\n",
    "\t\tsentence.metadata[\"sent_id\"] = ids_list[sentence_index]\n",
    "\t\tsentence.metadata[\"align_s\"] = aligns_source[sentence_index]\n",
    "\t\tsentence.metadata[\"align_t\"] = aligns_target[sentence_index]\n",
    "\t\tsentence.metadata[\"source\"] = source_text[sentence_index]\n",
    "\t\tsentence.metadata[\"initial_translation\"] = initial_translation[sentence_index]\n",
    "\n",
    "\t\t# Delete the current metadata for text\n",
    "\t\tdel sentence.metadata[\"text\"]\n",
    "\n",
    "\t\tnew_translation_text = \"\"\n",
    "\n",
    "\t\t# Iterate through tokens and add SpaceAfter information if SpaceAfter is \"No\"\n",
    "\t\tfor word in sentence:\n",
    "\t\t\tword_index = sentence.index(word)\n",
    "\n",
    "\t\t\t# Remove information on start_char and end_char from the annotation\n",
    "\t\t\tdel word[\"misc\"][\"start_char\"]\n",
    "\t\t\tdel word[\"misc\"][\"end_char\"]\n",
    "\t\t\t\n",
    "\t\t\t# Change the NER tags so that they are the same as in the source\n",
    "\t\t\tcurrent_ner = word[\"misc\"][\"ner\"]\n",
    "\t\t\tdel word[\"misc\"][\"ner\"]\n",
    "\t\t\t\n",
    "\t\t\t# Substitute parts of the tags so that they are tha same as in source\n",
    "\t\t\tcurrent_ner = re.sub(\"S-\", \"B-\", current_ner)\n",
    "\t\t\tcurrent_ner = re.sub(\"E-\", \"I-\", current_ner)\n",
    "\n",
    "\t\t\tword[\"misc\"][\"NER\"] = current_ner\n",
    "\n",
    "\t\t\t# Get information about the space after based on the index\n",
    "\t\t\tcurrent_space_after = space_after_list[sentence_index][word_index]\n",
    "\n",
    "\t\t# Create new text from translation, correcting the spaces around words\n",
    "\t\t# based on the SpaceAfter information\n",
    "\t\t\tif current_space_after == \"No\":\n",
    "\t\t\t\tword[\"misc\"][\"SpaceAfter\"] = \"No\"\n",
    "\t\t\t\tnew_translation_text += word[\"form\"]\n",
    "\t\t\telif current_space_after == \"Last\":\n",
    "\t\t\t\tnew_translation_text += word[\"form\"]\n",
    "\t\t\telse:\n",
    "\t\t\t\tnew_translation_text += word[\"form\"]\n",
    "\t\t\t\tnew_translation_text += \" \"\n",
    "\t\t\n",
    "\t\tsentence.metadata[\"text\"] = new_translation_text\n",
    "\t\n",
    "\t# Create a new conllu file with the updated information\n",
    "\n",
    "\tfinal_file = open(\"/home/tajak/Parlamint-translation/results/{}/final_translated_conllu/{}\".format(lang_code, file), \"w\")\n",
    "\n",
    "\tfor sentence in sentences:\n",
    "\t\tfinal_file.write(sentence.serialize())\n",
    "\t\n",
    "\tfinal_file.close()\n",
    "\n",
    "\tprint(\"Final file {} is saved.\".format(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The code if we want to add NER elements too\n",
    "def create_conllu(file, lang_code):\n",
    "\n",
    "\t# Process all sentences in the dataframe and save them to a conllu file\n",
    "\tfrom stanza.utils.conll import CoNLL\n",
    "\tfrom conllu import parse\n",
    "\timport ast\n",
    "\n",
    "\t# Use the dataframe, created in previous steps\n",
    "\tdf = pd.read_csv(\"{}\".format(final_dataframe), sep=\"\\t\")\n",
    "\n",
    "\t# When we open the df, the NER list turns into a string - we need to change it into a list\n",
    "\tdf[\"target-NER-annotations\"] = df[\"target-NER-annotations\"].apply(lambda x: ast.literal_eval(x))\n",
    "\t\n",
    "\t# Filter out only instances from the file in question\n",
    "\tdf = df[df[\"file\"] == file]\n",
    "\n",
    "\t# Create lists of information that we need to add to the conllu file\n",
    "\tids_list = df.sentence_id.to_list()\n",
    "\taligns_source = df[\"aligns-source\"].to_list()\n",
    "\taligns_target = df[\"aligns-target\"].to_list()\n",
    "\tner_list = df[\"target-NER-annotations\"].to_list()\n",
    "\t\n",
    "\tsentence_list = df.new_translations.to_list()\n",
    "\n",
    "\t# To feed the entire list into the pipeline, we need to create lists of tokens, split by space\n",
    "\tsentence_list = [x.split(\" \") for x in sentence_list]\n",
    "\n",
    "\t# Linguistically process the list\n",
    "\tdoc = nlp(sentence_list)\n",
    "\n",
    "\t# Save the conllu file\n",
    "\tCoNLL.write_doc2conll(doc, \"results/{}/ParlaMint-{}-translated.conllu/temp/{}\".format(lang_code, lang_code, file))\n",
    "\n",
    "\tprint(\"{} processed and saved.\".format(file))\n",
    "\n",
    "\t# Open the CONLL-u file with the CONLL-u parser\n",
    "\n",
    "\tdata = open(\"results/{}/ParlaMint-{}-translated.conllu/temp/{}\".format(lang_code, lang_code, file), \"r\").read()\n",
    "\n",
    "\tsentences = parse(data)\n",
    "\n",
    "\t# Adding additional information to the conllu\n",
    "\tfor sentence in sentences:\n",
    "\t\t# Get the sentence index\n",
    "\t\tsentence_index = sentences.index(sentence)\n",
    "\n",
    "\t\t# Add metadata\n",
    "\t\tsentence.metadata[\"sent_id\"] = ids_list[sentence_index]\n",
    "\t\tsentence.metadata[\"align_s\"] = aligns_source[sentence_index]\n",
    "\t\tsentence.metadata[\"align_t\"] = aligns_target[sentence_index]\n",
    "\n",
    "\t\t# Make the # text element be the last \n",
    "\t\tcurrent_text = sentence.metadata[\"text\"]\n",
    "\t\tdel sentence.metadata[\"text\"]\n",
    "\n",
    "\t\tsentence.metadata[\"text\"] = current_text\n",
    "\n",
    "\t\t# Iterate through tokens and add NER information to each\n",
    "\t\tfor word in sentence:\n",
    "\t\t\tword_index = sentence.index(word)\n",
    "\t\t\t# Add NER information based on the word index\n",
    "\t\t\tword[\"misc\"][\"NER\"] = ner_list[sentence_index][word_index]\n",
    "\t\t\n",
    "\t# Create a new conllu file with the updated information\n",
    "\n",
    "\tfinal_file = open(\"results/{}/ParlaMint-{}-translated.conllu/{}\".format(lang_code, lang_code, file), \"w\")\n",
    "\n",
    "\tfor sentence in sentences:\n",
    "\t\tfinal_file.write(sentence.serialize())\n",
    "\t\n",
    "\tfinal_file.close()\n",
    "\n",
    "\tprint(\"Final file {} is saved.\".format(file))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-17 13:29:37 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ebb4bbd8595495ca0c2c76e6cafb607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-17 13:29:37 WARNING: Can not find mwt: default from official model list. Ignoring it.\n",
      "2023-01-17 13:29:38 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| pos       | combined |\n",
      "| lemma     | combined |\n",
      "| ner       | conll03  |\n",
      "========================\n",
      "\n",
      "2023-01-17 13:29:38 INFO: Use device: gpu\n",
      "2023-01-17 13:29:38 INFO: Loading: tokenize\n",
      "2023-01-17 13:29:38 INFO: Loading: pos\n",
      "2023-01-17 13:29:38 INFO: Loading: lemma\n",
      "2023-01-17 13:29:38 INFO: Loading: ner\n",
      "2023-01-17 13:29:38 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003.conllu processed and saved.\n",
      "Final file ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003.conllu is saved.\n",
      "ParlaMint-CZ_2013-12-10-ps2013-004-01-004-003.conllu processed and saved.\n",
      "Final file ParlaMint-CZ_2013-12-10-ps2013-004-01-004-003.conllu is saved.\n",
      "ParlaMint-CZ_2013-12-10-ps2013-004-01-015-024.conllu processed and saved.\n",
      "Final file ParlaMint-CZ_2013-12-10-ps2013-004-01-015-024.conllu is saved.\n",
      "ParlaMint-CZ_2013-12-10-ps2013-004-01-019-021.conllu processed and saved.\n",
      "Final file ParlaMint-CZ_2013-12-10-ps2013-004-01-019-021.conllu is saved.\n",
      "ParlaMint-CZ_2013-11-27-ps2013-001-02-015-020.conllu processed and saved.\n",
      "Final file ParlaMint-CZ_2013-11-27-ps2013-001-02-015-020.conllu is saved.\n",
      "ParlaMint-CZ_2013-12-12-ps2013-004-02-002-029.conllu processed and saved.\n",
      "Final file ParlaMint-CZ_2013-12-12-ps2013-004-02-002-029.conllu is saved.\n",
      "ParlaMint-CZ_2013-11-27-ps2013-001-02-001-006.conllu processed and saved.\n",
      "Final file ParlaMint-CZ_2013-11-27-ps2013-001-02-001-006.conllu is saved.\n",
      "ParlaMint-CZ_2013-12-10-ps2013-004-01-013-028.conllu processed and saved.\n",
      "Final file ParlaMint-CZ_2013-12-10-ps2013-004-01-013-028.conllu is saved.\n",
      "ParlaMint-CZ_2013-12-19-ps2013-004-04-008-038.conllu processed and saved.\n",
      "Final file ParlaMint-CZ_2013-12-19-ps2013-004-04-008-038.conllu is saved.\n",
      "ParlaMint-CZ_2013-12-19-ps2013-004-04-009-042.conllu processed and saved.\n",
      "Final file ParlaMint-CZ_2013-12-19-ps2013-004-04-009-042.conllu is saved.\n",
      "ParlaMint-CZ_2013-12-04-ps2013-002-01-006-006.conllu processed and saved.\n",
      "Final file ParlaMint-CZ_2013-12-04-ps2013-002-01-006-006.conllu is saved.\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "# Now, let's feed the changed translation to the Stanza pipeline to create the final format\n",
    "#nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma,ner', tokenize_pretokenized=True)\n",
    "\n",
    "# Instruct it to use a specific package: \tCoNLL03\n",
    "nlp = stanza.Pipeline(lang='en', processors=\"tokenize,mwt,pos,lemma,ner\", package={\"ner\": [\"conll03\"]}, tokenize_pretokenized=True)\n",
    "\n",
    "for file in files[:11]:\n",
    "\tcreate_conllu(file, lang_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether the translated and source file have the same no. of sentences\n",
    "from conllu import parse\n",
    "\n",
    "source = open(\"/home/tajak/Parlamint-translation/ParlaMint-CZ/ParlaMint-CZ.conllu/ParlaMint-CZ.conllu/2013/ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003.conllu\", \"r\").read()\n",
    "source_sen = parse(source)\n",
    "\n",
    "translation = open(\"/home/tajak/Parlamint-translation/results/CZ/final_translated_conllu/ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003.conllu\", \"r\").read()\n",
    "\n",
    "translations_sen = parse(translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n",
      "53\n"
     ]
    }
   ],
   "source": [
    "# Check if number of sentences match\n",
    "print(len(source_sen))\n",
    "print(len(translations_sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003.u12.p1.s2\n",
      "ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003.u12.p1.s2\n",
      "ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003.u13.p1.s1\n",
      "ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003.u13.p1.s1\n",
      "ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003.u13.p2.s1\n",
      "ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003.u13.p2.s1\n"
     ]
    }
   ],
   "source": [
    "# Check if ids match\n",
    "for i in [50,51,52]:\n",
    "\tprint(source_sen[i].metadata[\"sent_id\"])\n",
    "\tprint(translations_sen[i].metadata[\"sent_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Takže přerušuji jednání Sněmovny do 15.30 hodin.\n",
      "So I interrupt the House meeting by 3:30 p.m.\n",
      "Dámy a pánové, čas, který byl vyhrazen na poradu poslaneckého klubu TOP 09, uplynul, a já vás tedy prosím, abyste zaujali svá místa v jednacím sále, a budeme pokračovat.\n",
      "Ladies and gentlemen, the time allocated to the meeting of the MEP's TOP 09 club is over, and I therefore ask you to take your seats in the Chamber of Commerce, and we will continue.\n",
      "Dalším bodem, který budeme projednávat, je\n",
      "The next item we're going to discuss is\n"
     ]
    }
   ],
   "source": [
    "# Check if content matches\n",
    "for i in [50,51,52]:\n",
    "\tprint(source_sen[i].metadata[\"text\"])\n",
    "\tprint(translations_sen[i].metadata[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect translations with possessive adjectives\n",
    "df = pd.read_csv(\"{}\".format(translated_dataframe_path), sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>file_path</th>\n",
       "      <th>file</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>NER</th>\n",
       "      <th>proper_nouns</th>\n",
       "      <th>length</th>\n",
       "      <th>translation</th>\n",
       "      <th>translation-tokenized</th>\n",
       "      <th>space-after-information</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/tajak/Parlamint-translation/ParlaMint-CZ...</td>\n",
       "      <td>ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....</td>\n",
       "      <td>ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....</td>\n",
       "      <td>3.</td>\n",
       "      <td>3 .</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{}</td>\n",
       "      <td>1</td>\n",
       "      <td>3.</td>\n",
       "      <td>3 .</td>\n",
       "      <td>['No', 'Last']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/tajak/Parlamint-translation/ParlaMint-CZ...</td>\n",
       "      <td>ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....</td>\n",
       "      <td>ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....</td>\n",
       "      <td>Návrh zasedacího pořádku poslanců v jednacím s...</td>\n",
       "      <td>Návrh zasedacího pořádku poslanců v jednacím s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{}</td>\n",
       "      <td>9</td>\n",
       "      <td>Proposal for a sitting order of Members in the...</td>\n",
       "      <td>Proposal for a sitting order of Members in the...</td>\n",
       "      <td>['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Ye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>/home/tajak/Parlamint-translation/ParlaMint-CZ...</td>\n",
       "      <td>ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....</td>\n",
       "      <td>ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....</td>\n",
       "      <td>Podle § 52 odst. 1 našeho jednacího řádu Posla...</td>\n",
       "      <td>Podle § 52 odst . 1 našeho jednacího řádu Posl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{}</td>\n",
       "      <td>33</td>\n",
       "      <td>According to Article 52 (2) (a) of the basic R...</td>\n",
       "      <td>According to Article 52 ( 2 ) ( a ) of the bas...</td>\n",
       "      <td>['Yes', 'Yes', 'Yes', 'Yes', 'No', 'No', 'Yes'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>/home/tajak/Parlamint-translation/ParlaMint-CZ...</td>\n",
       "      <td>ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....</td>\n",
       "      <td>ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....</td>\n",
       "      <td>Já bych k této věci otevřel rozpravu.</td>\n",
       "      <td>Já bych k této věci otevřel rozpravu .</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{}</td>\n",
       "      <td>7</td>\n",
       "      <td>I would like to open a debate on this matter.</td>\n",
       "      <td>I would like to open a debate on this matter .</td>\n",
       "      <td>['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Ye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>/home/tajak/Parlamint-translation/ParlaMint-CZ...</td>\n",
       "      <td>ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....</td>\n",
       "      <td>ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....</td>\n",
       "      <td>Pan poslanec a předseda klubu ODS Zbyněk Stanj...</td>\n",
       "      <td>Pan poslanec a předseda klubu ODS Zbyněk Stanj...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{6: ['Zbyněk', 'Zbyněk'], 7: ['Stanjura', 'Sta...</td>\n",
       "      <td>8</td>\n",
       "      <td>Member and Chairman of the ODS Club Zbyněk Sta...</td>\n",
       "      <td>Member and Chairman of the ODS Club Zbyněk Sta...</td>\n",
       "      <td>['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Ye...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1  \\\n",
       "0           0             0   \n",
       "1           1             1   \n",
       "2           2             2   \n",
       "3           3             3   \n",
       "4           4             4   \n",
       "\n",
       "                                           file_path  \\\n",
       "0  /home/tajak/Parlamint-translation/ParlaMint-CZ...   \n",
       "1  /home/tajak/Parlamint-translation/ParlaMint-CZ...   \n",
       "2  /home/tajak/Parlamint-translation/ParlaMint-CZ...   \n",
       "3  /home/tajak/Parlamint-translation/ParlaMint-CZ...   \n",
       "4  /home/tajak/Parlamint-translation/ParlaMint-CZ...   \n",
       "\n",
       "                                                file  \\\n",
       "0  ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....   \n",
       "1  ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....   \n",
       "2  ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....   \n",
       "3  ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....   \n",
       "4  ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....   \n",
       "\n",
       "                                         sentence_id  \\\n",
       "0  ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....   \n",
       "1  ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....   \n",
       "2  ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....   \n",
       "3  ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....   \n",
       "4  ParlaMint-CZ_2013-12-04-ps2013-002-01-003-003....   \n",
       "\n",
       "                                                text  \\\n",
       "0                                                 3.   \n",
       "1  Návrh zasedacího pořádku poslanců v jednacím s...   \n",
       "2  Podle § 52 odst. 1 našeho jednacího řádu Posla...   \n",
       "3              Já bych k této věci otevřel rozpravu.   \n",
       "4  Pan poslanec a předseda klubu ODS Zbyněk Stanj...   \n",
       "\n",
       "                                      tokenized_text  NER  \\\n",
       "0                                                3 .  NaN   \n",
       "1  Návrh zasedacího pořádku poslanců v jednacím s...  NaN   \n",
       "2  Podle § 52 odst . 1 našeho jednacího řádu Posl...  NaN   \n",
       "3             Já bych k této věci otevřel rozpravu .  NaN   \n",
       "4  Pan poslanec a předseda klubu ODS Zbyněk Stanj...  NaN   \n",
       "\n",
       "                                        proper_nouns  length  \\\n",
       "0                                                 {}       1   \n",
       "1                                                 {}       9   \n",
       "2                                                 {}      33   \n",
       "3                                                 {}       7   \n",
       "4  {6: ['Zbyněk', 'Zbyněk'], 7: ['Stanjura', 'Sta...       8   \n",
       "\n",
       "                                         translation  \\\n",
       "0                                                 3.   \n",
       "1  Proposal for a sitting order of Members in the...   \n",
       "2  According to Article 52 (2) (a) of the basic R...   \n",
       "3      I would like to open a debate on this matter.   \n",
       "4  Member and Chairman of the ODS Club Zbyněk Sta...   \n",
       "\n",
       "                               translation-tokenized  \\\n",
       "0                                                3 .   \n",
       "1  Proposal for a sitting order of Members in the...   \n",
       "2  According to Article 52 ( 2 ) ( a ) of the bas...   \n",
       "3     I would like to open a debate on this matter .   \n",
       "4  Member and Chairman of the ODS Club Zbyněk Sta...   \n",
       "\n",
       "                             space-after-information  \n",
       "0                                     ['No', 'Last']  \n",
       "1  ['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Ye...  \n",
       "2  ['Yes', 'Yes', 'Yes', 'Yes', 'No', 'No', 'Yes'...  \n",
       "3  ['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Ye...  \n",
       "4  ['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Ye...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParlaMint-CZ_2013-12-12-ps2013-004-02-005-045.conllu\n",
      "ParlaMint-CZ_2013-12-06-ps2013-002-02-002-008.conllu\n",
      "ParlaMint-CZ_2013-12-06-ps2013-002-02-002-008.conllu\n",
      "ParlaMint-CZ_2013-12-06-ps2013-002-02-002-008.conllu\n",
      "ParlaMint-CZ_2013-12-06-ps2013-002-02-002-008.conllu\n",
      "ParlaMint-CZ_2013-12-06-ps2013-002-02-002-008.conllu\n",
      "ParlaMint-CZ_2013-12-16-ps2013-004-03-003-018.conllu\n",
      "ParlaMint-CZ_2013-12-16-ps2013-004-03-003-018.conllu\n",
      "ParlaMint-CZ_2013-12-16-ps2013-004-03-003-018.conllu\n",
      "ParlaMint-CZ_2013-12-19-ps2013-004-04-005-035.conllu\n",
      "ParlaMint-CZ_2013-12-19-ps2013-004-04-005-035.conllu\n",
      "ParlaMint-CZ_2013-12-06-ps2013-003-01-001-001.conllu\n",
      "ParlaMint-CZ_2013-12-06-ps2013-003-01-001-001.conllu\n",
      "ParlaMint-CZ_2013-12-06-ps2013-003-01-001-001.conllu\n",
      "ParlaMint-CZ_2013-12-06-ps2013-003-01-001-001.conllu\n",
      "ParlaMint-CZ_2013-12-06-ps2013-003-01-001-001.conllu\n",
      "ParlaMint-CZ_2013-12-06-ps2013-003-01-001-001.conllu\n",
      "ParlaMint-CZ_2013-12-06-ps2013-003-01-001-001.conllu\n",
      "ParlaMint-CZ_2013-11-27-ps2013-001-02-013-019.conllu\n"
     ]
    }
   ],
   "source": [
    "file_list = list(df.file.unique())\n",
    "\n",
    "\n",
    "for file in file_list:\n",
    "\tfor i in instances:\n",
    "\t\tif file in i:\n",
    "\t\t\tprint(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open(\"possessive_adjectives.txt\", \"r\")\n",
    "\n",
    "instances = file.readlines()\n",
    "\n",
    "len(instances)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parlamint_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Mar 15 2022, 12:22:08) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e8622831a220209edc4d2d4e58bc2159575ea2f8d9419209393154757ff5f93"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
