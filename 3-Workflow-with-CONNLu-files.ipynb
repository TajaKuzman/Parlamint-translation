{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Workflow:\n",
    "1. Extract information from the CONLL-U\n",
    "2. Translate\n",
    "3. Tokenize English translations with Stanza\n",
    "4. Word alignment, substitute English NE translations with lemmas from the source, get information on NE annotations for each translated word from the source annotations\n",
    "5. Linguistically process English translation with Stanza (lemmas, POS)\n",
    "6. Parse CONLL-u file and add additional information (sentence ids, alignments, NER annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conllu import parse\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "414"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a list of TEI files\n",
    "\n",
    "path = \"ParlaMint-SI/ParlaMint-SI.conllu\"\n",
    "dir_list = os.listdir(path)\n",
    "\n",
    "# Keep only files with parliamentary sessions:\n",
    "\n",
    "parl_list = []\n",
    "\n",
    "# Filter out only relevant files\n",
    "for i in dir_list:\n",
    "\tif \"ParlaMint-SI_\" in i:\n",
    "\t\tif \".conllu\" in i:\n",
    "\t\t\tparl_list.append(\"{}\".format(i))\n",
    "\n",
    "len(parl_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ParlaMint-SI_2020-05-27-SDZ8-Redna-17.conllu'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parl_list[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract information from CONLL-U files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a test file - one that was also used in the big sample\n",
    "data = open(\"ParlaMint-SI/ParlaMint-SI.conllu/ParlaMint-SI_2019-10-23-SDZ8-Redna-12.conllu\", \"r\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Format:\n",
    "\n",
    "# newdoc id = ParlaMint-SI_2014-08-01-SDZ7-Redna-01.u1\n",
    "# newpar id = ParlaMint-SI_2014-08-01-SDZ7-Redna-01.seg1\n",
    "# sent_id = ParlaMint-SI_2014-08-01-SDZ7-Redna-01.seg1.1\n",
    "# text = Spoštovani, prosim, da zasedete svoja mesta.\n",
    "1\tSpoštovani\tspoštovan\tADJ\tAppmpn\tCase=Nom|Degree=Pos|Gender=Masc|Number=Sing|VerbForm=Part\t3\tdiscourse\t_\tNER=O|SpaceAfter=No\n",
    "2\t,\t,\tPUNCT\tZ\t_\t1\tpunct\t_\tNER=O\n",
    "3\tprosim\tprositi\tVERB\tVmpr1s\tAspect=Imp|Mood=Ind|Number=Sing|Person=1|Tense=Pres|VerbForm=Fin\t0\troot\t_\tNER=O|SpaceAfter=No\n",
    "4\t,\t,\tPUNCT\tZ\t_\t6\tpunct\t_\tNER=O\n",
    "5\tda\tda\tSCONJ\tCs\t_\t6\tmark\t_\tNER=O\n",
    "6\tzasedete\tzasesti\tVERB\tVmer2p\tAspect=Perf|Mood=Ind|Number=Plur|Person=2|Tense=Pres|VerbForm=Fin\t3\tccomp\t_\tNER=O\n",
    "7\tsvoja\tsvoj\tDET\tPx-npa\tCase=Acc|Gender=Neut|Number=Plur|Poss=Yes|PronType=Prs|Reflex=Yes\t8\tdet\t_\tNER=O\n",
    "8\tmesta\tmesto\tNOUN\tNcnpa\tCase=Acc|Gender=Neut|Number=Plur\t6\tobj\t_\tNER=O|SpaceAfter=No\n",
    "9\t.\t.\tPUNCT\tZ\t_\t3\tpunct\t_\tNER=O\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONLLU parser cheatsheet\n",
    "\"\"\"\n",
    "# Find which words are proper names with the filtering function\n",
    "sentence.filter(misc__NER=lambda x: x != \"O\")\n",
    "\n",
    "# Adding new metadata to the file\n",
    "sentences[0].metadata[\"alignment\"] =  \"1-1\"\n",
    "\n",
    "# To turn back into conll-u\n",
    "print(sentences[1].serialize())\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract from each sentence in the CONLL-u file:\n",
    "- sent_id (in metadata) (# sent_id = ParlaMint-SI_2014-08-01-SDZ7-Redna-01.seg1.1)\n",
    "- \"text\" (in metadata): to be feed into the MT system (# text = Spoštovani, prosim, da zasedete svoja mesta.)\n",
    "- tokenized text (punctuation separated from words by space): by iterating through the tokens in the sentence - create a list of tokens and join them into a string ([\"Spoštovani\", \"prosim\", \",\", \"da\"] -> \"Spoštovani prosim , da)\n",
    "- list of NE annotations (same length as the tokens) - we want NE annotations for all tokens, with the information on the lemma and index if the NE is not \"0\": [{0:[\"O\"]}, {1:[\"O\"]}, {2:[\"O\"]}, {3: [\"PER-I\", \"Borut\"]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty df\n",
    "df = pd.DataFrame({\"file\": [\"\"], \"sentence_id\": [\"\"], \"text\": [\"\"], \"tokenized_text\": [\"\"], \"NER\": [\"\"], \"proper_nouns\": [\"\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the data with CONLL-u parser\n",
    "\n",
    "for doc in parl_list[:3]:\n",
    "\t# Open the file\n",
    "\tdata = open(\"{}/{}\".format(path,doc), \"r\").read()\n",
    "\n",
    "\tsentences = parse(data)\n",
    "\n",
    "\tsentence_id_list = []\n",
    "\ttext_list = []\n",
    "\ttokenized_text_list = []\n",
    "\tNER_list = []\n",
    "\tproper_noun_list = []\n",
    "\n",
    "\tfor sentence in sentences:\n",
    "\t\t# Find sentence ids\n",
    "\t\tcurrent_sentence_id = sentence.metadata[\"sent_id\"]\n",
    "\t\tsentence_id_list.append(current_sentence_id)\n",
    "\n",
    "\t\t# Find text\n",
    "\t\tcurrent_text = sentence.metadata[\"text\"]\n",
    "\t\ttext_list.append(current_text)\n",
    "\n",
    "\t\t# Create a string out of tokens\n",
    "\t\tcurrent_token_list = []\n",
    "\t\tcurrent_ner_dict = {}\n",
    "\t\tword_dict = {}\n",
    "\n",
    "\t\tfor token in sentence:\n",
    "\t\t\tcurrent_token_list.append(token[\"form\"])\n",
    "\n",
    "\t\t\t# Create a list of NE annotations with word indices.\n",
    "\t\t\t# I'll substract one from the word index, because indexing in the CONLLU file starts with 1, not 0\n",
    "\t\t\tcurrent_index = int(token[\"id\"]) - 1\n",
    "\n",
    "\t\t\tcurrent_ner_dict[current_index] = token[\"misc\"][\"NER\"]\n",
    "\n",
    "\t\t\t# Add information on the lemma if the NE is personal name\n",
    "\t\t\tif \"PER\" in token[\"misc\"][\"NER\"]:\n",
    "\t\t\t\tword_dict[current_index] = [token[\"form\"], token[\"lemma\"]]\n",
    "\n",
    "\t\tproper_noun_list.append(word_dict)\n",
    "\n",
    "\t\tcurrent_string = \" \".join(current_token_list)\n",
    "\n",
    "\t\ttokenized_text_list.append(current_string)\n",
    "\t\tNER_list.append(current_ner_dict)\n",
    "\t\n",
    "\tnew_df = pd.DataFrame({\"sentence_id\": sentence_id_list, \"text\": text_list, \"tokenized_text\": tokenized_text_list, \"NER\": NER_list, \"proper_nouns\": proper_noun_list})\n",
    "\n",
    "\tnew_df[\"file\"] = doc\n",
    "\n",
    "\t# Merge df to the previous df\n",
    "\tdf = pd.concat([df, new_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>NER</th>\n",
       "      <th>proper_nouns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8220</td>\n",
       "      <td>8220</td>\n",
       "      <td>8220</td>\n",
       "      <td>8220</td>\n",
       "      <td>8220</td>\n",
       "      <td>8220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>3</td>\n",
       "      <td>8220</td>\n",
       "      <td>7388</td>\n",
       "      <td>7388</td>\n",
       "      <td>2184</td>\n",
       "      <td>722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>ParlaMint-SI_2020-05-27-SDZ8-Redna-17.conllu</td>\n",
       "      <td>ParlaMint-SI_2020-05-27-SDZ8-Redna-17.seg1.1</td>\n",
       "      <td>Hvala lepa.</td>\n",
       "      <td>Hvala lepa .</td>\n",
       "      <td>{0: 'O', 1: 'O'}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>3487</td>\n",
       "      <td>1</td>\n",
       "      <td>211</td>\n",
       "      <td>211</td>\n",
       "      <td>329</td>\n",
       "      <td>7404</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                file  \\\n",
       "count                                           8220   \n",
       "unique                                             3   \n",
       "top     ParlaMint-SI_2020-05-27-SDZ8-Redna-17.conllu   \n",
       "freq                                            3487   \n",
       "\n",
       "                                         sentence_id         text  \\\n",
       "count                                           8220         8220   \n",
       "unique                                          8220         7388   \n",
       "top     ParlaMint-SI_2020-05-27-SDZ8-Redna-17.seg1.1  Hvala lepa.   \n",
       "freq                                               1          211   \n",
       "\n",
       "       tokenized_text               NER proper_nouns  \n",
       "count            8220              8220         8220  \n",
       "unique           7388              2184          722  \n",
       "top      Hvala lepa .  {0: 'O', 1: 'O'}           {}  \n",
       "freq              211               329         7404  "
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reset index\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Remove the first row\n",
    "df = df.drop([0], axis=\"index\")\n",
    "\n",
    "# Show the results\n",
    "df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>NER</th>\n",
       "      <th>proper_nouns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ParlaMint-SI_2020-05-27-SDZ8-Redna-17.conllu</td>\n",
       "      <td>ParlaMint-SI_2020-05-27-SDZ8-Redna-17.seg1.1</td>\n",
       "      <td>Nadaljujemo s prekinjeno 17. sejo zbora.</td>\n",
       "      <td>Nadaljujemo s prekinjeno 17. sejo zbora .</td>\n",
       "      <td>{0: 'O', 1: 'O', 2: 'O', 3: 'O', 4: 'O', 5: 'O...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ParlaMint-SI_2020-05-27-SDZ8-Redna-17.conllu</td>\n",
       "      <td>ParlaMint-SI_2020-05-27-SDZ8-Redna-17.seg2.1</td>\n",
       "      <td>Prehajamo na 2. TOČKO DNEVNEGA REDA, TO JE NA ...</td>\n",
       "      <td>Prehajamo na 2. TOČKO DNEVNEGA REDA , TO JE NA...</td>\n",
       "      <td>{0: 'O', 1: 'O', 2: 'O', 3: 'O', 4: 'O', 5: 'O...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file  \\\n",
       "1  ParlaMint-SI_2020-05-27-SDZ8-Redna-17.conllu   \n",
       "2  ParlaMint-SI_2020-05-27-SDZ8-Redna-17.conllu   \n",
       "\n",
       "                                    sentence_id  \\\n",
       "1  ParlaMint-SI_2020-05-27-SDZ8-Redna-17.seg1.1   \n",
       "2  ParlaMint-SI_2020-05-27-SDZ8-Redna-17.seg2.1   \n",
       "\n",
       "                                                text  \\\n",
       "1           Nadaljujemo s prekinjeno 17. sejo zbora.   \n",
       "2  Prehajamo na 2. TOČKO DNEVNEGA REDA, TO JE NA ...   \n",
       "\n",
       "                                      tokenized_text  \\\n",
       "1          Nadaljujemo s prekinjeno 17. sejo zbora .   \n",
       "2  Prehajamo na 2. TOČKO DNEVNEGA REDA , TO JE NA...   \n",
       "\n",
       "                                                 NER proper_nouns  \n",
       "1  {0: 'O', 1: 'O', 2: 'O', 3: 'O', 4: 'O', 5: 'O...           {}  \n",
       "2  {0: 'O', 1: 'O', 2: 'O', 3: 'O', 4: 'O', 5: 'O...           {}  "
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file': 'ParlaMint-SI_2020-05-27-SDZ8-Redna-17.conllu',\n",
       " 'sentence_id': 'ParlaMint-SI_2020-05-27-SDZ8-Redna-17.seg11.1',\n",
       " 'text': 'Besedo dajem Marjanu Maučecu, predstavniku Državnega sveta kot predlagatelja predloga zakona za predstavitev stališča do predloga matičnega delovnega telesa.',\n",
       " 'tokenized_text': 'Besedo dajem Marjanu Maučecu , predstavniku Državnega sveta kot predlagatelja predloga zakona za predstavitev stališča do predloga matičnega delovnega telesa .',\n",
       " 'NER': {0: 'O',\n",
       "  1: 'O',\n",
       "  2: 'B-PER',\n",
       "  3: 'I-PER',\n",
       "  4: 'O',\n",
       "  5: 'O',\n",
       "  6: 'B-ORG',\n",
       "  7: 'I-ORG',\n",
       "  8: 'O',\n",
       "  9: 'O',\n",
       "  10: 'O',\n",
       "  11: 'O',\n",
       "  12: 'O',\n",
       "  13: 'O',\n",
       "  14: 'O',\n",
       "  15: 'O',\n",
       "  16: 'O',\n",
       "  17: 'O',\n",
       "  18: 'O',\n",
       "  19: 'O',\n",
       "  20: 'O'},\n",
       " 'proper_nouns': {2: ['Marjanu', 'Marjan'], 3: ['Maučecu', 'Maučec']}}"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect an example\n",
    "df.iloc[23].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe\n",
    "df.to_csv(\"Parlamint-SI-sentences-conllu-workflow-sample.csv\", sep=\"\\t\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word alignment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization with Stanza"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We apply the stanza tokenization over the translation; use tokenize_no_ssplit to avoid splitting sentences in multiple sentences."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alignment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Perform word alignment.\n",
    "- Save forward and reverse alignment information for each sentence (2 additional columns).\n",
    "- Transfer NE annotations to the translated sentence based on the alignment: add a column with information to which English token this information should go to (e.g. [{3: \"B-PER\", 5:\"I-LOC\"}])\n",
    "- Substitute translated NE words with lemmas based on the annotation, save new translation to a new column."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linguistic processing of translated text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We use Stanza to get POS and lemmas. Send in the \"pre-tokenized text\" (created in previous steps).\n",
    "- Transform the result into CONLL-u (which should contain tokens, lemmas, pos). Parse the CONLL-u file and add: 1) sentence_id as metadata 2) forward and reverse alignment as metadata (# align_s = 1-1 2-2... and #align_t = 1-1 2-2...), 3) based on alignment, add NER information to each token (misc = {NER:} field)\n",
    "- Save the file as CONLLU with the same name as the source CONLLU file (so each file will be saved separately). The number of sentences should be the same as in the source CONLLU and ANA file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding new metadata to the file\n",
    "sentences[0].metadata[\"alignment\"] =  \"1-1\"\n",
    "sentences[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To turn back into conll-u\n",
    "print(sentences[1].serialize())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e373e41fe05b496006fe2fc132d7af19f1d513370c44925a0044a5f3ee41336"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
