{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to process the /home/tajak/Parlamint-translation/Source-data/ParlaMint-CZ.conllu/ParlaMint-CZ.conllu/2016/ParlaMint-CZ_2016-10-26-ps2013-050-06-005-217.conllu in ParlaMint-CZ, because the file was empty when the previous version of the corpus was downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=6\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import zipfile\n",
    "import gzip\n",
    "import shutil\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define main information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the language code, used in the file names\n",
    "#lang_code = \"CZ\"\n",
    "lang_code = \"CZ\"\n",
    "\n",
    "# Main path\n",
    "main_path = \"/home/tajak/Parlamint-translation\"\n",
    "\n",
    "parl_list = [\"/home/tajak/Parlamint-translation/Source-data/ParlaMint-CZ.conllu/ParlaMint-CZ.conllu/2016/ParlaMint-CZ_2016-10-26-ps2013-050-06-005-217.conllu\"]\n",
    "\n",
    "file_name_list = [\"ParlaMint-CZ_2016-10-26-ps2013-050-06-005-217.conllu\"]\n",
    "\n",
    "extracted_dataframe_path = \"results/CZ/ParlaMint-CZ-extracted-source-data-missing-file.csv\"\n",
    "\n",
    "translated_dataframe_path = \"results/CZ/ParlaMint-CZ-translated-missing-file.csv\"\n",
    "\n",
    "translated_tokenized_dataframe_path = \"results/CZ/ParlaMint-CZ-translated-tokenized-missing-file.csv\"\n",
    "\n",
    "final_dataframe = \"results/CZ/ParlaMint-CZ-final-dataframe-missing-file.csv\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conllu_to-df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the corpora: 580\n",
      "Dataframe saved as results/CZ/ParlaMint-CZ-extracted-source-data-missing-file.csv\n",
      "|        | file_path                                                                                                                                       | file                                                 | sentence_id                                            | text    | tokenized_text   | proper_nouns   |   length |\n",
      "|:-------|:------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------|:-------------------------------------------------------|:--------|:-----------------|:---------------|---------:|\n",
      "| count  | 42                                                                                                                                              | 42                                                   | 42                                                     | 42      | 42               | 42             |  42      |\n",
      "| unique | 1                                                                                                                                               | 1                                                    | 42                                                     | 41      | 41               | 7              | nan      |\n",
      "| top    | /home/tajak/Parlamint-translation/Source-data/ParlaMint-CZ.conllu/ParlaMint-CZ.conllu/2016/ParlaMint-CZ_2016-10-26-ps2013-050-06-005-217.conllu | ParlaMint-CZ_2016-10-26-ps2013-050-06-005-217.conllu | ParlaMint-CZ_2016-10-26-ps2013-050-06-005-217.u1.p1.s1 | Děkuji. | Děkuji .         | {}             | nan      |\n",
      "| freq   | 42                                                                                                                                              | 42                                                   | 1                                                      | 2       | 2                | 36             | nan      |\n",
      "| mean   | nan                                                                                                                                             | nan                                                  | nan                                                    | nan     | nan              | nan            |  13.8095 |\n",
      "| std    | nan                                                                                                                                             | nan                                                  | nan                                                    | nan     | nan              | nan            |  15.924  |\n",
      "| min    | nan                                                                                                                                             | nan                                                  | nan                                                    | nan     | nan              | nan            |   1      |\n",
      "| 25%    | nan                                                                                                                                             | nan                                                  | nan                                                    | nan     | nan              | nan            |   5      |\n",
      "| 50%    | nan                                                                                                                                             | nan                                                  | nan                                                    | nan     | nan              | nan            |  12      |\n",
      "| 75%    | nan                                                                                                                                             | nan                                                  | nan                                                    | nan     | nan              | nan            |  18.5    |\n",
      "| max    | nan                                                                                                                                             | nan                                                  | nan                                                    | nan     | nan              | nan            | 100      |\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "|    | file_path                                                                                                                                       | file                                                 | sentence_id                                            | text                                                                                                                                                                                                                            | tokenized_text                                                                                                                                                                                                                    | proper_nouns   |   length |\n",
      "|---:|:------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------|:-------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------|---------:|\n",
      "|  0 | /home/tajak/Parlamint-translation/Source-data/ParlaMint-CZ.conllu/ParlaMint-CZ.conllu/2016/ParlaMint-CZ_2016-10-26-ps2013-050-06-005-217.conllu | ParlaMint-CZ_2016-10-26-ps2013-050-06-005-217.conllu | ParlaMint-CZ_2016-10-26-ps2013-050-06-005-217.u1.p1.s1 | 217.                                                                                                                                                                                                                            | 217 .                                                                                                                                                                                                                             | {}             |        1 |\n",
      "|  1 | /home/tajak/Parlamint-translation/Source-data/ParlaMint-CZ.conllu/ParlaMint-CZ.conllu/2016/ParlaMint-CZ_2016-10-26-ps2013-050-06-005-217.conllu | ParlaMint-CZ_2016-10-26-ps2013-050-06-005-217.conllu | ParlaMint-CZ_2016-10-26-ps2013-050-06-005-217.u1.p1.s2 | Návrh na volbu členů Nejvyššího kontrolního úřadu                                                                                                                                                                               | Návrh na volbu členů Nejvyššího kontrolního úřadu                                                                                                                                                                                 | {}             |        7 |\n",
      "|  2 | /home/tajak/Parlamint-translation/Source-data/ParlaMint-CZ.conllu/ParlaMint-CZ.conllu/2016/ParlaMint-CZ_2016-10-26-ps2013-050-06-005-217.conllu | ParlaMint-CZ_2016-10-26-ps2013-050-06-005-217.conllu | ParlaMint-CZ_2016-10-26-ps2013-050-06-005-217.u1.p2.s1 | Na lavice máte doručeno usnesení volební komise č. 139 ze dne 6. září a usnesení ze dne č. 141 ze dne 14. září.                                                                                                                 | Na lavice máte doručeno usnesení volební komise č . 139 ze dne 6 . září a usnesení ze dne č . 141 ze dne 14 . září .                                                                                                              | {}             |       23 |\n",
      "|  3 | /home/tajak/Parlamint-translation/Source-data/ParlaMint-CZ.conllu/ParlaMint-CZ.conllu/2016/ParlaMint-CZ_2016-10-26-ps2013-050-06-005-217.conllu | ParlaMint-CZ_2016-10-26-ps2013-050-06-005-217.conllu | ParlaMint-CZ_2016-10-26-ps2013-050-06-005-217.u1.p2.s2 | Pane předsedo volební komise, můžete se ujmout slova.                                                                                                                                                                           | Pane předsedo volební komise , můžete se ujmout slova .                                                                                                                                                                           | {}             |        8 |\n",
      "|  4 | /home/tajak/Parlamint-translation/Source-data/ParlaMint-CZ.conllu/ParlaMint-CZ.conllu/2016/ParlaMint-CZ_2016-10-26-ps2013-050-06-005-217.conllu | ParlaMint-CZ_2016-10-26-ps2013-050-06-005-217.conllu | ParlaMint-CZ_2016-10-26-ps2013-050-06-005-217.u2.p1.s1 | Za volební komisi jsme v tomto bodě postupně vlastně sloučili tři návrhy na nové kandidáty, protože teď během jarních a letních měsíců postupnými kroky skončila funkční období dovršením 65 let hned třem členům prezidia NKÚ. | Za volební komisi jsme v tomto bodě postupně vlastně sloučili tři návrhy na nové kandidáty , protože teď během jarních a letních měsíců postupnými kroky skončila funkční období dovršením 65 let hned třem členům prezidia NKÚ . | {}             |       35 |\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def conllu_to_df(parl_list, file_name_list, extracted_dataframe_path):\n",
    "\t\"\"\"\n",
    "\tTake the conllu files and extract relevant information. Save everything in a DataFrame.\n",
    "\n",
    "\tArgs:\n",
    "\t- parl_list: list of documents with their entire paths to be included (see step above).\n",
    "\t- file_name_list: list of names of the files (see step above)\n",
    "\t- extracted_dataframe_path: path to the output file\n",
    "\t\"\"\"\n",
    "\tfrom conllu import parse\n",
    "\timport pandas as pd\n",
    "\n",
    "\t# Create an empty df\n",
    "\tdf = pd.DataFrame({\"file_path\": [\"\"],\"file\": [\"\"], \"sentence_id\": [\"\"], \"text\": [\"\"], \"tokenized_text\": [\"\"], \"proper_nouns\": [\"\"]})\n",
    "\n",
    "\t# Check whether there are any problems with parsing the documents\n",
    "\t\"\"\"\n",
    "\t\n",
    "\terror_count = 0\n",
    "\tproblematic_doc_list = []\n",
    "\n",
    "\tfor doc in parl_list:\n",
    "\t\ttry:\n",
    "\t\t\t# Open the file\n",
    "\t\t\tdata = open(\"{}\".format(doc), \"r\").read()\n",
    "\n",
    "\t\t\tsentences = parse(data)\n",
    "\t\texcept:\n",
    "\t\t\terror_count += 1\n",
    "\t\t\tproblematic_doc_list.append(doc)\n",
    "\n",
    "\tprint(error_count)\n",
    "\tprint(problematic_doc_list)\n",
    "\t\"\"\"\n",
    "\t# Parse the data with CONLL-u parser\n",
    "\tfor doc in parl_list:\n",
    "\t\t# Open the file\n",
    "\t\tdata = open(\"{}\".format(doc), \"r\").read()\n",
    "\t\t\n",
    "\t\tsentences = parse(data)\n",
    "\n",
    "\t\tsentence_id_list = []\n",
    "\t\ttext_list = []\n",
    "\t\ttokenized_text_list = []\n",
    "\t\tproper_noun_list = []\n",
    "\n",
    "\t\tfor sentence in sentences:\n",
    "\t\t\t# Find sentence ids\n",
    "\t\t\tcurrent_sentence_id = sentence.metadata[\"sent_id\"]\n",
    "\t\t\tsentence_id_list.append(current_sentence_id)\n",
    "\n",
    "\t\t\t# Find text - if texts consists of multiword tokens, these tokens will appear as they are,\n",
    "\t\t\t# not separated into subwords\n",
    "\t\t\tcurrent_text = sentence.metadata[\"text\"]\n",
    "\t\t\ttext_list.append(current_text)\n",
    "\n",
    "\t\t\t# Create a string out of tokens\n",
    "\t\t\tcurrent_token_list = []\n",
    "\t\t\tword_dict = {}\n",
    "\n",
    "\t\t\tfor token in sentence:\n",
    "\t\t\t\t# Find multiword tokens and take their NER\n",
    "\t\t\t\tif type(token[\"id\"]) != int:\n",
    "\t\t\t\t\tmultiword_ner = token[\"misc\"][\"NER\"]\n",
    "\t\t\t\t\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t# Append to the tokenized text tokens that are not multiword tokens\n",
    "\t\t\t\t# (we append subtokens to the tokenized texts, not multiword tokens)\n",
    "\t\t\t\t\tcurrent_token_list.append(token[\"form\"])\n",
    "\t\t\t\t\t\n",
    "\n",
    "\t\t\t\t\t# Create a list of NE annotations with word indices.\n",
    "\t\t\t\t\t# I'll substract one from the word index,\n",
    "\t\t\t\t\t# because indexing in the CONLLU file starts with 1, not 0\n",
    "\t\t\t\t\tcurrent_index = int(token[\"id\"]) - 1\n",
    "\n",
    "\t\t\t\t\t# If the word does not have NER annotation,\n",
    "\t\t\t\t\t# take the annotation from the multiword token\n",
    "\t\t\t\t\tif token[\"misc\"] is None:\n",
    "\t\t\t\t\t\tcurrent_ner = multiword_ner\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tcurrent_ner = token[\"misc\"][\"NER\"]\n",
    "\n",
    "\t\t\t\t\t# Add information on the lemma if the NE is personal name\n",
    "\t\t\t\t\tif \"PER\" in current_ner:\n",
    "\t\t\t\t\t\tword_dict[current_index] = [token[\"form\"], token[\"lemma\"]]\n",
    "\n",
    "\t\t\tproper_noun_list.append(word_dict)\n",
    "\n",
    "\t\t\tcurrent_string = \" \".join(current_token_list)\n",
    "\n",
    "\t\t\ttokenized_text_list.append(current_string)\n",
    "\n",
    "\t\t\n",
    "\t\tnew_df = pd.DataFrame({\"sentence_id\": sentence_id_list, \"text\": text_list, \"tokenized_text\": tokenized_text_list, \"proper_nouns\": proper_noun_list})\n",
    "\n",
    "\t\tnew_df[\"file_path\"] = doc\n",
    "\n",
    "\t\t# Get the file name\n",
    "\t\tfile_name = file_name_list[parl_list.index(doc)]\n",
    "\t\tnew_df[\"file\"] = file_name\n",
    "\n",
    "\t\t# Merge df to the previous df\n",
    "\t\tdf = pd.concat([df, new_df])\n",
    "\t\n",
    "\t# Reset index\n",
    "\tdf = df.reset_index(drop=True)\n",
    "\n",
    "\t# Remove the first row\n",
    "\tdf = df.drop([0], axis=\"index\")\n",
    "\n",
    "\t# Reset index\n",
    "\tdf = df.reset_index(drop=True)\n",
    "\n",
    "\t# Add information on length\n",
    "\tdf[\"length\"] = df[\"text\"].str.split().str.len()\n",
    "\n",
    "\tprint(\"Number of words in the corpora: {}\".format(df[\"length\"].sum()))\n",
    "\n",
    "\t# Save the dataframe\n",
    "\tdf.to_csv(\"{}\".format(extracted_dataframe_path), sep=\"\\t\")\n",
    "\n",
    "\tprint(\"Dataframe saved as {}\".format(extracted_dataframe_path))\n",
    "\t\n",
    "\t# Show the results\n",
    "\tprint(df.describe(include=\"all\").to_markdown())\n",
    "\n",
    "\tprint(\"\\n\\n\\n\")\n",
    "\n",
    "\tprint(df.head().to_markdown())\n",
    "\n",
    "\tprint(\"\\n\\n\\n\")\n",
    "\t\n",
    "\treturn df\n",
    "\n",
    "#Extract information from the conllu files\n",
    "df = conllu_to_df(parl_list, file_name_list, extracted_dataframe_path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "translate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entire corpus has 42 sentences and 580 words.\n",
      "Translation started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tajak/Parlamint-translation/parlamint_env/lib/python3.8/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 512 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation completed. It took 0.1 minutes for 42 instances - 0.002380952380952381 minutes per one sentence.\n",
      "|    | file_path                                                                                                                                       | file                                                 | sentence_id                                            | text                                                                                                            | tokenized_text                                                                                                       | proper_nouns   |   length | translation                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "|---:|:------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------|:-------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------|:---------------|---------:|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "|  0 | /home/tajak/Parlamint-translation/Source-data/ParlaMint-CZ.conllu/ParlaMint-CZ.conllu/2016/ParlaMint-CZ_2016-10-26-ps2013-050-06-005-217.conllu | ParlaMint-CZ_2016-10-26-ps2013-050-06-005-217.conllu | ParlaMint-CZ_2016-10-26-ps2013-050-06-005-217.u1.p1.s1 | 217.                                                                                                            | 217 .                                                                                                                | {}             |        1 | 217.                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|  1 | /home/tajak/Parlamint-translation/Source-data/ParlaMint-CZ.conllu/ParlaMint-CZ.conllu/2016/ParlaMint-CZ_2016-10-26-ps2013-050-06-005-217.conllu | ParlaMint-CZ_2016-10-26-ps2013-050-06-005-217.conllu | ParlaMint-CZ_2016-10-26-ps2013-050-06-005-217.u1.p1.s2 | Návrh na volbu členů Nejvyššího kontrolního úřadu                                                               | Návrh na volbu členů Nejvyššího kontrolního úřadu                                                                    | {}             |        7 | Proposal for the election of members of the Supreme Audit Office                                                                                                                                                                                                                                                                                                                                                                   |\n",
      "|  2 | /home/tajak/Parlamint-translation/Source-data/ParlaMint-CZ.conllu/ParlaMint-CZ.conllu/2016/ParlaMint-CZ_2016-10-26-ps2013-050-06-005-217.conllu | ParlaMint-CZ_2016-10-26-ps2013-050-06-005-217.conllu | ParlaMint-CZ_2016-10-26-ps2013-050-06-005-217.u1.p2.s1 | Na lavice máte doručeno usnesení volební komise č. 139 ze dne 6. září a usnesení ze dne č. 141 ze dne 14. září. | Na lavice máte doručeno usnesení volební komise č . 139 ze dne 6 . září a usnesení ze dne č . 141 ze dne 14 . září . | {}             |       23 | On the bench you have received the resolution of the Election Commission No. Regulation (EC) No 139 of the European Parliament and of the Council of 6 September 2009 on the protection of natural persons with regard to the processing of personal data by the Community institutions and bodies and on the free movement of such data, and repealing Regulation (EC) No 45/2001 (OJ L 8, 12.1.2009, p. 1). 141 of 14 September. |\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The file is saved as results/CZ/ParlaMint-CZ-translated-missing-file.csv\n"
     ]
    }
   ],
   "source": [
    "def translate(lang_code, extracted_dataframe_path, translated_dataframe_path):\n",
    "\t\"\"\"\n",
    "\tThis function translates the text from the dataframe, created with the extract_text() function\n",
    "\twith OPUS-MT models using EasyNMT. It returns a dataframe with the translation.\n",
    "\n",
    "\tArgs:\n",
    "\t- opus_lang_code: the lang code to be used in the OPUS-MT model - use the one that performed the best in the comparison (see function choose_model())\n",
    "\t\"\"\"\n",
    "\timport pandas as pd\n",
    "\timport regex as re\n",
    "\tfrom easynmt import EasyNMT\n",
    "\tfrom IPython.display import display\n",
    "\timport time\n",
    "\n",
    "\t# Open the file, created in the previous step\n",
    "\tdf = pd.read_csv(\"{}\".format(extracted_dataframe_path), sep=\"\\t\", index_col=0)\n",
    "\n",
    "\t# Define the model\n",
    "\tmodel = EasyNMT('opus-mt')\n",
    "\n",
    "\tprint(\"Entire corpus has {} sentences and {} words.\".format(df[\"text\"].count(), df[\"length\"].sum()))\n",
    "\n",
    "\t# Create a list of sentences from the df\n",
    "\tsentence_list = df.text.to_list()\n",
    "\n",
    "\tlang_models_dict = {\"BG\": [\"bg\", \"sla\", \"zls\"], \"HR\": [\"zls\", \"sla\"], \"CZ\": [\"cs\"], \"DK\": [\"da\", \"gmq\", \"gem\"], \"NL\": [\"nl\", \"gem\", \"gmw\"], \"FR\": [\"fr\", \"itc\",\"roa\"], \"HU\": [\"hu\", \"fiu\", \"urj\"], \"IS\": [\"is\",\"gmq\", \"gem\"], \"IT\": [\"it\", \"roa\", \"itc\"], \"LV\": [\"lv\",\"bat\"], \"LT\": [\"bat\"], \"PL\": [\"pl\", \"sla\", \"zlw\"], \"SI\": [\"sla\"], \"ES\": [\"es\", \"roa\", \"itc\"], \"TR\": [\"tr\", \"trk\" ], \"AT\": [\"de\", \"gem\", \"gmw\"], \"ES-PV\": [\"eu\", \"mul\"], \"BA\": [\"sla\", \"zls\"], \"ES-CT\": [\"ca\", \"roa\", \"itc\"], \"EE\": [\"et\", \"urj\", \"fiu\"], \"FI\": [\"fi\", \"urj\", \"fiu\"], \"ES-GA\": [\"gl\", \"roa\", \"itc\"], \"GR\": [\"el\",\"grk\"], \"NO\": [\"gem\", \"gmq\"], \"PT\": [\"pt\", \"roa\", \"itc\"], \"RO\":[\"roa\", \"itc\"], \"RS\": [\"zls\", \"sla\"], \"SE\": [\"sv\", \"gmq\", \"gem\"], \"UA\":[\"uk\", \"sla\", \"zle\"]}\n",
    "\n",
    "\tprint(\"Translation started.\")\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\n",
    "\t#Translate the list of sentences - you need to provide the source language as it is in the name of the model - the opus_lang_code\n",
    "\tfor opus_lang_code in lang_models_dict[lang_code]:\n",
    "\t\ttranslation_list = model.translate(sentence_list, source_lang = \"{}\".format(opus_lang_code), target_lang='en')\n",
    "\n",
    "\ttranslation_time = round((time.time() - start_time)/60,2)\n",
    "\n",
    "\tprint(\"Translation completed. It took {} minutes for {} instances - {} minutes per one sentence.\".format(translation_time, len(sentence_list), translation_time/len(sentence_list)))\n",
    "\n",
    "\t# Add the translations to the df\n",
    "\tdf[\"translation\"] = translation_list\n",
    "\n",
    "\t# Display the df\n",
    "\tprint(df[:3].to_markdown())\n",
    "\n",
    "\tprint(\"\\n\\n\\n\")\n",
    "\n",
    "\t# Save the df\n",
    "\tdf.to_csv(\"{}\".format(translated_dataframe_path), sep=\"\\t\")\n",
    "\n",
    "\tprint(\"The file is saved as {}\".format(translated_dataframe_path))\n",
    "\n",
    "\treturn df\n",
    "\n",
    "df = translate(lang_code, extracted_dataframe_path, translated_dataframe_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prior file for alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tajak/Parlamint-translation/eflomal\n"
     ]
    }
   ],
   "source": [
    "# Create a prior file from alignments, created on the big file, to align the sentences based on them\n",
    "\n",
    "# move into the eflomal directory\n",
    "%cd eflomal\n",
    "\n",
    "# First, we need to create a fasttext format from the two sentence files\n",
    "!python3 mergefiles.py \"source_sentences_CZ.txt\" \"English_sentences_CZ.txt\" > cz-en.file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, create the prior file\n",
    "!python3 makepriors.py -i cz-en.file -f source-en_CZ.fwd -r source-en_CZ.rev --priors cz-en.priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, if you have another file to align, simply use the original sentences and the prior file:\n",
    "#!python3 align.py -s \"source_sentences_CZ.txt\" -t \"English_sentences_CZ.txt\" --priors cz-en.priors --model 3 -f source-en-CZ-add-file.fwd -r source-en-CZ-add-file.rev"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenize_translation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-07 15:21:23 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "========================\n",
      "\n",
      "2023-02-07 15:21:23 INFO: Use device: gpu\n",
      "2023-02-07 15:21:23 INFO: Loading: tokenize\n",
      "2023-02-07 15:21:23 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tajak/Parlamint-translation\n",
      "Tokenization of the translation started.\n",
      "Tokenization completed. It took 0.0 minutes.\n",
      "File saved as results/CZ/ParlaMint-CZ-translated-tokenized-missing-file.csv\n",
      "|    | file_path                                                                                                                                       | file                                                 | sentence_id                                            | text                                                                                                            | tokenized_text                                                                                                       | proper_nouns   |   length | translation                                                                                                                                                                                                                                                                                                                                                                                                                        | translation-tokenized                                                                                                                                                                                                                                                                                                                                                                                                                         | space-after-information                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
      "|---:|:------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------|:-------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------|:---------------|---------:|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "|  0 | /home/tajak/Parlamint-translation/Source-data/ParlaMint-CZ.conllu/ParlaMint-CZ.conllu/2016/ParlaMint-CZ_2016-10-26-ps2013-050-06-005-217.conllu | ParlaMint-CZ_2016-10-26-ps2013-050-06-005-217.conllu | ParlaMint-CZ_2016-10-26-ps2013-050-06-005-217.u1.p1.s1 | 217.                                                                                                            | 217 .                                                                                                                | {}             |        1 | 217.                                                                                                                                                                                                                                                                                                                                                                                                                               | 217 .                                                                                                                                                                                                                                                                                                                                                                                                                                         | ['No', 'Last']                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n",
      "|  1 | /home/tajak/Parlamint-translation/Source-data/ParlaMint-CZ.conllu/ParlaMint-CZ.conllu/2016/ParlaMint-CZ_2016-10-26-ps2013-050-06-005-217.conllu | ParlaMint-CZ_2016-10-26-ps2013-050-06-005-217.conllu | ParlaMint-CZ_2016-10-26-ps2013-050-06-005-217.u1.p1.s2 | Návrh na volbu členů Nejvyššího kontrolního úřadu                                                               | Návrh na volbu členů Nejvyššího kontrolního úřadu                                                                    | {}             |        7 | Proposal for the election of members of the Supreme Audit Office                                                                                                                                                                                                                                                                                                                                                                   | Proposal for the election of members of the Supreme Audit Office                                                                                                                                                                                                                                                                                                                                                                              | ['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Last']                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n",
      "|  2 | /home/tajak/Parlamint-translation/Source-data/ParlaMint-CZ.conllu/ParlaMint-CZ.conllu/2016/ParlaMint-CZ_2016-10-26-ps2013-050-06-005-217.conllu | ParlaMint-CZ_2016-10-26-ps2013-050-06-005-217.conllu | ParlaMint-CZ_2016-10-26-ps2013-050-06-005-217.u1.p2.s1 | Na lavice máte doručeno usnesení volební komise č. 139 ze dne 6. září a usnesení ze dne č. 141 ze dne 14. září. | Na lavice máte doručeno usnesení volební komise č . 139 ze dne 6 . září a usnesení ze dne č . 141 ze dne 14 . září . | {}             |       23 | On the bench you have received the resolution of the Election Commission No. Regulation (EC) No 139 of the European Parliament and of the Council of 6 September 2009 on the protection of natural persons with regard to the processing of personal data by the Community institutions and bodies and on the free movement of such data, and repealing Regulation (EC) No 45/2001 (OJ L 8, 12.1.2009, p. 1). 141 of 14 September. | On the bench you have received the resolution of the Election Commission No. Regulation ( EC ) No 139 of the European Parliament and of the Council of 6 September 2009 on the protection of natural persons with regard to the processing of personal data by the Community institutions and bodies and on the free movement of such data , and repealing Regulation ( EC ) No 45/2001 ( OJ L 8 , 12.1.2009 , p. 1 ) . 141 of 14 September . | ['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'No', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Last'] |\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "\n",
    "def tokenize_translation(translated_dataframe_path, translated_tokenized_dataframe_path):\n",
    "\timport stanza\n",
    "\timport time\n",
    "\timport gc\n",
    "\timport torch\n",
    "\tfrom stanza.pipeline.core import DownloadMethod\n",
    "\t\n",
    "\tprint(\"Tokenization of the translation started.\")\n",
    "\n",
    "\tnlp = stanza.Pipeline(lang='en', processors='tokenize', tokenize_no_ssplit = True, download_method=DownloadMethod.REUSE_RESOURCES, use_gpu=True)\n",
    "\n",
    "\t# Apply tokenization to English translation and add the sentences to the df\n",
    "\t# Open the df\n",
    "\tdf = pd.read_csv(\"{}\".format(translated_dataframe_path), sep=\"\\t\", index_col = 0)\n",
    "\n",
    "\t# Save also the information on whether there is a space after or before punctuation\n",
    "\t# which we will need later, to remove unnecessary spaces\n",
    "\tEn_sentences = df.translation.to_list()\n",
    "\n",
    "\ttokenized_sentences = []\n",
    "\tspace_after_list = []\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\n",
    "\tfor i in En_sentences:\n",
    "\t\tdoc = nlp(i).to_dict()\n",
    "\t\tcurrent_sentence_list = []\n",
    "\t\tcurrent_space_after_list = []\n",
    "\n",
    "\t\t# Define a list of start_char and end_char\n",
    "\t\tstart_chars = []\n",
    "\t\tend_chars = []\n",
    "\n",
    "\t\t# Loop through the tokens in the sentence and add them to a current sentence list\n",
    "\t\tfor sentence in doc:\n",
    "\t\t\tfor word in sentence:\n",
    "\t\t\t\tcurrent_sentence_list.append(word[\"text\"])\n",
    "\n",
    "\t\t\t\t# Add information on start and end chars to the list\n",
    "\t\t\t\tstart_chars.append(word[\"start_char\"])\n",
    "\t\t\t\tend_chars.append(word[\"end_char\"])\n",
    "\t\t\t\n",
    "\t\t# Now loop through the start_char and end_char lists and find instances\n",
    "\t\t# where the end_char of one word is the same as the start_char of the next one\n",
    "\t\t# this means there is no space between them\n",
    "\t\tfor char_index in range(len(start_chars)-1):\n",
    "\t\t\tif end_chars[char_index] == start_chars[(char_index+1)]:\n",
    "\t\t\t\tcurrent_space_after_list.append(\"No\")\n",
    "\t\t\telse:\n",
    "\t\t\t\tcurrent_space_after_list.append(\"Yes\")\n",
    "\t\n",
    "\t\t# This loop is not possible for the end token, so let's add information for the last token\n",
    "\t\t# just to avoid errors due to different lengths of lists\n",
    "\t\tcurrent_space_after_list.append(\"Last\")\n",
    "\n",
    "\t\t# Join the list into a space-separated string\n",
    "\t\tcurrent_string = \" \".join(current_sentence_list)\n",
    "\n",
    "\t\ttokenized_sentences.append(current_string)\n",
    "\n",
    "\t\tspace_after_list.append(current_space_after_list)\n",
    "\n",
    "\t# Add the result to the df\n",
    "\tdf[\"translation-tokenized\"] = tokenized_sentences\n",
    "\tdf[\"space-after-information\"] = space_after_list\n",
    "\n",
    "\t# Save the df\n",
    "\tdf.to_csv(\"{}\".format(translated_tokenized_dataframe_path), sep=\"\\t\")\n",
    "\n",
    "\tend_time = round((time.time() - start_time)/60,2)\n",
    "\n",
    "\tprint(\"Tokenization completed. It took {} minutes.\".format(end_time))\n",
    "\n",
    "\t#Delete nlp element, clean memory\n",
    "\tdel nlp\n",
    "\ttorch.cuda.empty_cache()\n",
    "\tgc.collect()\n",
    "\ttorch.cuda.empty_cache()\n",
    "\n",
    "\tprint(\"File saved as {}\".format(translated_tokenized_dataframe_path))\n",
    "\t\n",
    "\treturn df\n",
    "\n",
    "df = tokenize_translation(translated_dataframe_path, translated_tokenized_dataframe_path)\n",
    "\n",
    "print(df.head(3).to_markdown())\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "align - change the alignment files accordingly to your language in the code (after \"# Align sentences with eflomal and get out a file with alignments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Alignment started.\n",
      "\n",
      "Number of aligned sentences: 42\n",
      "\n",
      "\n",
      "Issue: index 18: ['.', '.']\n",
      "Alignment completed. It took 0.06 minutes.\n",
      "|                                                                             |   substituted_pairs |\n",
      "|:----------------------------------------------------------------------------|--------------------:|\n",
      "| 0                                                                           |                  36 |\n",
      "| [('Ing.', 'Ing')]                                                           |                   1 |\n",
      "| [('Karl', 'Karel'), ('Sehor', 'Sehoř'), ('Koucky', 'Koucký')]               |                   1 |\n",
      "| [('Kuf', 'Kufa')]                                                           |                   1 |\n",
      "| [('Kolohradník', 'Kolovratník')]                                            |                   1 |\n",
      "| [('Sharapatka', 'Šarapatka'), ('Free', 'Volný'), ('Miroslav', 'Miroslava')] |                   1 |\n",
      "| [('Kolovrobník', 'Kolovratník')]                                            |                   1 |\n",
      "\n",
      "\n",
      "\n",
      "Number of errors:\n",
      "(1, 19)\n",
      "\n",
      "\n",
      "\n",
      "Example of sentences with substituted words.\n",
      "|    | file_path                                                                                                                                       | file                                                 | sentence_id                                            | text                                                                                                                             | tokenized_text                                                                                                                       | proper_nouns                                                                                                                                                                          |   length | translation                                                                                                                            | translation-tokenized                                                                                                                        | space-after-information                                                                                                                                                                                       | fwd_align_dict                                                                                                                                                                                                                    | bwd_align_dict                                                                                                                                                                                                                    | alignments                                                                                                                                                                      | new_translations                                                                                                                              | substitution_info                                                                                                                                                                                                                                     | substituted_pairs                                             | substituted_words                       | errors                      | source_indices                                                                                                                                                                                                                                                                                                                                       |\n",
      "|---:|:------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------|:-------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------:|:---------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------|:----------------------------------------|:----------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "|  5 | /home/tajak/Parlamint-translation/Source-data/ParlaMint-CZ.conllu/ParlaMint-CZ.conllu/2016/ParlaMint-CZ_2016-10-26-ps2013-050-06-005-217.conllu | ParlaMint-CZ_2016-10-26-ps2013-050-06-005-217.conllu | ParlaMint-CZ_2016-10-26-ps2013-050-06-005-217.u2.p1.s2 | 5. června svoji funkci ukončil pan Ing. Antonín Macháček, na jeho místo je nominován Ing. Jan Kinšt.                             | 5 . června svoji funkci ukončil pan Ing . Antonín Macháček , na jeho místo je nominován Ing . Jan Kinšt .                            | {7: ['Ing', 'Ing'], 8: ['.', '.'], 9: ['Antonín', 'Antonín'], 10: ['Macháček', 'Macháček'], 17: ['Ing', 'Ing'], 18: ['.', '.'], 19: ['Jan', 'Jan'], 20: ['Kinšt', 'Kinšt']}           |       17 | On 5 June Mr Ing finished his duties. Antonín Macháček, in his place is nominated Ing. Jan Kinšt.                                      | On 5 June Mr Ing finished his duties . Antonín Macháček , in his place is nominated Ing. Jan Kinšt .                                         | ['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Last']                                                             | {1: '2', 2: '1', 3: '3', 4: '7', 5: '8', 6: '6', 7: '4', 8: '5', 9: '9', 10: '10', 11: '11', 12: '12', 13: '13', 14: '14', 15: '15', 16: '16', 17: '17', 18: '18', 19: '20', 20: '21', 21: '22'}                                  | {2: '1', 1: '2', 3: '3', 7: '4', 8: '5', 6: '6', 4: '7', 5: '8', 9: '9', 10: '10', 11: '11', 12: '12', 13: '13', 14: '14', 15: '15', 16: '16', 17: '17', 18: '18', 19: '20', 20: '21', 21: '22'}                                  | {0: 1, 1: 0, 2: 2, 3: 6, 4: 7, 5: 5, 6: 3, 7: 4, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 19: 18, 20: 19, 21: 20}                            | On 5 June Mr Ing finished his duties . Antonín Macháček , in his place is nominated Ing Jan Kinšt .                                           | [\"No substitution: ('Ing', 'Ing')\", \"No substitution: ('.', '.')\", \"No substitution: ('Antonín', 'Antonín')\", \"No substitution: ('Macháček', 'Macháček')\", ('Ing.', 'Ing'), \"No substitution: ('Jan', 'Jan')\", \"No substitution: ('Kinšt', 'Kinšt')\"] | [('Ing.', 'Ing')]                                             | {18: 'Ing.'}                            | Issue: index 18: ['.', '.'] | [['5', 1], ['.', 2], ['června', 3], ['svoji', 4], ['funkci', 5], ['ukončil', 6], ['pan', 7], ['Ing', 8], ['.', 9], ['Antonín', 10], ['Macháček', 11], [',', 12], ['na', 13], ['jeho', 14], ['místo', 15], ['je', 16], ['nominován', 17], ['Ing', 18], ['.', 19], ['Jan', 20], ['Kinšt', 21], ['.', 22]]                                              |\n",
      "|  6 | /home/tajak/Parlamint-translation/Source-data/ParlaMint-CZ.conllu/ParlaMint-CZ.conllu/2016/ParlaMint-CZ_2016-10-26-ps2013-050-06-005-217.conllu | ParlaMint-CZ_2016-10-26-ps2013-050-06-005-217.conllu | ParlaMint-CZ_2016-10-26-ps2013-050-06-005-217.u2.p1.s3 | Dále 19. července ze stejného důvodu skončilo funkční období panu Ing. Karlu Sehořovi a nominován je dnes Ing. Stanislav Koucký. | Dále 19 . července ze stejného důvodu skončilo funkční období panu Ing . Karlu Sehořovi a nominován je dnes Ing . Stanislav Koucký . | {11: ['Ing', 'Ing'], 12: ['.', '.'], 13: ['Karlu', 'Karel'], 14: ['Sehořovi', 'Sehoř'], 19: ['Ing', 'Ing'], 20: ['.', '.'], 21: ['Stanislav', 'Stanislav'], 22: ['Koucký', 'Koucký']} |       20 | Furthermore, on 19 July, Mr Ing's term of office ended for the same reason. Karl Sehor and the nominee is Ing today. Stanislav Koucky. | Furthermore , on 19 July , Mr Ing 's term of office ended for the same reason . Karl Sehor and the nominee is Ing today . Stanislav Koucky . | ['No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'No', 'Last'] | {1: '1', 3: '3', 4: '2', 5: '4', 7: '11', 8: '12', 10: '9', 12: '10', 13: '8', 14: '5', 16: '6', 17: '7', 18: '13', 19: '14', 20: '15', 21: '16', 23: '17', 24: '18', 25: '20', 26: '19', 27: '21', 28: '22', 29: '23', 30: '24'} | {1: '1', 4: '2', 3: '3', 5: '4', 14: '5', 16: '6', 17: '7', 13: '8', 12: '9', 10: '10', 7: '11', 8: '12', 18: '13', 19: '14', 20: '15', 21: '16', 23: '17', 24: '18', 26: '19', 25: '20', 27: '21', 28: '22', 29: '23', 30: '24'} | {0: 0, 1: 3, 2: 2, 3: 4, 4: 13, 5: 15, 6: 16, 7: 12, 8: 11, 9: 9, 10: 6, 11: 7, 12: 17, 13: 18, 14: 19, 15: 20, 16: 22, 17: 23, 18: 25, 19: 24, 20: 26, 21: 27, 22: 28, 23: 29} | Furthermore , on 19 July , Mr Ing 's term of office ended for the same reason . Karel Sehoř and the nominee is Ing today . Stanislav Koucký . | [\"No substitution: ('Ing', 'Ing')\", \"No substitution: ('.', '.')\", ('Karl', 'Karel'), ('Sehor', 'Sehoř'), \"No substitution: ('Ing', 'Ing')\", \"No substitution: ('.', '.')\", \"No substitution: ('Stanislav', 'Stanislav')\", ('Koucky', 'Koucký')]      | [('Karl', 'Karel'), ('Sehor', 'Sehoř'), ('Koucky', 'Koucký')] | {19: 'Karl', 20: 'Sehor', 29: 'Koucky'} | No                          | [['Dále', 1], ['19', 2], ['.', 3], ['července', 4], ['ze', 5], ['stejného', 6], ['důvodu', 7], ['skončilo', 8], ['funkční', 9], ['období', 10], ['panu', 11], ['Ing', 12], ['.', 13], ['Karlu', 14], ['Sehořovi', 15], ['a', 16], ['nominován', 17], ['je', 18], ['dnes', 19], ['Ing', 20], ['.', 21], ['Stanislav', 22], ['Koucký', 23], ['.', 24]] |\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary from the returned alignment files which will be added to each word in the final conllu\n",
    "\n",
    "def alignment_file_to_target_dict(file):\n",
    "\t\"\"\"\n",
    "\tThe output of the eflomal aligner is in the source to target direction. We want to get the alignments in the other direction\n",
    "\tand for each target word add to the conllu its aligned source word index (as it appears in conllu). In conllu, indices start\n",
    "\twith 1, not 0. So, we take the eflomal files, reverse the order and create dictionaries with target indexes as keys\n",
    "\tand source indexes as values. If there are more than one words aligned to the same target word, it looks like this: '1, 2'.\n",
    "\tWe use the conllu indexes which means that we add 1 to each index in the alingment pairs. \n",
    "\n",
    "\tArgs:\n",
    "\t\t- file: the path to the .fwd and .rev file that is produced by the eflomal tool\n",
    "\n",
    "\tThe result is a list of dictionaries, each dictionary corresponds to one sentence.\n",
    "\t\"\"\"\n",
    "\t# Create target alignments from the source alignment direction (by changing the direction in the file)\n",
    "\taligns_list_target = open(file, \"r\").readlines()\n",
    "\taligns_list_target = [i.replace(\"\\n\", \"\") for i in aligns_list_target]\n",
    "\taligns_list_target = [i.split(\" \") for i in aligns_list_target]\n",
    "\n",
    "\taligns_list_target_dict_list = []\n",
    "\n",
    "\t# Loop through the alignments for sentences\n",
    "\tfor i in aligns_list_target:\n",
    "\t\t# Create a dictionary for each sentence\n",
    "\t\tcurrent_sentence_align = {}\n",
    "\t\t# If alignment line is empty, keep the dictionary empty\n",
    "\t\tif len(i) == 1 and len(i[0]) == 0:\n",
    "\t\t\tcurrent_sentence_align = {}\n",
    "\t\telse:\n",
    "\t\t\t# For each alignment pair in the sentence:\n",
    "\t\t\tfor pair in i:\n",
    "\t\t\t\t# Split the pair: result is a list of lists with source index as the first element\n",
    "\t\t\t\t# and target index as the second element: [[0,0], [1,2], [1,3]]\n",
    "\t\t\t\tcurrent_pair = pair.split(\"-\")\n",
    "\n",
    "\t\t\t\t# Get the indices for target and source and add 1 to them (to get the conllu indices)\n",
    "\t\t\t\tcurrent_t_index = int(current_pair[1]) + 1\n",
    "\t\t\t\tcurrent_s_index = int(current_pair[0]) + 1\n",
    "\n",
    "\t\t\t\t# Check whether the target index is already aligned to anything (a case of 1-to-many alignment),\n",
    "\t\t\t\t# if not, save it as a key and save the source index as value.\n",
    "\t\t\t\tif current_sentence_align.get(current_t_index, None) == None:\n",
    "\t\t\t\t\tcurrent_sentence_align[current_t_index] = str(current_s_index)\n",
    "\t\t\t\t# If the index was already aligned to a previous source word, add the additional source word alignment as a string\n",
    "\t\t\t\t# (result: {0: \"1, 2\"))\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tcurrent_sentence_align[current_t_index] += str(\", \")\n",
    "\t\t\t\t\tcurrent_sentence_align[current_t_index] += str(current_s_index)\n",
    "\n",
    "\t\taligns_list_target_dict_list.append(current_sentence_align)\n",
    "\n",
    "\treturn aligns_list_target_dict_list\n",
    "\n",
    "def correct_proper_nouns(translated_tokenized_dataframe_path, final_dataframe, lang_code):\n",
    "\t\"\"\"\n",
    "\tThis function takes the translated text and the source text, aligns words with eflomal and corrects proper nouns.\n",
    "\tIt takes the dataframe that was created in the function extract_text() and to which the translation was added\n",
    "\tin the function translate().\n",
    "\n",
    "\tTo use eflomal, you need to install it first:\n",
    "\t!git clone https://github.com/robertostling/eflomal\n",
    "\t%cd eflomal\n",
    "\t!make\n",
    "\t!sudo make install\n",
    "\t!python3 setup.py install\n",
    "\n",
    "\tIn case you don't have sudo permission, you can skip !sudo make install. I did, and I also used a virtual environment (venv), and managed to install eflomal.\n",
    "\n",
    "\t\"\"\"\n",
    "\timport pandas as pd\n",
    "\timport re\n",
    "\timport ast\n",
    "\tfrom IPython.display import display\n",
    "\timport time\n",
    "\timport subprocess\n",
    "\timport os\n",
    "\n",
    "\t# Open the file, created in the previous step\n",
    "\tdf = pd.read_csv(\"{}\".format(translated_tokenized_dataframe_path), sep=\"\\t\", index_col=0)\n",
    "\n",
    "\t# Move into the eflomal folder\n",
    "\tos.chdir(\"/home/tajak/Parlamint-translation/eflomal\")\n",
    "\n",
    "\t# Then we need to create files for all texts and all translations\n",
    "\tsource_sentences = open(\"source_sentences_{}_missing_file.txt\".format(lang_code), \"w\")\n",
    "\tEnglish_sentences = open(\"English_sentences_{}_missing_file.txt\".format(lang_code), \"w\")\n",
    "\n",
    "\tfor i in df[\"tokenized_text\"].to_list():\n",
    "\t\tsource_sentences.write(i)\n",
    "\t\tsource_sentences.write(\"\\n\")\n",
    "\n",
    "\tfor i in df[\"translation-tokenized\"].to_list():\n",
    "\t\tEnglish_sentences.write(i)\n",
    "\t\tEnglish_sentences.write(\"\\n\")\n",
    "\n",
    "\tsource_sentences.close()\n",
    "\tEnglish_sentences.close()\n",
    "\n",
    "\tprint(\"\\n\\n\")\n",
    "\tprint(\"Alignment started.\")\n",
    "\tstart_time = time.time()\n",
    "\n",
    "\t# Align sentences with eflomal and get out a file with alignments\n",
    "\t!python3 align.py -s \"source_sentences_CZ_missing_file.txt\" -t \"English_sentences_CZ_missing_file.txt\" --priors cz-en.priors --model 3 -f source-en-CZ-add-file.fwd -r source-en-CZ-add-file.rev\n",
    "\n",
    "\t# Create a list of dictionaries of alignments from the returned files which will be added to the final conllu for each word\n",
    "\tforward_alignment_dict_list = alignment_file_to_target_dict(\"source-en-{}-add-file.fwd\".format(lang_code))\n",
    "\tbackward_alignment_dict_list = alignment_file_to_target_dict(\"source-en-{}-add-file.rev\".format(lang_code))\n",
    "\n",
    "\t# Add to the df\n",
    "\tdf[\"fwd_align_dict\"] = forward_alignment_dict_list\n",
    "\tdf[\"bwd_align_dict\"] = backward_alignment_dict_list\n",
    "\n",
    "\t# Create forward target alignments from the source alignment direction (by changing the direction in the rev file)\n",
    "\taligns_list = open(\"source-en-{}-add-file.rev\".format(lang_code), \"r\").readlines()\n",
    "\taligns_list = [i.replace(\"\\n\", \"\") for i in aligns_list]\n",
    "\n",
    "\t# Continue with processing the list to create the final alignments format which I'll use to correct proper names\n",
    "\taligns_list = [i.split(\" \") for i in aligns_list]\n",
    "\n",
    "\tfor i in aligns_list:\n",
    "\t\t# If alignment line is empty, keep the dictionary empty\n",
    "\t\tif len(i) == 1 and len(i[0]) == 0:\n",
    "\t\t\taligns_list[aligns_list.index(i)] = []\n",
    "\t\telse:\n",
    "\t\t\tfor pair in i:\n",
    "\t\t\t\tcurrent_pair = pair.split(\"-\")\n",
    "\t\t\t\ti[i.index(pair)] = {int(current_pair[0]): int(current_pair[1])}\n",
    "\t\n",
    "\tfinal_aligns = []\n",
    "\n",
    "\t# Create a dictionary out of the rev alignments\n",
    "\tfor i in aligns_list:\n",
    "\t\tcurrent_line = {}\n",
    "\n",
    "\t\ttry:\n",
    "\t\t\tfor element in i:\n",
    "\t\t\t\ta = list(element.items())[0][0]\n",
    "\t\t\t\tb = list(element.items())[0][1]\n",
    "\t\t\t\tcurrent_line[a] = b\n",
    "\t\t\n",
    "\t\t\t# Check whether the number of pairs in the list is the same as number of items\n",
    "\t\t\tif len(i) != len(list(current_line.items())):\n",
    "\t\t\t\tprint(\"Not okay:\")\n",
    "\t\t\t\tprint(i)\n",
    "\t\t\t\tprint(current_line)\n",
    "\n",
    "\t\t\tfinal_aligns.append(current_line)\n",
    "\t\t\n",
    "\t\texcept:\n",
    "\t\t\tprint(\"error\")\n",
    "\t\t\tprint(aligns_list.index(i))\n",
    "\t\t\tprint(i)\n",
    "\t\t\tfinal_aligns.append(\"Error\")\n",
    "\t\t\n",
    "\tprint(\"\\nNumber of aligned sentences: {}\\n\\n\".format(len(final_aligns)))\n",
    "\n",
    "\t# Add a to the df\n",
    "\tdf[\"alignments\"] = final_aligns\n",
    "\n",
    "\t# When we open the dataframe file, the dictionaries with proper names changed into strings - Change strings in the column proper_nouns into dictionaries\n",
    "\n",
    "\tdf[\"proper_nouns\"] = df.proper_nouns.astype(\"str\")\n",
    "\tdf[\"proper_nouns\"] = df.proper_nouns.apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "\t# Change nan values in the proper_nouns columns\n",
    "\tdf = df.fillna(0)\n",
    "\n",
    "\t# Substitute words in the translation based on alignments\n",
    "\tintermediate_list = list(zip(df[\"translation-tokenized\"], df[\"proper_nouns\"], df[\"alignments\"]))\n",
    "\n",
    "\tnew_translations = []\n",
    "\tsubstituted_all_info = []\n",
    "\tsubstituted_only = []\n",
    "\tsubstituted_words = []\n",
    "\n",
    "\t# Add information whether an error occurred\n",
    "\terror_list = []\n",
    "\n",
    "\tfor i in intermediate_list:\n",
    "\t\tcurrent_substituted_list = []\n",
    "\t\tcurrent_substituted_only = []\n",
    "\t\tcurrent_substituted_words = {}\n",
    "\t\tcurrent_error = \"No\"\n",
    "\n",
    "\t\t# If no proper names were detected, do not change the translation\n",
    "\t\tif i[1] == 0:\n",
    "\t\t\tnew_translations.append(i[0])\n",
    "\t\t\n",
    "\t\telse:\n",
    "\t\t\tcurrent_translation = i[0]\n",
    "\n",
    "\t\t\t# Substitute the word with the source lemma based on the index - loop through the proper nouns to be changed\n",
    "\t\t\tfor word_index in list(i[1].keys()):\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\t# split the translation into list of words\n",
    "\t\t\t\t\tword_list = current_translation.split()\n",
    "\n",
    "\t\t\t\t\t# Get index of the substituted word\n",
    "\t\t\t\t\tsubstituted_word_index = i[2][word_index]\n",
    "\n",
    "\t\t\t\t\t# Get the lemma to substitute the word with\n",
    "\t\t\t\t\tcorrect_lemma = i[1][word_index][1]\n",
    "\n",
    "\t\t\t\t\t# If the substitute word and lemma are not the same, get substituted word and its match\n",
    "\t\t\t\t\tif word_list[substituted_word_index] != correct_lemma:\n",
    "\t\t\t\t\t\tcurrent_substituted_list.append((word_list[substituted_word_index], correct_lemma))\n",
    "\t\t\t\t\t\tcurrent_substituted_only.append((word_list[substituted_word_index], correct_lemma))\n",
    "\n",
    "\t\t\t\t\t\t# Save information on which word was substituted with its conllu index (index + 1) as the key\n",
    "\t\t\t\t\t\tcurrent_substituted_words[int(substituted_word_index+1)] = word_list[substituted_word_index]\n",
    "\n",
    "\t\t\t\t\t\t# Substitute the word in the word list\n",
    "\t\t\t\t\t\tword_list[substituted_word_index] = correct_lemma\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t# Add information that substitution was not performed\n",
    "\t\t\t\t\t\tcurrent_substituted_list.append(f\"No substitution: {word_list[substituted_word_index], correct_lemma}\")\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t# Change the translation by merging the words back into a string\n",
    "\t\t\t\t\tcurrent_translation = \" \".join(word_list)\n",
    "\n",
    "\t\t\t\texcept:\n",
    "\t\t\t\t\tprint(f\"Issue: index {word_index}: {i[1][word_index]}\")\n",
    "\t\t\t\t\tcurrent_error = f\"Issue: index {word_index}: {i[1][word_index]}\"\n",
    "\n",
    "\t\t\t# After the loop through proper nouns, save the new translation\n",
    "\t\t\tnew_translations.append(current_translation)\n",
    "\t\t\n",
    "\t\t# Add information on what was substituted\n",
    "\t\tif len(substituted_all_info) != 0:\n",
    "\t\t\tsubstituted_all_info.append(current_substituted_list)\n",
    "\t\telse:\n",
    "\t\t\tsubstituted_all_info.append(0)\n",
    "\n",
    "\t\tif len(current_substituted_only) != 0:\n",
    "\t\t\tsubstituted_only.append(current_substituted_only)\n",
    "\t\telse:\n",
    "\t\t\tsubstituted_only.append(0)\n",
    "\n",
    "\t\terror_list.append(current_error)\n",
    "\n",
    "\t\tsubstituted_words.append(current_substituted_words)\n",
    "\n",
    "\n",
    "\t# Add to the df\n",
    "\tdf[\"new_translations\"] = new_translations\n",
    "\tdf[\"substitution_info\"] = substituted_all_info\n",
    "\tdf[\"substituted_pairs\"] = substituted_only\n",
    "\tdf[\"substituted_words\"] = substituted_words\n",
    "\tdf[\"errors\"] = error_list\n",
    "\n",
    "\tend_time = round((time.time() - start_time)/60,2)\n",
    "\n",
    "\tprint(\"Alignment completed. It took {} minutes.\".format(end_time))\n",
    "\n",
    "\t# Change the working directory once again\n",
    "\tos.chdir(\"..\")\n",
    "\n",
    "\t# Add the word list with indices to the df\n",
    "\ttokenized_text_list = df.tokenized_text.to_list()\n",
    "\ttokenized_text_list = [i.split(\" \") for i in tokenized_text_list]\n",
    "\ttokenized_text_dict_list = []\n",
    "\n",
    "\tfor sentence in tokenized_text_list:\n",
    "\t\tsentence_list = []\n",
    "\t\tcounter = 1\n",
    "\t\tfor word in sentence:\n",
    "\t\t\tsentence_list.append([word, counter])\n",
    "\t\t\tcounter += 1\n",
    "\t\ttokenized_text_dict_list.append(sentence_list)\n",
    "\n",
    "\tdf[\"source_indices\"] = tokenized_text_dict_list\n",
    "\n",
    "\t# Save the df\n",
    "\tdf.to_csv(\"{}\".format(final_dataframe), sep=\"\\t\")\n",
    "\n",
    "\t# Display most common substitutions\n",
    "\tdf_substituted = df[df[\"proper_nouns\"] != \"0\"]\n",
    "\tprint(df_substituted.substituted_pairs.value_counts()[:20].to_markdown())\n",
    "\tprint(\"\\n\\n\")\n",
    "\n",
    "\treturn df\n",
    "\n",
    "\n",
    "\n",
    "df = correct_proper_nouns(translated_tokenized_dataframe_path, final_dataframe, lang_code)\n",
    "\n",
    "# See if there were any errors in word substitution\n",
    "print(\"Number of errors:\")\n",
    "print(df[df[\"errors\"]!=\"No\"].shape)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "# See example of sentences with substituted words\n",
    "print(\"Example of sentences with substituted words.\")\n",
    "print(df[df[\"substituted_pairs\"]!= 0][:2].to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create_conllu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-07 15:30:43 WARNING: Can not find mwt: default from official model list. Ignoring it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-07 15:30:44 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| pos       | combined |\n",
      "| lemma     | combined |\n",
      "| ner       | conll03  |\n",
      "========================\n",
      "\n",
      "2023-02-07 15:30:44 INFO: Use device: gpu\n",
      "2023-02-07 15:30:44 INFO: Loading: tokenize\n",
      "2023-02-07 15:30:44 INFO: Loading: pos\n",
      "2023-02-07 15:30:44 INFO: Loading: lemma\n",
      "2023-02-07 15:30:44 INFO: Loading: ner\n",
      "2023-02-07 15:30:44 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParlaMint-CZ_2016-10-26-ps2013-050-06-005-217.conllu processed and saved.\n",
      "Final file /home/tajak/Parlamint-translation/Final-data/ParlaMint-CZ.conllu/ParlaMint-CZ.conllu/2016/ParlaMint-CZ_2016-10-26-ps2013-050-06-005-217.conllu is saved.\n",
      "Current running time: 0.03\n",
      "Processing completed. It took 0.03 minutes.\n"
     ]
    }
   ],
   "source": [
    "def create_conllu(file, lang_code, main_path, final_dataframe, nlp):\n",
    "\t\"\"\"\n",
    "\tThe function takes the dataframe (df), created in previous steps and takes only the instances from the df that belong\n",
    "\tto the file that is in the argument. It linguistically processes the translated sentences from the file and saves the file.\n",
    "\tThen we add additional information (metadata and NER annotations) to it with the conllu parser and save the final conllu file.\n",
    "\n",
    "\tArgs:\n",
    "\t\t- file (str): file name from the files list (see above)\n",
    "\t\t- lang_code (str): the lang code that is used in the names of the files, it should be the same as for extract_text()\n",
    "\t\"\"\"\n",
    "\n",
    "\t# Process all sentences in the dataframe and save them to a conllu file\n",
    "\tfrom stanza.utils.conll import CoNLL\n",
    "\timport stanza\n",
    "\tfrom conllu import parse\n",
    "\timport ast\n",
    "\timport regex as re\n",
    "\timport os\n",
    "\timport pandas as pd\n",
    "\n",
    "\t# Use the dataframe, created in previous steps\n",
    "\tdf = pd.read_csv(\"{}\".format(final_dataframe), sep=\"\\t\", index_col = 0)\n",
    "\n",
    "\t# Filter out only instances from the file in question\n",
    "\tdf = df[df[\"file\"] == file]\n",
    "\n",
    "\t# Add information on the target path\n",
    "\tdf[\"target_path\"] = df.file_path.str.replace(\"Source-data\", \"Final-data\")\n",
    "\n",
    "\t# Get target path\n",
    "\ttarget_path = list(df.target_path.unique())[0]\n",
    "\n",
    "\t# When we open the dataframe file, the lists and dictionaries turn into strings - change them back\n",
    "\tfor column in [\"space-after-information\", 'fwd_align_dict', 'bwd_align_dict', 'substituted_words', \"source_indices\"]:\n",
    "\t\tdf[column] = df[column].astype(\"str\")\n",
    "\t\tdf[column] = df[column].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "\t# Create lists of information that we need to add to the conllu file\n",
    "\tids_list = df.sentence_id.to_list()\n",
    "\tsource_text = df.text.to_list()\n",
    "\t# initial_translation = df.translation.to_list()\n",
    "\tspace_after_list = df[\"space-after-information\"].to_list()\n",
    "\tfwd_align_list = df['fwd_align_dict'].to_list()\n",
    "\tbwd_align_list = df['bwd_align_dict'].to_list()\n",
    "\tsubstituted_words_list = df['substituted_words'].to_list()\n",
    "\t# tokenized_text_list = df[\"source_indices\"].to_list()\n",
    "\tsentence_list = df.new_translations.to_list()\n",
    "\n",
    "\t# To feed the entire list into the pipeline, we need to create lists of tokens, split by space\n",
    "\tsentence_list = [x.split(\" \") for x in sentence_list]\n",
    "\t\n",
    "\t# Linguistically process the list\n",
    "\tdoc = nlp(sentence_list)\n",
    "\n",
    "\t# Save the conllu file\n",
    "\tCoNLL.write_doc2conll(doc, \"{}/results/{}/temp/{}\".format(main_path, lang_code, file))\n",
    "\n",
    "\tprint(\"{} processed and saved.\".format(file))\n",
    "\n",
    "\t# Open the CONLL-u file with the CONLL-u parser\n",
    "\n",
    "\tdata = open(\"{}/results/{}/temp/{}\".format(main_path, lang_code, file), \"r\").read()\n",
    "\n",
    "\tsentences = parse(data)\n",
    "\n",
    "\t# Adding additional information to the conllu\n",
    "\tfor sentence in sentences:\n",
    "\t\t# Get the sentence index\n",
    "\t\tsentence_index = sentences.index(sentence)\n",
    "\n",
    "\t\t# Add metadata\n",
    "\t\tsentence.metadata[\"sent_id\"] = ids_list[sentence_index]\n",
    "\t\tsentence.metadata[\"source\"] = source_text[sentence_index]\n",
    "\t\t# sentence.metadata[\"source_indices\"] = tokenized_text_list[sentence_index]\n",
    "\t\t# sentence.metadata[\"initial_translation\"] = initial_translation[sentence_index]\n",
    "\n",
    "\t\t# Delete the current metadata for text\n",
    "\t\tdel sentence.metadata[\"text\"]\n",
    "\n",
    "\t\tnew_translation_text = \"\"\n",
    "\n",
    "\t\t# Iterate through tokens\n",
    "\t\tfor word in sentence:\n",
    "\t\t\tword_index = sentence.index(word)\n",
    "\t\t\tword_conllu_index = word[\"id\"]\n",
    "\n",
    "\t\t\t# Check whether the word conllu index (word id) is in the substituted_words_list (it is if it was substituted)\n",
    "\t\t\t# If it is, add information on the original translated word\n",
    "\t\t\tif substituted_words_list[sentence_index].get(word_conllu_index, None) != None:\n",
    "\t\t\t\tword[\"misc\"][\"Translated\"] = substituted_words_list[sentence_index][word_conllu_index]\n",
    "\t\t\t\n",
    "\t\t\t# Do the same for the forward and backward alignment\n",
    "\t\t\tif fwd_align_list[sentence_index].get(word_conllu_index, None) != None:\n",
    "\t\t\t\tword[\"misc\"][\"ForwardAlignment\"] = fwd_align_list[sentence_index][word_conllu_index]\n",
    "\n",
    "\t\t\tif bwd_align_list[sentence_index].get(word_conllu_index, None) != None:\n",
    "\t\t\t\tword[\"misc\"][\"BackwardAlignment\"] = bwd_align_list[sentence_index][word_conllu_index]\n",
    "\n",
    "\t\t\t# Remove information on start_char and end_char from the annotation\n",
    "\t\t\tdel word[\"misc\"][\"start_char\"]\n",
    "\t\t\tdel word[\"misc\"][\"end_char\"]\n",
    "\t\t\t\n",
    "\t\t\t# Change the NER tags so that they are the same as in the source\n",
    "\t\t\tcurrent_ner = word[\"misc\"][\"ner\"]\n",
    "\t\t\tdel word[\"misc\"][\"ner\"]\n",
    "\t\t\t\n",
    "\t\t\t# Substitute parts of the tags so that they are the same as in source\n",
    "\t\t\tcurrent_ner = re.sub(\"S-\", \"B-\", current_ner)\n",
    "\t\t\tcurrent_ner = re.sub(\"E-\", \"I-\", current_ner)\n",
    "\n",
    "\t\t\tword[\"misc\"][\"NER\"] = current_ner\n",
    "\n",
    "\t\t\ttry:\n",
    "\t\t\t\t# Get information about the space after based on the index\n",
    "\t\t\t\tcurrent_space_after = space_after_list[sentence_index][word_index]\n",
    "\t\t\texcept:\n",
    "\t\t\t\tprint(\"Error based on current_space after in sentence {}, sentence index: {}, word {}, word index {}.\".format(sentence, sentence_index, word, word_index))\n",
    "\t\t\t\tcurrent_space_after = \"Yes\"\n",
    "\n",
    "\t\t# Create new text from translation, correcting the spaces around words\n",
    "\t\t# based on the SpaceAfter information\n",
    "\t\t\tif current_space_after == \"No\":\n",
    "\t\t\t\tword[\"misc\"][\"SpaceAfter\"] = \"No\"\n",
    "\t\t\t\tnew_translation_text += word[\"form\"]\n",
    "\t\t\telif current_space_after == \"Last\":\n",
    "\t\t\t\tnew_translation_text += word[\"form\"]\n",
    "\t\t\telse:\n",
    "\t\t\t\tnew_translation_text += word[\"form\"]\n",
    "\t\t\t\tnew_translation_text += \" \"\n",
    "\t\t\n",
    "\t\tsentence.metadata[\"text\"] = new_translation_text\n",
    "\t\n",
    "\t# Create a new conllu file with the updated information\n",
    "\tos.makedirs(os.path.dirname(target_path), exist_ok=True)\n",
    "\tfinal_file = open(\"{}\".format(target_path), \"w\")\n",
    "\n",
    "\tfor sentence in sentences:\n",
    "\t\tfinal_file.write(sentence.serialize())\n",
    "\t\n",
    "\tfinal_file.close()\n",
    "\n",
    "\tprint(\"Final file {} is saved.\".format(target_path))\n",
    "\n",
    "def produce_final_conllu(lang_code, final_dataframe):\n",
    "\timport pandas as pd\n",
    "\timport stanza\n",
    "\timport time\n",
    "\tfrom stanza.pipeline.core import DownloadMethod\n",
    "\t\n",
    "\tdf = pd.read_csv(\"{}\".format(final_dataframe), sep=\"\\t\", index_col=0)\n",
    "\n",
    "\t# Create a list of files\n",
    "\tfiles = list(df.file.unique())\n",
    "\t\n",
    "\tstart_time = time.time()\n",
    "\n",
    "\tprint(\"Processing started.\")\n",
    "\n",
    "\t# Define the pipeline, instruct it to use a specific package: \tCoNLL03\n",
    "\tnlp = stanza.Pipeline(lang='en', processors=\"tokenize,mwt,pos,lemma,ner\", package={\"ner\": [\"conll03\"]}, tokenize_pretokenized=True, download_method=DownloadMethod.REUSE_RESOURCES, use_gpu=True)\n",
    "\n",
    "\tfor file in files:\n",
    "\t\tcreate_conllu(file, lang_code, main_path, final_dataframe, nlp)\n",
    "\t\tcurrent_end_time = round((time.time() - start_time)/60,2)\n",
    "\t\tprint(\"Current running time: {}\".format(current_end_time))\n",
    "\t\n",
    "\tend_time = round((time.time() - start_time)/60,2)\n",
    "\n",
    "\tprint(\"Processing completed. It took {} minutes.\".format(end_time))\n",
    "\n",
    "\n",
    "produce_final_conllu(lang_code, final_dataframe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parlamint_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e8622831a220209edc4d2d4e58bc2159575ea2f8d9419209393154757ff5f93"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
