{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=5\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conllu import parse\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the language code, used in the file names\n",
    "lang_code = \"PT\"\n",
    "\n",
    "# Main path\n",
    "main_path = \"/home/tajak/Parlamint-translation\"\n",
    "\n",
    "# Check whether the path to the folder with conllu files is ok\n",
    "path = \"{}/Source-data/ParlaMint-{}.conllu/ParlaMint-{}.conllu\".format(main_path, lang_code, lang_code)\n",
    "\n",
    "# Define other paths\n",
    "extracted_dataframe_path = \"{}/results/{}/ParlaMint-{}-extracted-source-data.csv\".format(main_path, lang_code, lang_code)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to translate the following corpora into English:\n",
    "- Belgian (BE) - in Dutch and French, separate CONLL-us (!!). French: [\"fr\", \"itc\",\"roa\"], Dutch: [\"nl\", \"gem\", \"gmw\"]\n",
    "- Bulgarian (BG): [\"bg\", \"sla\", \"zls\"]\n",
    "- Croatian (HR) -> \"zls\"\n",
    "- Czech (CZ) -> \"cs\"\n",
    "- Danish (DK): [\"da\", \"gmq\", \"gem\"]\n",
    "- Dutch (NL): [\"nl\", \"gem\", \"gmw\"]\n",
    "- French (FR): [\"fr\", \"itc\",\"roa\"]\n",
    "- Hungarian (HU): [\"hu\", \"fiu\", \"urj\"]\n",
    "- Icelandic (IS): [\"is\",\"gmq\", \"gem\"]\n",
    "- Italian (IT): [\"it\", \"roa\", \"itc\"]\n",
    "- Latvian (LV): [\"lv\", \"bat\"]\n",
    "- Lithuanian (LT): only \"bat\"\n",
    "- Polish (PL): [\"pl\", \"sla\", \"zlw\"]\n",
    "- Slovenian (SI) - We will use \"Slavic MT\" based on the results of the manual analysis --> \"sla\"\n",
    "- Spanish? (ES): [\"es\", \"roa\", \"itc\"]\n",
    "- Turkish (TR): [\"tr\", \"trk\" ]\n",
    "- Austrian (AT): [\"de\", \"gem\", \"gmw\"]\n",
    "- Basque (ES-PV): [\"eu\", \"mul\"]\n",
    "- Bosnian (BA): [\"sla\", \"zls\"]\n",
    "- Catalan (ES-CT): [\"ca\", \"roa\", \"itc\"]\n",
    "- Estonian (EE): [\"et\", \"urj\", \"fiu\"]\n",
    "- Finnish (FI): [\"fi\", \"urj\", \"fiu\"]\n",
    "- Galician (ES-GA): [\"gl\", \"roa\", \"itc\"]\n",
    "- Greek (GR):  [\"el\", \"grk\"]\n",
    "- Norwegian (NO): [\"gem\", \"gmq\"]\n",
    "- Portuguese (PT): [\"pt\", \"roa\", \"itc\"]\n",
    "- Romanian (RO): [\"roa\", \"itc\"]\n",
    "- Serbian (RS): [\"sla\", \"zls\"]\n",
    "- Swedish (SE): [\"sv\", \"gmq\", \"gem\"]\n",
    "- Ukrainian (UA): [\"uk\", \"sla\", \"zle\"]\n",
    "\n",
    "Languages with only one option: Lithuanian (\"LT\": \"bat\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of language codes:\n",
    "- sla = Slavic\n",
    "- zls = South Slavic\n",
    "- zlw = West Slavic\n",
    "- zle = East Slavic\n",
    "- gmq = North Germanic\n",
    "- gem = Germanic\n",
    "- gmw = West Germanic\n",
    "- roa = Romance\n",
    "- itc = Italic\n",
    "- bat = Baltic\n",
    "- trk = Turkic\n",
    "- urj = Uralic\n",
    "- fiu = Finno-Ugrian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "All languages with target_lang=de. I.e., we can translate from these languages to English (en).\n",
      "['aav', 'af', 'alv', 'ar', 'az', 'bat', 'bcl', 'bem', 'ber', 'bg', 'bi', 'bnt', 'bzs', 'ca', 'ceb', 'cel', 'chk', 'cpf', 'crs', 'cs', 'cus', 'cy', 'da', 'de', 'dra', 'ee', 'efi', 'el', 'eo', 'es', 'et', 'eu', 'euq', 'fi', 'fj', 'fr', 'ga', 'gaa', 'gil', 'gl', 'grk', 'guw', 'gv', 'ha', 'he', 'hi', 'hil', 'ho', 'ht', 'hu', 'hy', 'id', 'ig', 'ilo', 'is', 'iso', 'it', 'jap', 'kg', 'kj', 'kqn', 'kwn', 'kwy', 'lg', 'ln', 'loz', 'lu', 'lua', 'lue', 'lun', 'luo', 'lus', 'map', 'mfe', 'mg', 'mh', 'mk', 'mkh', 'ml', 'mos', 'mr', 'mt', 'mul', 'ng', 'nic', 'niu', 'nl', 'nso', 'ny', 'nyk', 'om', 'pag', 'pap', 'phi', 'pis', 'pon', 'poz', 'pqe', 'pqw', 'rn', 'rnd', 'ro', 'roa', 'ru', 'run', 'rw', 'sal', 'sg', 'sit', 'sk', 'sm', 'sn', 'sq', 'ss', 'st', 'sv', 'sw', 'swc', 'tdt', 'ti', 'tiv', 'tl', 'tll', 'tn', 'to', 'toi', 'tpi', 'trk', 'ts', 'tut', 'tvl', 'tw', 'ty', 'uk', 'umb', 'ur', 'vi', 'xh', 'zh']\n"
     ]
    }
   ],
   "source": [
    "from easynmt import EasyNMT\n",
    "\n",
    "# Define the model\n",
    "model = EasyNMT('opus-mt')\n",
    "\n",
    "print(\"\\n\\nAll languages with target_lang=de. I.e., we can translate from these languages to English (en).\")\n",
    "print(model.get_languages(source_lang='en'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_model(lang_code, extracted_dataframe_path):\n",
    "\t\"\"\"\n",
    "\tCompare a small sample of translations of all OPUS-MT models that are available\n",
    "\tfor the language, to decide which one to use. The function prints out a dataframe with all translations of the sample and saves it as ParlaMint-{lang_code}-sample-model-comparison.csv.\n",
    "\n",
    "\tArgs:\n",
    "\t- lang_code: the lang code that is used in the names of the files, it should be the same as for extract_text()\n",
    "\t- extracted_dataframe_path: path to the final output of 1-conllu-to-df.py\n",
    "\t\"\"\"\n",
    "\timport pandas as pd\n",
    "\timport regex as re\n",
    "\tfrom easynmt import EasyNMT\n",
    "\tfrom IPython.display import display\n",
    "\t\n",
    "\tlang_models_dict = {\"BG\": [\"bg\", \"sla\", \"zls\"], \"HR\": [\"zls\", \"sla\"], \"CZ\": [\"cs\", \"sla\", \"zlw\" ], \"DK\": [\"da\", \"gmq\", \"gem\"], \"NL\": [\"nl\", \"gem\", \"gmw\"], \"FR\": [\"fr\", \"itc\",\"roa\"], \"HU\": [\"hu\", \"fiu\", \"urj\"], \"IS\": [\"is\",\"gmq\", \"gem\"], \"IT\": [\"it\", \"roa\", \"itc\"], \"LV\": [\"lv\",\"bat\"], \"LT\": [\"bat\"], \"PL\": [\"pl\", \"sla\", \"zlw\"], \"SI\": [\"sla\"], \"ES\": [\"es\", \"roa\", \"itc\"], \"TR\": [\"tr\", \"trk\" ], \"AT\": [\"de\", \"gem\", \"gmw\"], \"ES-PV\": [\"eu\", \"mul\"], \"BA\": [\"sla\", \"zls\"], \"ES-CT\": [\"ca\", \"roa\", \"itc\"], \"EE\": [\"et\", \"urj\", \"fiu\"], \"FI\": [\"fi\", \"urj\", \"fiu\"], \"ES-GA\": [\"gl\", \"roa\", \"itc\"], \"GR\": [\"el\",\"grk\"], \"NO\": [\"gem\", \"gmq\"], \"PT\": [\"roa\", \"itc\"], \"RO\":[\"roa\", \"itc\"], \"RS\": [\"zls\", \"sla\"], \"SE\": [\"sv\", \"gmq\", \"gem\"], \"UA\":[\"uk\", \"sla\", \"zle\"]}\n",
    "\n",
    "\n",
    "\t# Open the file, created in the previous step\n",
    "\tdf = pd.read_csv(\"{}\".format(extracted_dataframe_path), sep=\"\\t\", index_col=0)\n",
    "\n",
    "\t# Define the model\n",
    "\tmodel = EasyNMT('opus-mt')\n",
    "\n",
    "\tprint(\"Entire corpus has {} sentences and {} words.\".format(df[\"text\"].count(), df[\"length\"].sum()))\n",
    "\n",
    "\t# Create a smaller sample - just a couple of sentences from one file\n",
    "\tdf = df[df.file == list(df[\"file\"].unique())[0]][:30]\n",
    "\n",
    "\tprint(\"Sample files has {} sentences and {} words.\".format(df[\"text\"].count(), df[\"length\"].sum()))\n",
    "\n",
    "\t# Create a list of sentences from the df\n",
    "\tsentence_list = df.text.to_list()\n",
    "\n",
    "\t# Translate the sample using all available models for this language\n",
    "\tfor opus_lang_code in lang_models_dict[lang_code]:\n",
    "\t\ttranslation_list = model.translate(sentence_list, source_lang = \"{}\".format(opus_lang_code), target_lang='en')\n",
    "\n",
    "\t\t# Add the translations to the df\n",
    "\t\tdf[\"translation-{}\".format(opus_lang_code)] = translation_list\n",
    "\t\n",
    "\tdf = df.drop(columns=[\"file\", \"sentence_id\", \"tokenized_text\", \"proper_nouns\", \"length\"])\n",
    "\n",
    "\t# For Portuguese, let's try another Portuguese model that is on HF, but is not OPUS-MT\n",
    "\tif lang_code == \"PT\":\n",
    "\t\tnew_translation_list = []\n",
    "\n",
    "\t\tfrom transformers import MBart50TokenizerFast, MBartForConditionalGeneration\n",
    "\t\tckpt = 'Narrativa/mbart-large-50-finetuned-opus-pt-en-translation'\n",
    "\n",
    "\t\ttokenizer = MBart50TokenizerFast.from_pretrained(ckpt)\n",
    "\t\tmodel = MBartForConditionalGeneration.from_pretrained(ckpt).to(\"cuda\")\n",
    "\n",
    "\t\ttokenizer.src_lang = 'pt_XX'\n",
    "\n",
    "\t\tdef translate(text):\n",
    "\t\t\tinputs = tokenizer(text, return_tensors='pt')\n",
    "\t\t\tinput_ids = inputs.input_ids.to('cuda')\n",
    "\t\t\tattention_mask = inputs.attention_mask.to('cuda')\n",
    "\t\t\toutput = model.generate(input_ids, attention_mask=attention_mask, forced_bos_token_id=tokenizer.lang_code_to_id['en_XX'])\n",
    "\t\t\treturn tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "\t\tfor sentence in sentence_list:\n",
    "\t\t\ttranslation = translate(sentence)\n",
    "\t\t\tnew_translation_list.append(translation)\n",
    "\t\t\n",
    "\t\tdf[\"translation-narrativa\"] = new_translation_list\n",
    "\n",
    "\t# Save the df\n",
    "\tdf.to_csv(\"/home/tajak/Parlamint-translation/results/{}/ParlaMint-{}-sample-model-comparison.csv\".format(lang_code, lang_code))\n",
    "\n",
    "\tprint(\"The file is saved as/home/tajak/Parlamint-translation/results/{}/ParlaMint-{}-sample-model-comparison.csv. \".format(lang_code, lang_code))\n",
    "\n",
    "\treturn df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entire corpus has 458643 sentences and 18336113 words.\n",
      "Sample files has 30 sentences and 1376 words.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)olve/main/source.spm: 100%|██████████| 800k/800k [00:00<00:00, 1.23MB/s]\n",
      "Downloading (…)olve/main/target.spm: 100%|██████████| 779k/779k [00:00<00:00, 1.21MB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 1.46M/1.46M [00:00<00:00, 1.94MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 265/265 [00:00<00:00, 111kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.36k/1.36k [00:00<00:00, 689kB/s]\n",
      "Downloading (…)\"pytorch_model.bin\";: 100%|██████████| 312M/312M [00:08<00:00, 35.7MB/s] \n",
      "Downloading (…)neration_config.json: 100%|██████████| 293/293 [00:00<00:00, 178kB/s]\n",
      "/home/tajak/Parlamint-translation/parlamint_env/lib/python3.8/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 512 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Downloading (…)olve/main/source.spm: 100%|██████████| 792k/792k [00:00<00:00, 1.22MB/s]\n",
      "Downloading (…)olve/main/target.spm: 100%|██████████| 787k/787k [00:00<00:00, 1.22MB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 1.27M/1.27M [00:00<00:00, 1.47MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 44.0/44.0 [00:00<00:00, 17.2kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.40k/1.40k [00:00<00:00, 572kB/s]\n",
      "Downloading (…)\"pytorch_model.bin\";: 100%|██████████| 296M/296M [00:10<00:00, 27.4MB/s] \n",
      "Downloading (…)neration_config.json: 100%|██████████| 293/293 [00:00<00:00, 75.5kB/s]\n",
      "/home/tajak/Parlamint-translation/parlamint_env/lib/python3.8/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 512 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/tajak/Parlamint-translation/parlamint_env/lib/python3.8/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 200 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file is saved as/home/tajak/Parlamint-translation/results/PT/ParlaMint-PT-sample-model-comparison.csv. \n"
     ]
    }
   ],
   "source": [
    "df = choose_model(lang_code, extracted_dataframe_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>text</th>\n",
       "      <th>translation-bg</th>\n",
       "      <th>translation-sla</th>\n",
       "      <th>translation-zls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/tajak/Parlamint-translation/Source-data/...</td>\n",
       "      <td>Уважаеми народни представители, добър ден!</td>\n",
       "      <td>Dear MPs, good afternoon!</td>\n",
       "      <td>Ladies and gentlemen, good afternoon!</td>\n",
       "      <td>Dear People's Representatives, good afternoon!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/tajak/Parlamint-translation/Source-data/...</td>\n",
       "      <td>Моля да се регистрираме.</td>\n",
       "      <td>Please register.</td>\n",
       "      <td>Please sign up.</td>\n",
       "      <td>Please sign up.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_path  \\\n",
       "0  /home/tajak/Parlamint-translation/Source-data/...   \n",
       "1  /home/tajak/Parlamint-translation/Source-data/...   \n",
       "\n",
       "                                         text             translation-bg  \\\n",
       "0  Уважаеми народни представители, добър ден!  Dear MPs, good afternoon!   \n",
       "1                    Моля да се регистрираме.           Please register.   \n",
       "\n",
       "                         translation-sla  \\\n",
       "0  Ladies and gentlemen, good afternoon!   \n",
       "1                        Please sign up.   \n",
       "\n",
       "                                  translation-zls  \n",
       "0  Dear People's Representatives, good afternoon!  \n",
       "1                                 Please sign up.  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Then open the sample and manually evaluate which model is better in the column \"comparison\"\n",
    "# Open the analysed sample\n",
    "\n",
    "sample = pd.read_csv(\"/home/tajak/Parlamint-translation/results/{}/ParlaMint-{}-sample-model-comparison.csv\".format(lang_code, lang_code), index_col = 0)\n",
    "sample.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "zls    10\n",
       "sla     1\n",
       "Name: comparison, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.comparison.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the \"zls\" model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parlamint_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e8622831a220209edc4d2d4e58bc2159575ea2f8d9419209393154757ff5f93"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
