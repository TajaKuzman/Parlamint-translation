{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=3\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conllu import parse\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the language code, used in the file names\n",
    "lang_code = \"AT\"\n",
    "\n",
    "# Main path\n",
    "main_path = \"/home/tajak/Parlamint-translation\"\n",
    "\n",
    "# Check whether the path to the folder with conllu files is ok\n",
    "path = \"{}/Source-data/ParlaMint-{}.conllu/ParlaMint-{}.conllu\".format(main_path, lang_code, lang_code)\n",
    "\n",
    "# Define other paths\n",
    "extracted_dataframe_path = \"{}/results/{}/ParlaMint-{}-extracted-source-data.csv\".format(main_path, lang_code, lang_code)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to translate the following corpora into English:\n",
    "- Belgian (BE) - in Dutch and French, separate CONLL-us (!!). French: [\"fr\", \"itc\",\"roa\"], Dutch: [\"nl\", \"gem\", \"gmw\"]\n",
    "- Bulgarian (BG): [\"bg\", \"sla\", \"zls\"]\n",
    "- Croatian (HR) -> \"zls\"\n",
    "- Czech (CZ) -> \"cs\"\n",
    "- Danish (DK): [\"da\", \"gmq\", \"gem\"]\n",
    "- Dutch (NL): [\"nl\", \"gem\", \"gmw\"]\n",
    "- French (FR): [\"fr\", \"itc\",\"roa\"]\n",
    "- Hungarian (HU): [\"hu\", \"fiu\", \"urj\"]\n",
    "- Icelandic (IS): [\"is\",\"gmq\", \"gem\"]\n",
    "- Italian (IT): [\"it\", \"roa\", \"itc\"]\n",
    "- Latvian (LV): [\"lv\", \"bat\"]\n",
    "- Lithuanian (LT): only \"bat\"\n",
    "- Polish (PL): [\"pl\", \"sla\", \"zlw\"]\n",
    "- Slovenian (SI) - We will use \"Slavic MT\" based on the results of the manual analysis --> \"sla\"\n",
    "- Spanish? (ES): [\"es\", \"roa\", \"itc\"]\n",
    "- Turkish (TR): [\"tr\", \"trk\" ]\n",
    "- Austrian (AT): [\"de\", \"gem\", \"gmw\"]\n",
    "- Basque (ES-PV): [\"eu\", \"mul\"]\n",
    "- Bosnian (BA): [\"sla\", \"zls\"]\n",
    "- Catalan (ES-CT): [\"ca\", \"roa\", \"itc\"]\n",
    "- Estonian (EE): [\"et\", \"urj\", \"fiu\"]\n",
    "- Finnish (FI): [\"fi\", \"urj\", \"fiu\"]\n",
    "- Galician (ES-GA): [\"gl\", \"roa\", \"itc\"]\n",
    "- Greek (GR):  [\"el\", \"grk\"]\n",
    "- Norwegian (NO): [\"gem\", \"gmq\"]\n",
    "- Portuguese (PT): [\"pt\", \"roa\", \"itc\"]\n",
    "- Romanian (RO): [\"roa\", \"itc\"]\n",
    "- Serbian (RS): [\"sla\", \"zls\"]\n",
    "- Swedish (SE): [\"sv\", \"gmq\", \"gem\"]\n",
    "- Ukrainian (UA): [\"uk\", \"sla\", \"zle\"]\n",
    "\n",
    "Languages with only one option: Lithuanian (\"LT\": \"bat\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of language codes:\n",
    "- sla = Slavic\n",
    "- zls = South Slavic\n",
    "- zlw = West Slavic\n",
    "- zle = East Slavic\n",
    "- gmq = North Germanic\n",
    "- gem = Germanic\n",
    "- gmw = West Germanic\n",
    "- roa = Romance\n",
    "- itc = Italic\n",
    "- bat = Baltic\n",
    "- trk = Turkic\n",
    "- urj = Uralic\n",
    "- fiu = Finno-Ugrian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_model(lang_code, extracted_dataframe_path):\n",
    "\t\"\"\"\n",
    "\tCompare a small sample of translations of all OPUS-MT models that are available\n",
    "\tfor the language, to decide which one to use. The function prints out a dataframe with all translations of the sample and saves it as ParlaMint-{lang_code}-sample-model-comparison.csv.\n",
    "\n",
    "\tArgs:\n",
    "\t- lang_code: the lang code that is used in the names of the files, it should be the same as for extract_text()\n",
    "\t- extracted_dataframe_path: path to the final output of 1-conllu-to-df.py\n",
    "\t\"\"\"\n",
    "\timport pandas as pd\n",
    "\timport regex as re\n",
    "\tfrom easynmt import EasyNMT\n",
    "\tfrom IPython.display import display\n",
    "\t\n",
    "\tlang_models_dict = {\"BG\": [\"bg\", \"sla\", \"zls\"], \"HR\": [\"zls\", \"sla\"], \"CZ\": [\"cs\", \"sla\", \"zlw\" ], \"DK\": [\"da\", \"gmq\", \"gem\"], \"NL\": [\"nl\", \"gem\", \"gmw\"], \"FR\": [\"fr\", \"itc\",\"roa\"], \"HU\": [\"hu\", \"fiu\", \"urj\"], \"IS\": [\"is\",\"gmq\", \"gem\"], \"IT\": [\"it\", \"roa\", \"itc\"], \"LV\": [\"lv\",\"bat\"], \"LT\": [\"bat\"], \"PL\": [\"pl\", \"sla\", \"zlw\"], \"SI\": [\"sla\"], \"ES\": [\"es\", \"roa\", \"itc\"], \"TR\": [\"tr\", \"trk\" ], \"AT\": [\"de\", \"gem\", \"gmw\"], \"ES-PV\": [\"eu\", \"mul\"], \"BA\": [\"sla\", \"zls\"], \"ES-CT\": [\"ca\", \"roa\", \"itc\"], \"EE\": [\"et\", \"urj\", \"fiu\"], \"FI\": [\"fi\", \"urj\", \"fiu\"], \"ES-GA\": [\"gl\", \"roa\", \"itc\"], \"GR\": [\"el\",\"grk\"], \"NO\": [\"gem\", \"gmq\"], \"PT\": [\"pt\", \"roa\", \"itc\"], \"RO\":[\"roa\", \"itc\"], \"RS\": [\"zls\", \"sla\"], \"SE\": [\"sv\", \"gmq\", \"gem\"], \"UA\":[\"uk\", \"sla\", \"zle\"]}\n",
    "\n",
    "\n",
    "\t# Open the file, created in the previous step\n",
    "\tdf = pd.read_csv(\"{}\".format(extracted_dataframe_path), sep=\"\\t\", index_col=0)\n",
    "\n",
    "\t# Define the model\n",
    "\tmodel = EasyNMT('opus-mt')\n",
    "\n",
    "\tprint(\"Entire corpus has {} sentences and {} words.\".format(df[\"text\"].count(), df[\"length\"].sum()))\n",
    "\n",
    "\t# Create a smaller sample - just a couple of sentences from one file\n",
    "\tdf = df[df.file == list(df[\"file\"].unique())[0]][:30]\n",
    "\n",
    "\tprint(\"Sample files has {} sentences and {} words.\".format(df[\"text\"].count(), df[\"length\"].sum()))\n",
    "\n",
    "\t# Create a list of sentences from the df\n",
    "\tsentence_list = df.text.to_list()\n",
    "\n",
    "\t# Translate the sample using all available models for this language\n",
    "\tfor opus_lang_code in lang_models_dict[lang_code]:\n",
    "\t\ttranslation_list = model.translate(sentence_list, source_lang = \"{}\".format(opus_lang_code), target_lang='en')\n",
    "\n",
    "\t\t# Add the translations to the df\n",
    "\t\tdf[\"translation-{}\".format(opus_lang_code)] = translation_list\n",
    "\t\n",
    "\tdf = df.drop(columns=[\"file\", \"sentence_id\", \"tokenized_text\", \"proper_nouns\", \"length\"])\n",
    "\n",
    "\t# Save the df\n",
    "\tdf.to_csv(\"/home/tajak/Parlamint-translation/results/{}/ParlaMint-{}-sample-model-comparison.csv\".format(lang_code, lang_code))\n",
    "\n",
    "\tprint(\"The file is saved as/home/tajak/Parlamint-translation/results/{}/ParlaMint-{}-sample-model-comparison.csv. \".format(lang_code, lang_code))\n",
    "\n",
    "\treturn df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tajak/Parlamint-translation/parlamint_env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-02-06 08:42:18.226823: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-06 08:42:19.025168: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-06 08:42:19.025237: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-06 08:42:19.025242: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entire corpus has 3919672 sentences and 59959897 words.\n",
      "Sample files has 30 sentences and 349 words.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)olve/main/source.spm: 100%|██████████| 797k/797k [00:00<00:00, 879kB/s]\n",
      "Downloading (…)olve/main/target.spm: 100%|██████████| 768k/768k [00:00<00:00, 1.15MB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 1.27M/1.27M [00:01<00:00, 901kB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 42.0/42.0 [00:00<00:00, 19.3kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.38k/1.38k [00:00<00:00, 534kB/s]\n",
      "Downloading (…)\"pytorch_model.bin\";: 100%|██████████| 298M/298M [00:42<00:00, 6.96MB/s] \n",
      "Downloading (…)neration_config.json: 100%|██████████| 293/293 [00:00<00:00, 80.5kB/s]\n",
      "/home/tajak/Parlamint-translation/parlamint_env/lib/python3.8/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 512 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Downloading (…)olve/main/source.spm: 100%|██████████| 790k/790k [00:00<00:00, 1.42MB/s]\n",
      "Downloading (…)olve/main/target.spm: 100%|██████████| 784k/784k [00:00<00:00, 1.16MB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 1.28M/1.28M [00:01<00:00, 1.15MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 44.0/44.0 [00:00<00:00, 21.1kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.40k/1.40k [00:00<00:00, 771kB/s]\n",
      "Downloading (…)\"pytorch_model.bin\";: 100%|██████████| 295M/295M [00:50<00:00, 5.89MB/s] \n",
      "Downloading (…)neration_config.json: 100%|██████████| 293/293 [00:00<00:00, 83.6kB/s]\n",
      "/home/tajak/Parlamint-translation/parlamint_env/lib/python3.8/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 512 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Downloading (…)olve/main/source.spm: 100%|██████████| 801k/801k [00:00<00:00, 1.20MB/s]\n",
      "Downloading (…)olve/main/target.spm: 100%|██████████| 783k/783k [00:00<00:00, 1.18MB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 1.26M/1.26M [00:00<00:00, 1.60MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 44.0/44.0 [00:00<00:00, 26.0kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.40k/1.40k [00:00<00:00, 792kB/s]\n",
      "Downloading (…)\"pytorch_model.bin\";: 100%|██████████| 293M/293M [00:16<00:00, 17.3MB/s] \n",
      "Downloading (…)neration_config.json: 100%|██████████| 293/293 [00:00<00:00, 53.3kB/s]\n",
      "/home/tajak/Parlamint-translation/parlamint_env/lib/python3.8/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 512 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file is saved as/home/tajak/Parlamint-translation/results/AT/ParlaMint-AT-sample-model-comparison.csv. \n"
     ]
    }
   ],
   "source": [
    "df = choose_model(lang_code, extracted_dataframe_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>text</th>\n",
       "      <th>translation-de</th>\n",
       "      <th>translation-gem</th>\n",
       "      <th>translation-gmw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/tajak/Parlamint-translation/Source-data/...</td>\n",
       "      <td>Guten Tag, meine Damen und Herren!</td>\n",
       "      <td>Hello, ladies and gentlemen!</td>\n",
       "      <td>Good afternoon, ladies and gentlemen!</td>\n",
       "      <td>Good day, ladies and gentlemen!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/tajak/Parlamint-translation/Source-data/...</td>\n",
       "      <td>Ich eröffne die 190. Sitzung des Nationalrates...</td>\n",
       "      <td>I'll open the 190. Meeting of the National Cou...</td>\n",
       "      <td>I'll open the 190. Meeting of the National Cou...</td>\n",
       "      <td>I'll open the 190. Meeting of the National Cou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_path  \\\n",
       "0  /home/tajak/Parlamint-translation/Source-data/...   \n",
       "1  /home/tajak/Parlamint-translation/Source-data/...   \n",
       "\n",
       "                                                text  \\\n",
       "0                 Guten Tag, meine Damen und Herren!   \n",
       "1  Ich eröffne die 190. Sitzung des Nationalrates...   \n",
       "\n",
       "                                      translation-de  \\\n",
       "0                       Hello, ladies and gentlemen!   \n",
       "1  I'll open the 190. Meeting of the National Cou...   \n",
       "\n",
       "                                     translation-gem  \\\n",
       "0              Good afternoon, ladies and gentlemen!   \n",
       "1  I'll open the 190. Meeting of the National Cou...   \n",
       "\n",
       "                                     translation-gmw  \n",
       "0                    Good day, ladies and gentlemen!  \n",
       "1  I'll open the 190. Meeting of the National Cou...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Then open the sample and manually evaluate which model is better in the column \"comparison\"\n",
    "# Open the analysed sample\n",
    "\n",
    "sample = pd.read_csv(\"/home/tajak/Parlamint-translation/results/{}/ParlaMint-{}-sample-model-comparison.csv\".format(lang_code, lang_code), index_col = 0)\n",
    "sample.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "zls    10\n",
       "sla     1\n",
       "Name: comparison, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.comparison.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the \"zls\" model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parlamint_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Mar 15 2022, 12:22:08) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e8622831a220209edc4d2d4e58bc2159575ea2f8d9419209393154757ff5f93"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
