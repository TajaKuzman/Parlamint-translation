{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the code for the pipeline to get all the files, compare models on a sample, translate all files using the best model and correct proper nouns."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Workflow:\n",
    "1. Extract information from the CONLL-U\n",
    "2. Translate\n",
    "3. Tokenize English translations with Stanza\n",
    "4. Word alignment, substitute English NE translations with lemmas from the source, get information on NE annotations for each translated word from the source annotations\n",
    "5. Linguistically process English translation with Stanza (lemmas, POS)\n",
    "6. Parse CONLL-u file and add additional information (sentence ids, alignments, NER annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conllu_to_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conllu_to_df(parl_list, file_name_list, main_path, lang_code):\n",
    "\t\"\"\"\n",
    "\tTake the conllu files and extract relevant information. Save everything in a DataFrame.\n",
    "\n",
    "\tArgs:\n",
    "\t- parl_list: list of documents with their entire paths to be included (see step above).\n",
    "\t- file_name_list: list of names of the files (see step above)\n",
    "\t- main_path: main path to the working directory\n",
    "\t- lang_code: e.g., CZ\n",
    "\t\"\"\"\n",
    "\tfrom conllu import parse\n",
    "\timport pandas as pd\n",
    "\tfrom IPython.display import display\n",
    "\tfrom itertools import islice\n",
    "\timport math\n",
    "\timport numpy as np\n",
    "\n",
    "\t# Create an empty df\n",
    "\tdf = pd.DataFrame({\"file_path\": [\"\"],\"file\": [\"\"], \"sentence_id\": [\"\"], \"text\": [\"\"], \"tokenized_text\": [\"\"], \"proper_nouns\": [\"\"]})\n",
    "\n",
    "\t# Check whether there are any problems with parsing the documents\n",
    "\t\"\"\"\n",
    "\t\n",
    "\terror_count = 0\n",
    "\tproblematic_doc_list = []\n",
    "\n",
    "\tfor doc in parl_list:\n",
    "\t\ttry:\n",
    "\t\t\t# Open the file\n",
    "\t\t\tdata = open(\"{}\".format(doc), \"r\").read()\n",
    "\n",
    "\t\t\tsentences = parse(data)\n",
    "\t\texcept:\n",
    "\t\t\terror_count += 1\n",
    "\t\t\tproblematic_doc_list.append(doc)\n",
    "\n",
    "\tprint(error_count)\n",
    "\tprint(problematic_doc_list)\n",
    "\t\"\"\"\n",
    "\t# Parse the data with CONLL-u parser\n",
    "\tfor doc in parl_list:\n",
    "\t\t# Open the file\n",
    "\t\tdata = open(\"{}\".format(doc), \"r\").read()\n",
    "\t\t\n",
    "\t\tsentences = parse(data)\n",
    "\n",
    "\t\tsentence_id_list = []\n",
    "\t\ttext_list = []\n",
    "\t\ttokenized_text_list = []\n",
    "\t\tproper_noun_list = []\n",
    "\n",
    "\t\tfor sentence in sentences:\n",
    "\t\t\t# Find sentence ids\n",
    "\t\t\tcurrent_sentence_id = sentence.metadata[\"sent_id\"]\n",
    "\t\t\tsentence_id_list.append(current_sentence_id)\n",
    "\n",
    "\t\t\t# Find text - if texts consists of multiword tokens, these tokens will appear as they are,\n",
    "\t\t\t# not separated into subwords\n",
    "\t\t\tcurrent_text = sentence.metadata[\"text\"]\n",
    "\t\t\ttext_list.append(current_text)\n",
    "\n",
    "\t\t\t# Create a string out of tokens\n",
    "\t\t\tcurrent_token_list = []\n",
    "\t\t\tword_dict = {}\n",
    "\n",
    "\t\t\tfor token in sentence:\n",
    "\t\t\t\t# Find multiword tokens and take their NER\n",
    "\t\t\t\tif type(token[\"id\"]) != int:\n",
    "\t\t\t\t\tmultiword_ner = token[\"misc\"][\"NER\"]\n",
    "\t\t\t\t\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t# Append to the tokenized text tokens that are not multiword tokens\n",
    "\t\t\t\t# (we append subtokens to the tokenized texts, not multiword tokens)\n",
    "\t\t\t\t\tcurrent_token_list.append(token[\"form\"])\n",
    "\t\t\t\t\t\n",
    "\n",
    "\t\t\t\t\t# Create a list of NE annotations with word indices.\n",
    "\t\t\t\t\t# I'll substract one from the word index,\n",
    "\t\t\t\t\t# because indexing in the CONLLU file starts with 1, not 0\n",
    "\t\t\t\t\tcurrent_index = int(token[\"id\"]) - 1\n",
    "\n",
    "\t\t\t\t\t# If the word does not have NER annotation,\n",
    "\t\t\t\t\t# take the annotation from the multiword token\n",
    "\t\t\t\t\tif token[\"misc\"] is None:\n",
    "\t\t\t\t\t\tcurrent_ner = multiword_ner\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tcurrent_ner = token[\"misc\"][\"NER\"]\n",
    "\n",
    "\t\t\t\t\t# Add information on the lemma if the NE is personal name\n",
    "\t\t\t\t\tif \"PER\" in current_ner:\n",
    "\t\t\t\t\t\tword_dict[current_index] = [token[\"form\"], token[\"lemma\"]]\n",
    "\n",
    "\t\t\tproper_noun_list.append(word_dict)\n",
    "\n",
    "\t\t\tcurrent_string = \" \".join(current_token_list)\n",
    "\n",
    "\t\t\ttokenized_text_list.append(current_string)\n",
    "\n",
    "\t\t\n",
    "\t\tnew_df = pd.DataFrame({\"sentence_id\": sentence_id_list, \"text\": text_list, \"tokenized_text\": tokenized_text_list, \"proper_nouns\": proper_noun_list})\n",
    "\n",
    "\t\tnew_df[\"file_path\"] = doc\n",
    "\n",
    "\t\t# Get the file name\n",
    "\t\tfile_name = file_name_list[parl_list.index(doc)]\n",
    "\t\tnew_df[\"file\"] = file_name\n",
    "\n",
    "\t\t# Merge df to the previous df\n",
    "\t\tdf = pd.concat([df, new_df])\n",
    "\t\n",
    "\t# Reset index\n",
    "\tdf = df.reset_index(drop=True)\n",
    "\n",
    "\t# Remove the first row\n",
    "\tdf = df.drop([0], axis=\"index\")\n",
    "\n",
    "\t# Reset index\n",
    "\tdf = df.reset_index(drop=True)\n",
    "\n",
    "\t# Add information on length\n",
    "\tdf[\"length\"] = df[\"text\"].str.split().str.len()\n",
    "\n",
    "\tprint(\"Number of words in the corpora: {}\".format(df[\"length\"].sum()))\n",
    "\n",
    "\t# Split the dataframe into 3 batches based on the list of files\n",
    "\tfile_list = list(df.file.unique())\n",
    "\n",
    "\tdef chunk(arr_range, arr_size):\n",
    "\t\tarr_range = iter(arr_range)\n",
    "\t\treturn iter(lambda: tuple(islice(arr_range, arr_size)), ())\n",
    "\n",
    "\tbatches_list = list(chunk(file_list, math.ceil(len(file_list)/3)))\n",
    "\n",
    "\tprint(\"File is separated into {} batches, sizes of batches (in no. of files): {}, {}, {}.\".format(len(batches_list), len(batches_list[0]), len(batches_list[1]), len(batches_list[2])))\n",
    "\n",
    "\t# Add information on the batch in the dataframe\n",
    "\tfor i in range(len(batches_list)):\n",
    "\t\tif i==0:\n",
    "\t\t\tdf[\"batch\"] = np.where((df[\"file\"].isin(list(batches_list[i]))), int(i+1), \"none\")\n",
    "\t\telse:\n",
    "\t\t\tdf[\"batch\"] = np.where((df[\"file\"].isin(list(batches_list[i]))), int(i+1), df[\"batch\"])\n",
    "\n",
    "\textracted_dataframe_path = \"{}/results/{}/ParlaMint-{}-extracted-source-data.csv\".format(main_path, lang_code, lang_code)\n",
    "\n",
    "\t# Save the dataframe\n",
    "\tdf.to_csv(\"{}\".format(extracted_dataframe_path), sep=\"\\t\")\n",
    "\n",
    "\tprint(\"Dataframe saved as {}\".format(extracted_dataframe_path))\n",
    "\t\n",
    "\t# Show the results\n",
    "\tprint(df.describe(include=\"all\").to_markdown())\n",
    "\n",
    "\t# Save each batch separately to be translated separately\n",
    "\tfor i in list(df.batch.unique()):\n",
    "\t\tdf[df[\"batch\"] == i].to_csv(\"{}.{}.csv\".format(extracted_dataframe_path, i), sep=\"\\t\")\n",
    "\t\tprint(\"Batch {} saved as {}.{}.csv\".format(i, extracted_dataframe_path, i))\n",
    "\n",
    "\treturn df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "choose_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_model(lang_code, main_path):\n",
    "\t\"\"\"\n",
    "\tCompare a small sample of translations of all OPUS-MT models that are available\n",
    "\tfor the language, to decide which one to use. The function prints out a dataframe with all translations of the sample and saves it as ParlaMint-{lang_code}-sample-model-comparison.csv.\n",
    "\n",
    "\tArgs:\n",
    "\t- lang_code: the lang code that is used in the names of the files, it should be the same as for extract_text()\n",
    "\t\"\"\"\n",
    "\timport pandas as pd\n",
    "\timport regex as re\n",
    "\tfrom easynmt import EasyNMT\n",
    "\tfrom IPython.display import display\n",
    "\t\n",
    "\tlang_models_dict = {\"BG\": [\"bg\", \"sla\", \"zls\"], \"HR\": [\"zls\"], \"CZ\": [\"cs\", \"sla\", \"zlw\" ], \"DK\": [\"da\", \"gmq\", \"gem\"], \"NL\": [\"nl\", \"gem\", \"gmw\"], \"FR\": [\"fr\", \"itc\",\"roa\"], \"HU\": [\"mul\"], \"IS\": [\"is\",\"gmq\", \"gem\"], \"IT\": [\"it\", \"roa\", \"itc\"], \"LV\": [\"lv\",\"bat\"], \"LT\": [\"bat\"], \"PL\": [\"pl\", \"sla\", \"zlw\"], \"SI\": [\"sla\", \"zls\"], \"ES\": [\"es\", \"roa\", \"itc\"], \"TR\": [\"tr\", \"trk\" ], \"AT\": [\"de\", \"gem\", \"gmw\"], \"ES-PV\": [\"eu\", \"mul\"], \"BA\": [\"sla\", \"zls\"], \"ES-CT\": [\"ca\", \"roa\", \"itc\"], \"EE\": [\"et\", \"urj\", \"fiu\"], \"FI\": [\"fi\", \"urj\", \"fiu\"], \"ES-GA\": [\"gl\", \"roa\", \"itc\"], \"GR\": [\"grk\"], \"PT\": [\"roa\", \"itc\"], \"RO\":[\"roa\", \"itc\"], \"RS\": [\"zls\", \"sla\"], \"SE\": [\"sv\", \"gmq\", \"gem\"], \"UA\":[\"uk\", \"sla\", \"zle\"]}\n",
    "\n",
    "\textracted_dataframe_path = \"{}/results/{}/ParlaMint-{}-extracted-source-data.csv\".format(main_path, lang_code, lang_code)\n",
    "\n",
    "\t# Open the file, created in the previous step\n",
    "\tdf = pd.read_csv(\"{}\".format(extracted_dataframe_path), sep=\"\\t\", index_col=0)\n",
    "\n",
    "\t# Define the model\n",
    "\tmodel = EasyNMT('opus-mt')\n",
    "\n",
    "\tprint(\"Entire corpus has {} sentences and {} words.\".format(df[\"text\"].count(), df[\"length\"].sum()))\n",
    "\n",
    "\t# Create a smaller sample - just a couple of sentences from one file\n",
    "\tdf = df[df.file == list(df[\"file\"].unique())[0]][:20]\n",
    "\n",
    "\tprint(\"Sample files has {} sentences and {} words.\".format(df[\"text\"].count(), df[\"length\"].sum()))\n",
    "\n",
    "\t# Create a list of sentences from the df\n",
    "\tsentence_list = df.text.to_list()\n",
    "\n",
    "\t# Translate the sample using all available models for this language\n",
    "\tfor opus_lang_code in lang_models_dict[lang_code]:\n",
    "\t\ttranslation_list = model.translate(sentence_list, source_lang = \"{}\".format(opus_lang_code), target_lang='en')\n",
    "\n",
    "\t\t# Add the translations to the df\n",
    "\t\tdf[\"translation-{}\".format(opus_lang_code)] = translation_list\n",
    "\t\n",
    "\tdf = df.drop(columns=[\"file\", \"sentence_id\", \"tokenized_text\", \"proper_nouns\", \"length\"])\n",
    "\n",
    "\t# Save the df\n",
    "\tdf.to_csv(\"/home/tajak/Parlamint-translation/results/{}/ParlaMint-{}-sample-model-comparison.csv\".format(lang_code, lang_code))\n",
    "\n",
    "\tprint(\"The file is saved as/home/tajak/Parlamint-translation/ results/{}/ParlaMint-{}-sample-model-comparison.csv. \".format(lang_code, lang_code))\n",
    "\n",
    "\treturn df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "translate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(lang_code, opus_lang_code, main_path):\n",
    "\t\"\"\"\n",
    "\tThis function translates the text from the dataframe, created with the extract_text() function\n",
    "\twith OPUS-MT models using EasyNMT. It returns a dataframe with the translation.\n",
    "\n",
    "\tArgs:\n",
    "\t- lang_code: the lang code that is used in the names of the files, it should be the same as for extract_text()\n",
    "\t- opus_lang_code: the lang code to be used in the OPUS-MT model - use the one that performed the best in the comparison (see function choose_model())\n",
    "\t\"\"\"\n",
    "\timport pandas as pd\n",
    "\timport regex as re\n",
    "\tfrom easynmt import EasyNMT\n",
    "\tfrom IPython.display import display\n",
    "\timport time\n",
    "\n",
    "\textracted_dataframe_path = \"{}/results/{}/ParlaMint-{}-extracted-source-data.csv\".format(main_path, lang_code, lang_code)\n",
    "\n",
    "\ttranslated_dataframe_path = \"{}/results/{}/ParlaMint-{}-translated.csv\".format(main_path, lang_code, lang_code)\n",
    "\n",
    "\t# Open the file, created in the previous step\n",
    "\tdf = pd.read_csv(\"{}\".format(extracted_dataframe_path), sep=\"\\t\", index_col=0)\n",
    "\n",
    "\t# Define the model\n",
    "\tmodel = EasyNMT('opus-mt')\n",
    "\n",
    "\tprint(\"Entire corpus has {} sentences and {} words.\".format(df[\"text\"].count(), df[\"length\"].sum()))\n",
    "\n",
    "\t# Create a list of sentences from the df\n",
    "\tsentence_list = df.text.to_list()\n",
    "\n",
    "\tprint(\"Translation started.\")\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\n",
    "\t#Translate the list of sentences - you need to provide the source language as it is in the name of the model - the opus_lang_code\n",
    "\ttranslation_list = model.translate(sentence_list, source_lang = \"{}\".format(opus_lang_code), target_lang='en')\n",
    "\n",
    "\ttranslation_time = round((time.time() - start_time)/60,2)\n",
    "\n",
    "\tprint(\"Translation completed. It took {} minutes for {} instances - {} minutes per one sentence.\".format(translation_time, len(sentence_list), translation_time/len(sentence_list)))\n",
    "\n",
    "\t# Add the translations to the df\n",
    "\tdf[\"translation\"] = translation_list\n",
    "\n",
    "\t# Display the df\n",
    "\tdisplay(df[:3])\n",
    "\n",
    "\t# Save the df\n",
    "\tdf.to_csv(\"{}\".format(translated_dataframe_path), sep=\"\\t\")\n",
    "\n",
    "\tprint(\"The file is saved as {}\".format(translated_dataframe_path))\n",
    "\n",
    "\treturn df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenize_translation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_translation(lang_code, main_path):\n",
    "\timport stanza\n",
    "\n",
    "\tnlp = stanza.Pipeline(lang='en', processors='tokenize', tokenize_no_ssplit = True)\n",
    "\n",
    "\t# Apply tokenization to English translation and add the sentences to the df\n",
    "\t# Open the df\n",
    "\ttranslated_dataframe_path = \"{}/results/{}/ParlaMint-{}-translated.csv\".format(main_path, lang_code, lang_code)\n",
    "\n",
    "\tdf = pd.read_csv(\"{}\".format(translated_dataframe_path), sep=\"\\t\", index_col = 0)\n",
    "\n",
    "\t# Save also the information on whether there is a space after or before punctuation\n",
    "\t# which we will need later, to remove unnecessary spaces\n",
    "\tEn_sentences = df.translation.to_list()\n",
    "\n",
    "\ttokenized_sentences = []\n",
    "\tspace_after_list = []\n",
    "\n",
    "\tfor i in En_sentences:\n",
    "\t\tdoc = nlp(i).to_dict()\n",
    "\t\tcurrent_sentence_list = []\n",
    "\t\tcurrent_space_after_list = []\n",
    "\n",
    "\t\t# Define a list of start_char and end_char\n",
    "\t\tstart_chars = []\n",
    "\t\tend_chars = []\n",
    "\n",
    "\t\t# Loop through the tokens in the sentence and add them to a current sentence list\n",
    "\t\tfor sentence in doc:\n",
    "\t\t\tfor word in sentence:\n",
    "\t\t\t\tcurrent_sentence_list.append(word[\"text\"])\n",
    "\n",
    "\t\t\t\t# Add information on start and end chars to the list\n",
    "\t\t\t\tstart_chars.append(word[\"start_char\"])\n",
    "\t\t\t\tend_chars.append(word[\"end_char\"])\n",
    "\t\t\t\n",
    "\t\t# Now loop through the start_char and end_char lists and find instances\n",
    "\t\t# where the end_char of one word is the same as the start_char of the next one\n",
    "\t\t# this means there is no space between them\n",
    "\t\tfor char_index in range(len(start_chars)-1):\n",
    "\t\t\tif end_chars[char_index] == start_chars[(char_index+1)]:\n",
    "\t\t\t\tcurrent_space_after_list.append(\"No\")\n",
    "\t\t\telse:\n",
    "\t\t\t\tcurrent_space_after_list.append(\"Yes\")\n",
    "\n",
    "\t\t# This loop is not possible for the end token, so let's add information for the last token\n",
    "\t\t# just to avoid errors due to different lengths of lists\n",
    "\t\tcurrent_space_after_list.append(\"Last\")\n",
    "\n",
    "\t\t# Join the list into a space-separated string\n",
    "\t\tcurrent_string = \" \".join(current_sentence_list)\n",
    "\n",
    "\t\ttokenized_sentences.append(current_string)\n",
    "\n",
    "\t\tspace_after_list.append(current_space_after_list)\n",
    "\n",
    "\t# Add the result to the df\n",
    "\tdf[\"translation-tokenized\"] = tokenized_sentences\n",
    "\tdf[\"space-after-information\"] = space_after_list\n",
    "\n",
    "\ttranslated_tokenized_dataframe_path = \"{}/results/{}/ParlaMint-{}-translated-tokenized.csv\".format(main_path,lang_code, lang_code)\n",
    "\t# Save the df\n",
    "\tdf.to_csv(\"{}\".format(translated_tokenized_dataframe_path), sep=\"\\t\")\n",
    "\t\n",
    "\tprint(\"File saved as {}\".format(translated_tokenized_dataframe_path))\n",
    "\t\n",
    "\treturn df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alignment_file_to_target_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary from the returned alignment files which will be added to each word in the final conllu\n",
    "\n",
    "def alignment_file_to_target_dict(file):\n",
    "\t\"\"\"\n",
    "\tThe output of the eflomal aligner is in the source to target direction. We want to get the alignments in the other direction\n",
    "\tand for each target word add to the conllu its aligned source word index (as it appears in conllu). In conllu, indices start\n",
    "\twith 1, not 0. So, we take the eflomal files, reverse the order and create dictionaries with target indexes as keys\n",
    "\tand source indexes as values. If there are more than one words aligned to the same target word, it looks like this: '1, 2'.\n",
    "\tWe use the conllu indexes which means that we add 1 to each index in the alingment pairs. \n",
    "\n",
    "\tArgs:\n",
    "\t\t- file: the path to the .fwd and .rev file that is produced by the eflomal tool\n",
    "\n",
    "\tThe result is a list of dictionaries, each dictionary corresponds to one sentence.\n",
    "\t\"\"\"\n",
    "\t# Create target alignments from the source alignment direction (by changing the direction in the file)\n",
    "\taligns_list_target = open(file, \"r\").readlines()\n",
    "\taligns_list_target = [i.replace(\"\\n\", \"\") for i in aligns_list_target]\n",
    "\taligns_list_target = [i.split(\" \") for i in aligns_list_target]\n",
    "\n",
    "\taligns_list_target_dict_list = []\n",
    "\n",
    "\t# Loop through the alignments for sentences\n",
    "\tfor i in aligns_list_target:\n",
    "\t\t# Create a dictionary for each sentence\n",
    "\t\tcurrent_sentence_align = {}\n",
    "\t\t# For each alignment pair in the sentence:\n",
    "\t\tfor pair in i:\n",
    "\t\t\t# Split the pair: result is a list of lists with source index as the first element\n",
    "\t\t\t# and target index as the second element: [[0,0], [1,2], [1,3]]\n",
    "\t\t\tcurrent_pair = pair.split(\"-\")\n",
    "\n",
    "\t\t\t# Get the indices for target and source and add 1 to them (to get the conllu indices)\n",
    "\t\t\tcurrent_t_index = int(current_pair[1]) + 1\n",
    "\t\t\tcurrent_s_index = int(current_pair[0]) + 1\n",
    "\n",
    "\t\t\t# Check whether the target index is already aligned to anything (a case of 1-to-many alignment),\n",
    "\t\t\t# if not, save it as a key and save the source index as value.\n",
    "\t\t\tif current_sentence_align.get(current_t_index, None) == None:\n",
    "\t\t\t\tcurrent_sentence_align[current_t_index] = str(current_s_index)\n",
    "\t\t\t# If the index was already aligned to a previous source word, add the additional source word alignment as a string\n",
    "\t\t\t# (result: {0: \"1, 2\"))\n",
    "\t\t\telse:\n",
    "\t\t\t\tcurrent_sentence_align[current_t_index] += str(\", \")\n",
    "\t\t\t\tcurrent_sentence_align[current_t_index] += str(current_s_index)\n",
    "\n",
    "\t\taligns_list_target_dict_list.append(current_sentence_align)\n",
    "\n",
    "\treturn aligns_list_target_dict_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "correct_proper_nouns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_proper_nouns(lang_code, main_path):\n",
    "\t\"\"\"\n",
    "\tThis function takes the translated text and the source text, aligns words with eflomal and corrects proper nouns.\n",
    "\tIt takes the dataframe that was created in the function extract_text() and to which the translation was added\n",
    "\tin the function translate().\n",
    "\n",
    "\tTo use eflomal, you need to install it first:\n",
    "\t!git clone https://github.com/robertostling/eflomal\n",
    "\t%cd eflomal\n",
    "\t!make\n",
    "\t!sudo make install\n",
    "\t!python3 setup.py install\n",
    "\n",
    "\tIn case you don't have sudo permission, you can skip !sudo make install. I did, and I also used a virtual environment (venv), and managed to install eflomal.\n",
    "\n",
    "\tArgs:\n",
    "\t- lang_code: the lang code that is used in the names of the files, it should be the same as for extract_text()\n",
    "\t\"\"\"\n",
    "\timport pandas as pd\n",
    "\timport re\n",
    "\timport ast\n",
    "\tfrom IPython.display import display\n",
    "\n",
    "\t# Open the file, created in the previous step\n",
    "\ttranslated_tokenized_dataframe_path = \"{}/results/{}/ParlaMint-{}-translated-tokenized.csv\".format(main_path,lang_code, lang_code)\n",
    "\n",
    "\tdf = pd.read_csv(\"{}\".format(translated_tokenized_dataframe_path), sep=\"\\t\", index_col=0)\n",
    "\n",
    "\t# Move into the eflomal folder\n",
    "\t%cd /home/tajak/Parlamint-translation/eflomal\n",
    "\n",
    "\t# Then we need to create files for all texts and all translations\n",
    "\tsource_sentences = open(\"source_sentences.txt\", \"w\")\n",
    "\tEnglish_sentences = open(\"English_sentences.txt\", \"w\")\n",
    "\n",
    "\tfor i in df[\"tokenized_text\"].to_list():\n",
    "\t\tsource_sentences.write(i)\n",
    "\t\tsource_sentences.write(\"\\n\")\n",
    "\n",
    "\tfor i in df[\"translation-tokenized\"].to_list():\n",
    "\t\tEnglish_sentences.write(i)\n",
    "\t\tEnglish_sentences.write(\"\\n\")\n",
    "\n",
    "\tsource_sentences.close()\n",
    "\tEnglish_sentences.close()\n",
    "\n",
    "\t# Align sentences with eflomal and get out a file with alignments\n",
    "\t!python3 align.py -s source_sentences.txt -t English_sentences.txt --model 3 -r source-en.rev -f source-en.fwd\n",
    "\n",
    "\t# Create a list of dictionaries of alignments from the returned files which will be added to the final conllu for each word\n",
    "\tforward_alignment_dict_list = alignment_file_to_target_dict(\"source-en.fwd\")\n",
    "\tbackward_alignment_dict_list = alignment_file_to_target_dict(\"source-en.rev\")\n",
    "\n",
    "\t# Add to the df\n",
    "\tdf[\"fwd_align_dict\"] = forward_alignment_dict_list\n",
    "\tdf[\"bwd_align_dict\"] = backward_alignment_dict_list\n",
    "\n",
    "\t# Create forward target alignments from the source alignment direction (by changing the direction in the rev file)\n",
    "\taligns_list = open(\"source-en.rev\", \"r\").readlines()\n",
    "\taligns_list = [i.replace(\"\\n\", \"\") for i in aligns_list]\n",
    "\n",
    "\t# Continue with processing the list to create the final alignments format which I'll use to correct proper names\n",
    "\taligns_list = [i.split(\" \") for i in aligns_list]\n",
    "\n",
    "\tfor i in aligns_list:\n",
    "\t\tfor pair in i:\n",
    "\t\t\tcurrent_pair = pair.split(\"-\")\n",
    "\t\t\ti[i.index(pair)] = {int(current_pair[0]): int(current_pair[1])}\n",
    "\t\n",
    "\tfinal_aligns = []\n",
    "\n",
    "\t# Create a dictionary out of the rev alignments\n",
    "\tfor i in aligns_list:\n",
    "\t\tcurrent_line = {}\n",
    "\n",
    "\t\ttry:\n",
    "\t\t\tfor element in i:\n",
    "\t\t\t\ta = list(element.items())[0][0]\n",
    "\t\t\t\tb = list(element.items())[0][1]\n",
    "\t\t\t\tcurrent_line[a] = b\n",
    "\t\t\n",
    "\t\t\t# Check whether the number of pairs in the list is the same as number of items\n",
    "\t\t\tif len(i) != len(list(current_line.items())):\n",
    "\t\t\t\tprint(\"Not okay:\")\n",
    "\t\t\t\tprint(i)\n",
    "\t\t\t\tprint(current_line)\n",
    "\n",
    "\t\t\tfinal_aligns.append(current_line)\n",
    "\t\t\n",
    "\t\texcept:\n",
    "\t\t\tprint(\"error\")\n",
    "\t\t\tprint(aligns_list.index(i))\n",
    "\t\t\tprint(i)\n",
    "\t\t\tfinal_aligns.append(\"Error\")\n",
    "\t\t\n",
    "\tprint(\"Number of aligned sentences: {}\".format(len(final_aligns)))\n",
    "\n",
    "\t# Add a to the df\n",
    "\tdf[\"alignments\"] = final_aligns\n",
    "\n",
    "\t# Remove the rev and fwd file\n",
    "\t%rm source-en.rev\n",
    "\t%rm source-en.fwd\n",
    "\n",
    "\t# When we open the dataframe file, the dictionaries with proper names changed into strings - Change strings in the column proper_nouns into dictionaries\n",
    "\n",
    "\tdf[\"proper_nouns\"] = df.proper_nouns.astype(\"str\")\n",
    "\tdf[\"proper_nouns\"] = df.proper_nouns.apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "\t# Change nan values in the proper_nouns columns\n",
    "\tdf = df.fillna(0)\n",
    "\n",
    "\t# Substitute words in the translation based on alignments\n",
    "\tintermediate_list = list(zip(df[\"translation-tokenized\"], df[\"proper_nouns\"], df[\"alignments\"]))\n",
    "\n",
    "\tnew_translations = []\n",
    "\tsubstituted_all_info = []\n",
    "\tsubstituted_only = []\n",
    "\tsubstituted_words = []\n",
    "\n",
    "\t# Add information whether an error occurred\n",
    "\terror_list = []\n",
    "\n",
    "\tfor i in intermediate_list:\n",
    "\t\tcurrent_substituted_list = []\n",
    "\t\tcurrent_substituted_only = []\n",
    "\t\tcurrent_substituted_words = {}\n",
    "\t\tcurrent_error = \"No\"\n",
    "\n",
    "\t\t# If no proper names were detected, do not change the translation\n",
    "\t\tif i[1] == 0:\n",
    "\t\t\tnew_translations.append(i[0])\n",
    "\t\t\n",
    "\t\telse:\n",
    "\t\t\tcurrent_translation = i[0]\n",
    "\n",
    "\t\t\t# Substitute the word with the source lemma based on the index - loop through the proper nouns to be changed\n",
    "\t\t\tfor word_index in list(i[1].keys()):\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\t# split the translation into list of words\n",
    "\t\t\t\t\tword_list = current_translation.split()\n",
    "\n",
    "\t\t\t\t\t# Get index of the substituted word\n",
    "\t\t\t\t\tsubstituted_word_index = i[2][word_index]\n",
    "\n",
    "\t\t\t\t\t# Get the lemma to substitute the word with\n",
    "\t\t\t\t\tcorrect_lemma = i[1][word_index][1]\n",
    "\n",
    "\t\t\t\t\t# If the substitute word and lemma are not the same, get substituted word and its match\n",
    "\t\t\t\t\tif word_list[substituted_word_index] != correct_lemma:\n",
    "\t\t\t\t\t\tcurrent_substituted_list.append((word_list[substituted_word_index], correct_lemma))\n",
    "\t\t\t\t\t\tcurrent_substituted_only.append((word_list[substituted_word_index], correct_lemma))\n",
    "\n",
    "\t\t\t\t\t\t# Save information on which word was substituted with its conllu index (index + 1) as the key\n",
    "\t\t\t\t\t\tcurrent_substituted_words[int(substituted_word_index+1)] = word_list[substituted_word_index]\n",
    "\n",
    "\t\t\t\t\t\t# Substitute the word in the word list\n",
    "\t\t\t\t\t\tword_list[substituted_word_index] = correct_lemma\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t# Add information that substitution was not performed\n",
    "\t\t\t\t\t\tcurrent_substituted_list.append(f\"No substitution: {word_list[substituted_word_index], correct_lemma}\")\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t# Change the translation by merging the words back into a string\n",
    "\t\t\t\t\tcurrent_translation = \" \".join(word_list)\n",
    "\n",
    "\t\t\t\texcept:\n",
    "\t\t\t\t\tprint(f\"Issue: index {word_index}: {i[1][word_index]}\")\n",
    "\t\t\t\t\tcurrent_error = f\"Issue: index {word_index}: {i[1][word_index]}\"\n",
    "\n",
    "\t\t\t# After the loop through proper nouns, save the new translation\n",
    "\t\t\tnew_translations.append(current_translation)\n",
    "\t\t\n",
    "\t\t# Add information on what was substituted\n",
    "\t\tif len(substituted_all_info) != 0:\n",
    "\t\t\tsubstituted_all_info.append(current_substituted_list)\n",
    "\t\telse:\n",
    "\t\t\tsubstituted_all_info.append(0)\n",
    "\n",
    "\t\tif len(current_substituted_only) != 0:\n",
    "\t\t\tsubstituted_only.append(current_substituted_only)\n",
    "\t\telse:\n",
    "\t\t\tsubstituted_only.append(0)\n",
    "\n",
    "\t\terror_list.append(current_error)\n",
    "\n",
    "\t\tsubstituted_words.append(current_substituted_words)\n",
    "\n",
    "\n",
    "\t# Add to the df\n",
    "\tdf[\"new_translations\"] = new_translations\n",
    "\tdf[\"substitution_info\"] = substituted_all_info\n",
    "\tdf[\"substituted_pairs\"] = substituted_only\n",
    "\tdf[\"substituted_words\"] = substituted_words\n",
    "\tdf[\"errors\"] = error_list\n",
    "\n",
    "\t# Change the working directory once again\n",
    "\t%cd ..\n",
    "\n",
    "\t# Add the word list with indices to the df\n",
    "\ttokenized_text_list = df.tokenized_text.to_list()\n",
    "\ttokenized_text_list = [i.split(\" \") for i in tokenized_text_list]\n",
    "\ttokenized_text_dict_list = []\n",
    "\n",
    "\tfor sentence in tokenized_text_list:\n",
    "\t\tsentence_list = []\n",
    "\t\tcounter = 1\n",
    "\t\tfor word in sentence:\n",
    "\t\t\tsentence_list.append([word, counter])\n",
    "\t\t\tcounter += 1\n",
    "\t\ttokenized_text_dict_list.append(sentence_list)\n",
    "\n",
    "\tdf[\"source_indices\"] = tokenized_text_dict_list\n",
    "\n",
    "\t# Save the df\n",
    "\tfinal_dataframe = \"{}/results/{}/ParlaMint-{}-final-dataframe.csv\".format(main_path,lang_code, lang_code)\n",
    "\tdf.to_csv(\"{}\".format(final_dataframe), sep=\"\\t\")\n",
    "\n",
    "\t# Display most common substitutions\n",
    "\tdf_substituted = df[df[\"proper_nouns\"] != \"0\"]\n",
    "\tdisplay(df_substituted.substituted_pairs.value_counts()[:20])\n",
    "\n",
    "\treturn df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create_conllu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conllu(file, lang_code, main_path, nlp):\n",
    "\t\"\"\"\n",
    "\tThe function takes the dataframe (df), created in previous steps and takes only the instances from the df that belong\n",
    "\tto the file that is in the argument. It linguistically processes the translated sentences from the file and saves the file.\n",
    "\tThen we add additional information (metadata and NER annotations) to it with the conllu parser and save the final conllu file.\n",
    "\n",
    "\tArgs:\n",
    "\t\t- file (str): file name from the files list (see above)\n",
    "\t\t- lang_code (str): the lang code that is used in the names of the files, it should be the same as for extract_text()\n",
    "\t\"\"\"\n",
    "\n",
    "\t# Process all sentences in the dataframe and save them to a conllu file\n",
    "\tfrom stanza.utils.conll import CoNLL\n",
    "\timport stanza\n",
    "\tfrom conllu import parse\n",
    "\timport ast\n",
    "\timport regex as re\n",
    "\timport os\n",
    "\timport pandas as pd\n",
    "\n",
    "\tfinal_dataframe = \"{}/results/{}/ParlaMint-{}-final-dataframe.csv\".format(main_path,lang_code, lang_code)\n",
    "\n",
    "\t# Use the dataframe, created in previous steps\n",
    "\tdf = pd.read_csv(\"{}\".format(final_dataframe), sep=\"\\t\", index_col = 0)\n",
    "\n",
    "\t# Filter out only instances from the file in question\n",
    "\tdf = df[df[\"file\"] == file]\n",
    "\n",
    "\t# Add information on the target path\n",
    "\tdf[\"target_path\"] = df.file_path.str.replace(\"Source-data\", \"Final-data\")\n",
    "\n",
    "\t# Get target path\n",
    "\ttarget_path = list(df.target_path.unique())[0]\n",
    "\n",
    "\t# When we open the dataframe file, the lists and dictionaries turn into strings - change them back\n",
    "\tfor column in [\"space-after-information\", 'fwd_align_dict', 'bwd_align_dict', 'substituted_words', \"source_indices\"]:\n",
    "\t\tdf[column] = df[column].astype(\"str\")\n",
    "\t\tdf[column] = df[column].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "\t# Create lists of information that we need to add to the conllu file\n",
    "\tids_list = df.sentence_id.to_list()\n",
    "\tsource_text = df.text.to_list()\n",
    "\tinitial_translation = df.translation.to_list()\n",
    "\tspace_after_list = df[\"space-after-information\"].to_list()\n",
    "\tfwd_align_list = df['fwd_align_dict'].to_list()\n",
    "\tbwd_align_list = df['bwd_align_dict'].to_list()\n",
    "\tsubstituted_words_list = df['substituted_words'].to_list()\n",
    "\ttokenized_text_list = df[\"source_indices\"].to_list()\n",
    "\tsentence_list = df.new_translations.to_list()\n",
    "\n",
    "\t# To feed the entire list into the pipeline, we need to create lists of tokens, split by space\n",
    "\tsentence_list = [x.split(\" \") for x in sentence_list]\n",
    "\t\n",
    "\t# Linguistically process the list\n",
    "\tdoc = nlp(sentence_list)\n",
    "\n",
    "\t# Save the conllu file\n",
    "\tCoNLL.write_doc2conll(doc, \"{}/results/{}/temp/{}\".format(main_path, lang_code, file))\n",
    "\n",
    "\tprint(\"{} processed and saved.\".format(file))\n",
    "\n",
    "\t# Open the CONLL-u file with the CONLL-u parser\n",
    "\n",
    "\tdata = open(\"{}/results/{}/temp/{}\".format(main_path, lang_code, file), \"r\").read()\n",
    "\n",
    "\tsentences = parse(data)\n",
    "\n",
    "\t# Adding additional information to the conllu\n",
    "\tfor sentence in sentences:\n",
    "\t\t# Get the sentence index\n",
    "\t\tsentence_index = sentences.index(sentence)\n",
    "\n",
    "\t\t# Add metadata\n",
    "\t\tsentence.metadata[\"sent_id\"] = ids_list[sentence_index]\n",
    "\t\tsentence.metadata[\"source\"] = source_text[sentence_index]\n",
    "\t\tsentence.metadata[\"source_indices\"] = tokenized_text_list[sentence_index]\n",
    "\t\tsentence.metadata[\"initial_translation\"] = initial_translation[sentence_index]\n",
    "\n",
    "\t\t# Delete the current metadata for text\n",
    "\t\tdel sentence.metadata[\"text\"]\n",
    "\n",
    "\t\tnew_translation_text = \"\"\n",
    "\n",
    "\t\t# Iterate through tokens\n",
    "\t\tfor word in sentence:\n",
    "\t\t\tword_index = sentence.index(word)\n",
    "\t\t\tword_conllu_index = word[\"id\"]\n",
    "\n",
    "\t\t\t# Check whether the word conllu index (word id) is in the substituted_words_list (it is if it was substituted)\n",
    "\t\t\t# If it is, add information on the original translated word\n",
    "\t\t\tif substituted_words_list[sentence_index].get(word_conllu_index, None) != None:\n",
    "\t\t\t\tword[\"misc\"][\"Translated\"] = substituted_words_list[sentence_index][word_conllu_index]\n",
    "\t\t\t\n",
    "\t\t\t# Do the same for the forward and backward alignment\n",
    "\t\t\tif fwd_align_list[sentence_index].get(word_conllu_index, None) != None:\n",
    "\t\t\t\tword[\"misc\"][\"ForwardAlignment\"] = fwd_align_list[sentence_index][word_conllu_index]\n",
    "\n",
    "\t\t\tif bwd_align_list[sentence_index].get(word_conllu_index, None) != None:\n",
    "\t\t\t\tword[\"misc\"][\"BackwardAlignment\"] = bwd_align_list[sentence_index][word_conllu_index]\n",
    "\n",
    "\t\t\t# Remove information on start_char and end_char from the annotation\n",
    "\t\t\tdel word[\"misc\"][\"start_char\"]\n",
    "\t\t\tdel word[\"misc\"][\"end_char\"]\n",
    "\t\t\t\n",
    "\t\t\t# Change the NER tags so that they are the same as in the source\n",
    "\t\t\tcurrent_ner = word[\"misc\"][\"ner\"]\n",
    "\t\t\tdel word[\"misc\"][\"ner\"]\n",
    "\t\t\t\n",
    "\t\t\t# Substitute parts of the tags so that they are tha same as in source\n",
    "\t\t\tcurrent_ner = re.sub(\"S-\", \"B-\", current_ner)\n",
    "\t\t\tcurrent_ner = re.sub(\"E-\", \"I-\", current_ner)\n",
    "\n",
    "\t\t\tword[\"misc\"][\"NER\"] = current_ner\n",
    "\n",
    "\t\t\t# Get information about the space after based on the index\n",
    "\t\t\tcurrent_space_after = space_after_list[sentence_index][word_index]\n",
    "\n",
    "\t\t# Create new text from translation, correcting the spaces around words\n",
    "\t\t# based on the SpaceAfter information\n",
    "\t\t\tif current_space_after == \"No\":\n",
    "\t\t\t\tword[\"misc\"][\"SpaceAfter\"] = \"No\"\n",
    "\t\t\t\tnew_translation_text += word[\"form\"]\n",
    "\t\t\telif current_space_after == \"Last\":\n",
    "\t\t\t\tnew_translation_text += word[\"form\"]\n",
    "\t\t\telse:\n",
    "\t\t\t\tnew_translation_text += word[\"form\"]\n",
    "\t\t\t\tnew_translation_text += \" \"\n",
    "\t\t\n",
    "\t\tsentence.metadata[\"text\"] = new_translation_text\n",
    "\t\n",
    "\t# Create a new conllu file with the updated information\n",
    "\n",
    "\t#final_file = open(\"{}/results/{}/final_translated_conllu/{}\".format(main_path,lang_code, file), \"w\")\n",
    "\tos.makedirs(os.path.dirname(target_path), exist_ok=True)\n",
    "\tfinal_file = open(\"{}\".format(target_path), \"w\")\n",
    "\n",
    "\tfor sentence in sentences:\n",
    "\t\tfinal_file.write(sentence.serialize())\n",
    "\t\n",
    "\tfinal_file.close()\n",
    "\n",
    "\tprint(\"Final file {} is saved.\".format(target_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "produce_final_conllu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_final_conllu(lang_code, main_path):\n",
    "\timport pandas as pd\n",
    "\timport stanza\n",
    "\t\n",
    "\t# Open df\n",
    "\tfinal_dataframe = \"{}/results/{}/ParlaMint-{}-final-dataframe.csv\".format(main_path,lang_code, lang_code)\n",
    "\n",
    "\tdf = pd.read_csv(\"{}\".format(final_dataframe), sep=\"\\t\", index_col=0)\n",
    "\n",
    "\t# Create a list of files\n",
    "\tfiles = list(df.file.unique())\n",
    "\n",
    "\t# Define the pipeline, instruct it to use a specific package: \tCoNLL03\n",
    "\tnlp = stanza.Pipeline(lang='en', processors=\"tokenize,mwt,pos,lemma,ner\", package={\"ner\": [\"conll03\"]}, tokenize_pretokenized=True)\n",
    "\n",
    "\t# Test file\n",
    "\t#for file in [\"ParlaMint-CZ_2013-11-25-ps2013-001-01-002-002.conllu\"]:\n",
    "\t#\tcreate_conllu(file, lang_code)\n",
    "\n",
    "\tfor file in files:\n",
    "\t\tcreate_conllu(file, lang_code, main_path, nlp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conllu import parse\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import os\n",
    "import zipfile\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip the folder with the files\n",
    "#with zipfile.ZipFile(\"/home/tajak/Parlamint-translation/ParlaMint-CZ/ParlaMint-CZ.conllu.zip\", 'r') as zip_ref:\n",
    "#    zip_ref.extractall(\"/home/tajak/Parlamint-translation/ParlaMint-CZ/ParlaMint-CZ.conllu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘results/CZ’: File exists\n"
     ]
    }
   ],
   "source": [
    "# Define the language code, used in the file names\n",
    "lang_code = \"CZ\"\n",
    "\n",
    "# Main path\n",
    "main_path = \"/home/tajak/Parlamint-translation\"\n",
    "\n",
    "# Define the translation model to be used\n",
    "opus_lang_code = \"cs\"\n",
    "\n",
    "# Check whether the path to the folder with conllu files is ok\n",
    "path = \"{}/Source-data/ParlaMint-{}.conllu/ParlaMint-{}.conllu\".format(main_path, lang_code, lang_code)\n",
    "\n",
    "# Create a folder with results for this language, e.g. results/CZ\n",
    "%mkdir results/CZ\n",
    "\n",
    "# Create (manually) a \"temp\" folder inside the results/CZ\n",
    "\n",
    "# Define other paths\n",
    "extracted_dataframe_path = \"{}/results/{}/ParlaMint-{}-extracted-source-data.csv\".format(main_path, lang_code, lang_code)\n",
    "\n",
    "translated_dataframe_path = \"{}/results/{}/ParlaMint-{}-translated.csv\".format(main_path, lang_code, lang_code)\n",
    "translated_tokenized_dataframe_path = \"{}/results/{}/ParlaMint-{}-translated-tokenized.csv\".format(main_path,lang_code, lang_code)\n",
    "final_dataframe = \"{}/results/{}/ParlaMint-{}-final-dataframe.csv\".format(main_path,lang_code, lang_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/tajak/Parlamint-translation/Source-data/ParlaMint-CZ.conllu/ParlaMint-CZ.conllu/2013/ParlaMint-CZ_2013-11-27-ps2013-001-02-016-021.conllu', '/home/tajak/Parlamint-translation/Source-data/ParlaMint-CZ.conllu/ParlaMint-CZ.conllu/2013/ParlaMint-CZ_2013-12-10-ps2013-004-01-014-022.conllu']\n",
      "['ParlaMint-CZ_2013-11-27-ps2013-001-02-016-021.conllu', 'ParlaMint-CZ_2013-12-10-ps2013-004-01-014-022.conllu']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6328"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract a list with paths to conllu files and a list with their names\n",
    "parl_list = []\n",
    "file_name_list = []\n",
    "\n",
    "for dir1 in os.listdir(path):\n",
    "    full_path = os.path.join(path, dir1)\n",
    "    if os.path.isdir(full_path):\n",
    "        current = os.listdir(full_path)\n",
    "        # Keep only files with parliamentary sessions:\n",
    "        for file in current:\n",
    "            if \"ParlaMint-{}_\".format(lang_code) in file:\n",
    "                if \".conllu\" in file:\n",
    "                    final_path = \"{}/{}\".format(full_path, file)\n",
    "                    parl_list.append(final_path)\n",
    "                    file_name_list.append(file)\n",
    "\n",
    "print(parl_list[:2])\n",
    "print(file_name_list[:2])\n",
    "\n",
    "# See how many files we have:\n",
    "len(parl_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract information from CONLL-U files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conllu_to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the corpora: 13762\n",
      "File is separated into 3 batches, sizes of batches (in no. of files): 2, 2, 1.\n",
      "Dataframe saved as /home/tajak/Parlamint-translation/results/CZ/ParlaMint-CZ-extracted-source-data.csv\n",
      "|        | file_path                                                                                                                                       | file                                                 | sentence_id                                            | text    | tokenized_text   | proper_nouns   |   length |   batch |\n",
      "|:-------|:------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------|:-------------------------------------------------------|:--------|:-----------------|:---------------|---------:|--------:|\n",
      "| count  | 956                                                                                                                                             | 956                                                  | 956                                                    | 956     | 956              | 956            | 956      |     956 |\n",
      "| unique | 5                                                                                                                                               | 5                                                    | 956                                                    | 844     | 844              | 134            | nan      |       3 |\n",
      "| top    | /home/tajak/Parlamint-translation/Source-data/ParlaMint-CZ.conllu/ParlaMint-CZ.conllu/2013/ParlaMint-CZ_2013-11-27-ps2013-001-02-014-017.conllu | ParlaMint-CZ_2013-11-27-ps2013-001-02-014-017.conllu | ParlaMint-CZ_2013-11-27-ps2013-001-02-016-021.u1.p1.s1 | Děkuji. | Děkuji .         | {}             | nan      |       3 |\n",
      "| freq   | 637                                                                                                                                             | 637                                                  | 1                                                      | 28      | 28               | 819            | nan      |     637 |\n",
      "| mean   | nan                                                                                                                                             | nan                                                  | nan                                                    | nan     | nan              | nan            |  14.3954 |     nan |\n",
      "| std    | nan                                                                                                                                             | nan                                                  | nan                                                    | nan     | nan              | nan            |  12.9001 |     nan |\n",
      "| min    | nan                                                                                                                                             | nan                                                  | nan                                                    | nan     | nan              | nan            |   1      |     nan |\n",
      "| 25%    | nan                                                                                                                                             | nan                                                  | nan                                                    | nan     | nan              | nan            |   5      |     nan |\n",
      "| 50%    | nan                                                                                                                                             | nan                                                  | nan                                                    | nan     | nan              | nan            |  11      |     nan |\n",
      "| 75%    | nan                                                                                                                                             | nan                                                  | nan                                                    | nan     | nan              | nan            |  19      |     nan |\n",
      "| max    | nan                                                                                                                                             | nan                                                  | nan                                                    | nan     | nan              | nan            | 111      |     nan |\n",
      "Batch 1 saved as /home/tajak/Parlamint-translation/results/CZ/ParlaMint-CZ-extracted-source-data.csv.1.csv\n",
      "Batch 2 saved as /home/tajak/Parlamint-translation/results/CZ/ParlaMint-CZ-extracted-source-data.csv.2.csv\n",
      "Batch 3 saved as /home/tajak/Parlamint-translation/results/CZ/ParlaMint-CZ-extracted-source-data.csv.3.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>file</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>proper_nouns</th>\n",
       "      <th>length</th>\n",
       "      <th>batch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/tajak/Parlamint-translation/Source-data/...</td>\n",
       "      <td>ParlaMint-CZ_2013-11-27-ps2013-001-02-016-021....</td>\n",
       "      <td>ParlaMint-CZ_2013-11-27-ps2013-001-02-016-021....</td>\n",
       "      <td>21.</td>\n",
       "      <td>21 .</td>\n",
       "      <td>{}</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/tajak/Parlamint-translation/Source-data/...</td>\n",
       "      <td>ParlaMint-CZ_2013-11-27-ps2013-001-02-016-021....</td>\n",
       "      <td>ParlaMint-CZ_2013-11-27-ps2013-001-02-016-021....</td>\n",
       "      <td>Návrh termínu 2. schůze Poslanecké sněmovny</td>\n",
       "      <td>Návrh termínu 2 . schůze Poslanecké sněmovny</td>\n",
       "      <td>{}</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/tajak/Parlamint-translation/Source-data/...</td>\n",
       "      <td>ParlaMint-CZ_2013-11-27-ps2013-001-02-016-021....</td>\n",
       "      <td>ParlaMint-CZ_2013-11-27-ps2013-001-02-016-021....</td>\n",
       "      <td>Vážené paní poslankyně a páni poslanci, připra...</td>\n",
       "      <td>Vážené paní poslankyně a páni poslanci , připr...</td>\n",
       "      <td>{}</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/tajak/Parlamint-translation/Source-data/...</td>\n",
       "      <td>ParlaMint-CZ_2013-11-27-ps2013-001-02-016-021....</td>\n",
       "      <td>ParlaMint-CZ_2013-11-27-ps2013-001-02-016-021....</td>\n",
       "      <td>Protože tento výbor bude ustaven až na 2. schů...</td>\n",
       "      <td>Protože tento výbor bude ustaven až na 2 . sch...</td>\n",
       "      <td>{}</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/tajak/Parlamint-translation/Source-data/...</td>\n",
       "      <td>ParlaMint-CZ_2013-11-27-ps2013-001-02-016-021....</td>\n",
       "      <td>ParlaMint-CZ_2013-11-27-ps2013-001-02-016-021....</td>\n",
       "      <td>Chtěl bych upozornit, že pracovní návrh 2. sch...</td>\n",
       "      <td>Chtěl bych upozornit , že pracovní návrh 2 . s...</td>\n",
       "      <td>{}</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_path  \\\n",
       "0  /home/tajak/Parlamint-translation/Source-data/...   \n",
       "1  /home/tajak/Parlamint-translation/Source-data/...   \n",
       "2  /home/tajak/Parlamint-translation/Source-data/...   \n",
       "3  /home/tajak/Parlamint-translation/Source-data/...   \n",
       "4  /home/tajak/Parlamint-translation/Source-data/...   \n",
       "\n",
       "                                                file  \\\n",
       "0  ParlaMint-CZ_2013-11-27-ps2013-001-02-016-021....   \n",
       "1  ParlaMint-CZ_2013-11-27-ps2013-001-02-016-021....   \n",
       "2  ParlaMint-CZ_2013-11-27-ps2013-001-02-016-021....   \n",
       "3  ParlaMint-CZ_2013-11-27-ps2013-001-02-016-021....   \n",
       "4  ParlaMint-CZ_2013-11-27-ps2013-001-02-016-021....   \n",
       "\n",
       "                                         sentence_id  \\\n",
       "0  ParlaMint-CZ_2013-11-27-ps2013-001-02-016-021....   \n",
       "1  ParlaMint-CZ_2013-11-27-ps2013-001-02-016-021....   \n",
       "2  ParlaMint-CZ_2013-11-27-ps2013-001-02-016-021....   \n",
       "3  ParlaMint-CZ_2013-11-27-ps2013-001-02-016-021....   \n",
       "4  ParlaMint-CZ_2013-11-27-ps2013-001-02-016-021....   \n",
       "\n",
       "                                                text  \\\n",
       "0                                                21.   \n",
       "1        Návrh termínu 2. schůze Poslanecké sněmovny   \n",
       "2  Vážené paní poslankyně a páni poslanci, připra...   \n",
       "3  Protože tento výbor bude ustaven až na 2. schů...   \n",
       "4  Chtěl bych upozornit, že pracovní návrh 2. sch...   \n",
       "\n",
       "                                      tokenized_text proper_nouns  length  \\\n",
       "0                                               21 .           {}       1   \n",
       "1       Návrh termínu 2 . schůze Poslanecké sněmovny           {}       6   \n",
       "2  Vážené paní poslankyně a páni poslanci , připr...           {}      19   \n",
       "3  Protože tento výbor bude ustaven až na 2 . sch...           {}      26   \n",
       "4  Chtěl bych upozornit , že pracovní návrh 2 . s...           {}      15   \n",
       "\n",
       "  batch  \n",
       "0     1  \n",
       "1     1  \n",
       "2     1  \n",
       "3     1  \n",
       "4     1  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = conllu_to_df(parl_list[:5], file_name_list[:5], main_path, lang_code)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to translate the following corpora into English:\n",
    "- Belgian (BE) - which language??\n",
    "- Bulgarian (BG)\n",
    "- Croatian (HR) - We will use \"South Slavic MT\" based on the manual analysis\n",
    "- Czech (CZ)\n",
    "- Danish (DK)\n",
    "- Dutch (NL)\n",
    "- French (FR)\n",
    "- Hungarian (HU) - multilingual model only\n",
    "- Icelandic (IS)\n",
    "- Italian (IT)\n",
    "- Latvian (LV)\n",
    "- Lithuanian (LT)\n",
    "- Polish (PL)\n",
    "- Slovenian (SI) - We will use \"Slavic MT\" based on the results of the manual analysis\n",
    "- Spanish? (ES)\n",
    "- Turkish (TR)\n",
    "- Austrian (AT)\n",
    "- Basque (ES-PV)\n",
    "- Bosnian (BA)\n",
    "- Catalan (ES-CT)\n",
    "- Estonian (EE)\n",
    "- Finnish (FI)\n",
    "- Galician (ES-GA)\n",
    "- Greek (GR)\n",
    "- Norwegian (NO) - NO OPUS-MT model (!) - we can use GT or eTranslation\n",
    "- Portuguese (PT)\n",
    "- Romanian (RO)\n",
    "- Serbian (RS)\n",
    "- Swedish (SE)\n",
    "- Ukrainian (UA)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "choose_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entire corpus has 956 sentences and 13762 words.\n",
      "Sample files has 20 sentences and 246 words.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tajak/Parlamint-translation/parlamint_env/lib/python3.8/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 512 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/tajak/Parlamint-translation/parlamint_env/lib/python3.8/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 512 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/tajak/Parlamint-translation/parlamint_env/lib/python3.8/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 512 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file is saved as/home/tajak/Parlamint-translation/ results/CZ/ParlaMint-CZ-sample-model-comparison.csv. \n"
     ]
    }
   ],
   "source": [
    "df = choose_model(lang_code, main_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>text</th>\n",
       "      <th>translation-cs</th>\n",
       "      <th>translation-sla</th>\n",
       "      <th>translation-zlw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/tajak/Parlamint-translation/Source-data/...</td>\n",
       "      <td>21.</td>\n",
       "      <td>21.</td>\n",
       "      <td>21.</td>\n",
       "      <td>21.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/tajak/Parlamint-translation/Source-data/...</td>\n",
       "      <td>Návrh termínu 2. schůze Poslanecké sněmovny</td>\n",
       "      <td>Draft date of the 2nd meeting of the Chamber o...</td>\n",
       "      <td>Draft deadline 2nd meeting of the Chamber of A...</td>\n",
       "      <td>Proposal for a 2nd meeting of the Chamber of D...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_path  \\\n",
       "0  /home/tajak/Parlamint-translation/Source-data/...   \n",
       "1  /home/tajak/Parlamint-translation/Source-data/...   \n",
       "\n",
       "                                          text  \\\n",
       "0                                          21.   \n",
       "1  Návrh termínu 2. schůze Poslanecké sněmovny   \n",
       "\n",
       "                                      translation-cs  \\\n",
       "0                                                21.   \n",
       "1  Draft date of the 2nd meeting of the Chamber o...   \n",
       "\n",
       "                                     translation-sla  \\\n",
       "0                                                21.   \n",
       "1  Draft deadline 2nd meeting of the Chamber of A...   \n",
       "\n",
       "                                     translation-zlw  \n",
       "0                                                21.  \n",
       "1  Proposal for a 2nd meeting of the Chamber of D...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Then open the sample and manually evaluate which model is better in the column \"comparison\"\n",
    "# Open the analysed sample\n",
    "\n",
    "sample = pd.read_csv(\"/home/tajak/Parlamint-translation/results/{}/ParlaMint-{}-sample-model-comparison.csv\".format(lang_code, lang_code), index_col = 0)\n",
    "sample.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.comparison.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model for Czech was shown to be cs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "translate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entire corpus has 956 sentences and 13762 words.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tajak/Parlamint-translation/parlamint_env/lib/python3.8/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 512 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>file</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>proper_nouns</th>\n",
       "      <th>length</th>\n",
       "      <th>translation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/tajak/Parlamint-translation/Source-data/...</td>\n",
       "      <td>ParlaMint-CZ_2013-11-27-ps2013-001-02-016-021....</td>\n",
       "      <td>ParlaMint-CZ_2013-11-27-ps2013-001-02-016-021....</td>\n",
       "      <td>21.</td>\n",
       "      <td>21 .</td>\n",
       "      <td>{}</td>\n",
       "      <td>1</td>\n",
       "      <td>21.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/tajak/Parlamint-translation/Source-data/...</td>\n",
       "      <td>ParlaMint-CZ_2013-11-27-ps2013-001-02-016-021....</td>\n",
       "      <td>ParlaMint-CZ_2013-11-27-ps2013-001-02-016-021....</td>\n",
       "      <td>Návrh termínu 2. schůze Poslanecké sněmovny</td>\n",
       "      <td>Návrh termínu 2 . schůze Poslanecké sněmovny</td>\n",
       "      <td>{}</td>\n",
       "      <td>6</td>\n",
       "      <td>Draft date of the 2nd meeting of the Chamber o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/tajak/Parlamint-translation/Source-data/...</td>\n",
       "      <td>ParlaMint-CZ_2013-11-27-ps2013-001-02-016-021....</td>\n",
       "      <td>ParlaMint-CZ_2013-11-27-ps2013-001-02-016-021....</td>\n",
       "      <td>Vážené paní poslankyně a páni poslanci, připra...</td>\n",
       "      <td>Vážené paní poslankyně a páni poslanci , připr...</td>\n",
       "      <td>{}</td>\n",
       "      <td>19</td>\n",
       "      <td>The honourable Members, prepare the House meet...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_path  \\\n",
       "0  /home/tajak/Parlamint-translation/Source-data/...   \n",
       "1  /home/tajak/Parlamint-translation/Source-data/...   \n",
       "2  /home/tajak/Parlamint-translation/Source-data/...   \n",
       "\n",
       "                                                file  \\\n",
       "0  ParlaMint-CZ_2013-11-27-ps2013-001-02-016-021....   \n",
       "1  ParlaMint-CZ_2013-11-27-ps2013-001-02-016-021....   \n",
       "2  ParlaMint-CZ_2013-11-27-ps2013-001-02-016-021....   \n",
       "\n",
       "                                         sentence_id  \\\n",
       "0  ParlaMint-CZ_2013-11-27-ps2013-001-02-016-021....   \n",
       "1  ParlaMint-CZ_2013-11-27-ps2013-001-02-016-021....   \n",
       "2  ParlaMint-CZ_2013-11-27-ps2013-001-02-016-021....   \n",
       "\n",
       "                                                text  \\\n",
       "0                                                21.   \n",
       "1        Návrh termínu 2. schůze Poslanecké sněmovny   \n",
       "2  Vážené paní poslankyně a páni poslanci, připra...   \n",
       "\n",
       "                                      tokenized_text proper_nouns  length  \\\n",
       "0                                               21 .           {}       1   \n",
       "1       Návrh termínu 2 . schůze Poslanecké sněmovny           {}       6   \n",
       "2  Vážené paní poslankyně a páni poslanci , připr...           {}      19   \n",
       "\n",
       "                                         translation  \n",
       "0                                                21.  \n",
       "1  Draft date of the 2nd meeting of the Chamber o...  \n",
       "2  The honourable Members, prepare the House meet...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file is saved as /home/tajak/Parlamint-translation/results/CZ/ParlaMint-CZ-translated.csv\n"
     ]
    }
   ],
   "source": [
    "df = translate(lang_code, opus_lang_code, main_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word alignment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenize_translation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-25 15:29:13 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json: 193kB [00:00, 51.8MB/s]                    \n",
      "2023-01-25 15:29:13 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "========================\n",
      "\n",
      "2023-01-25 15:29:13 INFO: Use device: gpu\n",
      "2023-01-25 15:29:13 INFO: Loading: tokenize\n",
      "2023-01-25 15:29:13 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved as /home/tajak/Parlamint-translation/results/CZ/ParlaMint-CZ-translated-tokenized.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>file</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>proper_nouns</th>\n",
       "      <th>length</th>\n",
       "      <th>translation</th>\n",
       "      <th>translation-tokenized</th>\n",
       "      <th>space-after-information</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/tajak/Parlamint-translation/Source-data/...</td>\n",
       "      <td>ParlaMint-CZ_2013-11-27-ps2013-001-02-016-021....</td>\n",
       "      <td>ParlaMint-CZ_2013-11-27-ps2013-001-02-016-021....</td>\n",
       "      <td>21.</td>\n",
       "      <td>21 .</td>\n",
       "      <td>{}</td>\n",
       "      <td>1</td>\n",
       "      <td>21.</td>\n",
       "      <td>21 .</td>\n",
       "      <td>[No, Last]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/tajak/Parlamint-translation/Source-data/...</td>\n",
       "      <td>ParlaMint-CZ_2013-11-27-ps2013-001-02-016-021....</td>\n",
       "      <td>ParlaMint-CZ_2013-11-27-ps2013-001-02-016-021....</td>\n",
       "      <td>Návrh termínu 2. schůze Poslanecké sněmovny</td>\n",
       "      <td>Návrh termínu 2 . schůze Poslanecké sněmovny</td>\n",
       "      <td>{}</td>\n",
       "      <td>6</td>\n",
       "      <td>Draft date of the 2nd meeting of the Chamber o...</td>\n",
       "      <td>Draft date of the 2nd meeting of the Chamber o...</td>\n",
       "      <td>[Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/tajak/Parlamint-translation/Source-data/...</td>\n",
       "      <td>ParlaMint-CZ_2013-11-27-ps2013-001-02-016-021....</td>\n",
       "      <td>ParlaMint-CZ_2013-11-27-ps2013-001-02-016-021....</td>\n",
       "      <td>Vážené paní poslankyně a páni poslanci, připra...</td>\n",
       "      <td>Vážené paní poslankyně a páni poslanci , připr...</td>\n",
       "      <td>{}</td>\n",
       "      <td>19</td>\n",
       "      <td>The honourable Members, prepare the House meet...</td>\n",
       "      <td>The honourable Members , prepare the House mee...</td>\n",
       "      <td>[Yes, Yes, No, Yes, Yes, Yes, Yes, Yes, Yes, Y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_path  \\\n",
       "0  /home/tajak/Parlamint-translation/Source-data/...   \n",
       "1  /home/tajak/Parlamint-translation/Source-data/...   \n",
       "2  /home/tajak/Parlamint-translation/Source-data/...   \n",
       "\n",
       "                                                file  \\\n",
       "0  ParlaMint-CZ_2013-11-27-ps2013-001-02-016-021....   \n",
       "1  ParlaMint-CZ_2013-11-27-ps2013-001-02-016-021....   \n",
       "2  ParlaMint-CZ_2013-11-27-ps2013-001-02-016-021....   \n",
       "\n",
       "                                         sentence_id  \\\n",
       "0  ParlaMint-CZ_2013-11-27-ps2013-001-02-016-021....   \n",
       "1  ParlaMint-CZ_2013-11-27-ps2013-001-02-016-021....   \n",
       "2  ParlaMint-CZ_2013-11-27-ps2013-001-02-016-021....   \n",
       "\n",
       "                                                text  \\\n",
       "0                                                21.   \n",
       "1        Návrh termínu 2. schůze Poslanecké sněmovny   \n",
       "2  Vážené paní poslankyně a páni poslanci, připra...   \n",
       "\n",
       "                                      tokenized_text proper_nouns  length  \\\n",
       "0                                               21 .           {}       1   \n",
       "1       Návrh termínu 2 . schůze Poslanecké sněmovny           {}       6   \n",
       "2  Vážené paní poslankyně a páni poslanci , připr...           {}      19   \n",
       "\n",
       "                                         translation  \\\n",
       "0                                                21.   \n",
       "1  Draft date of the 2nd meeting of the Chamber o...   \n",
       "2  The honourable Members, prepare the House meet...   \n",
       "\n",
       "                               translation-tokenized  \\\n",
       "0                                               21 .   \n",
       "1  Draft date of the 2nd meeting of the Chamber o...   \n",
       "2  The honourable Members , prepare the House mee...   \n",
       "\n",
       "                             space-after-information  \n",
       "0                                         [No, Last]  \n",
       "1  [Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, ...  \n",
       "2  [Yes, Yes, No, Yes, Yes, Yes, Yes, Yes, Yes, Y...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = tokenize_translation(lang_code, main_path)\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tajak/Parlamint-translation/eflomal\n",
      "Number of aligned sentences: 956\n",
      "/home/tajak/Parlamint-translation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                                       894\n",
       "[(Faltynek, Faltýnek)]                    3\n",
       "[(Jerome, Jeroným)]                       3\n",
       "[(Karl, Karel)]                           3\n",
       "[(Titanic, Titanik)]                      2\n",
       "[(Krakora, Krákora)]                      2\n",
       "[(Klašek, Klaška)]                        2\n",
       "[(Foldyn, Foldyna)]                       2\n",
       "[(Mark, Marková)]                         2\n",
       "[(Zlatuska, Zlatuška)]                    2\n",
       "[(Bendlo, Bendl)]                         1\n",
       "[(Vyzul, Vyzula)]                         1\n",
       "[(forward, Julínek)]                      1\n",
       "[(Louis, Ludvík), (Hovork, Hovorka)]      1\n",
       "[(Zemanovs, Zemanovec)]                   1\n",
       "[(Petra, Petr), (Kořek, Kořenko)]         1\n",
       "[(Nykl, Igor), (Igor, Nykl)]              1\n",
       "[(Kořek, Kořenko)]                        1\n",
       "[(Kořek, Kořenek)]                        1\n",
       "[(Nekla, Nekl)]                           1\n",
       "Name: substituted_pairs, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>file</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>proper_nouns</th>\n",
       "      <th>length</th>\n",
       "      <th>translation</th>\n",
       "      <th>translation-tokenized</th>\n",
       "      <th>space-after-information</th>\n",
       "      <th>fwd_align_dict</th>\n",
       "      <th>bwd_align_dict</th>\n",
       "      <th>alignments</th>\n",
       "      <th>new_translations</th>\n",
       "      <th>substitution_info</th>\n",
       "      <th>substituted_pairs</th>\n",
       "      <th>substituted_words</th>\n",
       "      <th>errors</th>\n",
       "      <th>source_indices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>/home/tajak/Parlamint-translation/Source-data/...</td>\n",
       "      <td>ParlaMint-CZ_2013-12-10-ps2013-004-01-014-022....</td>\n",
       "      <td>ParlaMint-CZ_2013-12-10-ps2013-004-01-014-022....</td>\n",
       "      <td>Nyní bych požádal pana poslance Jeronýma Tejce...</td>\n",
       "      <td>Nyní bych požádal pana poslance Jeronýma Tejce...</td>\n",
       "      <td>{5: ['Jeronýma', 'Jeroným'], 6: ['Tejce', 'Tej...</td>\n",
       "      <td>18</td>\n",
       "      <td>I would now ask Mr Jerome Tejc as President of...</td>\n",
       "      <td>I would now ask Mr Jerome Tejc as President of...</td>\n",
       "      <td>['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Ye...</td>\n",
       "      <td>{2: '2', 3: '1', 4: '4', 5: '5', 6: '6', 7: '7...</td>\n",
       "      <td>{3: '1', 2: '2', 4: '3, 4', 5: '5', 6: '6', 7:...</td>\n",
       "      <td>{0: 2, 1: 1, 2: 3, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...</td>\n",
       "      <td>I would now ask Mr Jeroným Tejc as President o...</td>\n",
       "      <td>[(Jerome, Jeroným), No substitution: ('Tejc', ...</td>\n",
       "      <td>[(Jerome, Jeroným)]</td>\n",
       "      <td>{6: 'Jerome'}</td>\n",
       "      <td>No</td>\n",
       "      <td>[[Nyní, 1], [bych, 2], [požádal, 3], [pana, 4]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>/home/tajak/Parlamint-translation/Source-data/...</td>\n",
       "      <td>ParlaMint-CZ_2013-12-10-ps2013-004-01-014-022....</td>\n",
       "      <td>ParlaMint-CZ_2013-12-10-ps2013-004-01-014-022....</td>\n",
       "      <td>Vážený pane předsedající, vážené kolegyně a ko...</td>\n",
       "      <td>Vážený pane předsedající , vážené kolegyně a k...</td>\n",
       "      <td>{14: ['Stanislava', 'Stanislav'], 15: ['Grospi...</td>\n",
       "      <td>14</td>\n",
       "      <td>Mr President, ladies and gentlemen, I would li...</td>\n",
       "      <td>Mr President , ladies and gentlemen , I would ...</td>\n",
       "      <td>['Yes', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes'...</td>\n",
       "      <td>{1: '2', 2: '3', 3: '4', 4: '5', 5: '7', 6: '8...</td>\n",
       "      <td>{1: '1', 2: '3', 3: '4', 4: '5, 6', 5: '7', 6:...</td>\n",
       "      <td>{0: 0, 2: 1, 3: 2, 4: 3, 5: 3, 6: 4, 7: 5, 8: ...</td>\n",
       "      <td>Mr President , ladies and gentlemen , I would ...</td>\n",
       "      <td>[No substitution: ('Stanislav', 'Stanislav'), ...</td>\n",
       "      <td>[(Grospic, Grospič)]</td>\n",
       "      <td>{15: 'Grospic'}</td>\n",
       "      <td>No</td>\n",
       "      <td>[[Vážený, 1], [pane, 2], [předsedající, 3], [,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            file_path  \\\n",
       "60  /home/tajak/Parlamint-translation/Source-data/...   \n",
       "62  /home/tajak/Parlamint-translation/Source-data/...   \n",
       "\n",
       "                                                 file  \\\n",
       "60  ParlaMint-CZ_2013-12-10-ps2013-004-01-014-022....   \n",
       "62  ParlaMint-CZ_2013-12-10-ps2013-004-01-014-022....   \n",
       "\n",
       "                                          sentence_id  \\\n",
       "60  ParlaMint-CZ_2013-12-10-ps2013-004-01-014-022....   \n",
       "62  ParlaMint-CZ_2013-12-10-ps2013-004-01-014-022....   \n",
       "\n",
       "                                                 text  \\\n",
       "60  Nyní bych požádal pana poslance Jeronýma Tejce...   \n",
       "62  Vážený pane předsedající, vážené kolegyně a ko...   \n",
       "\n",
       "                                       tokenized_text  \\\n",
       "60  Nyní bych požádal pana poslance Jeronýma Tejce...   \n",
       "62  Vážený pane předsedající , vážené kolegyně a k...   \n",
       "\n",
       "                                         proper_nouns  length  \\\n",
       "60  {5: ['Jeronýma', 'Jeroným'], 6: ['Tejce', 'Tej...      18   \n",
       "62  {14: ['Stanislava', 'Stanislav'], 15: ['Grospi...      14   \n",
       "\n",
       "                                          translation  \\\n",
       "60  I would now ask Mr Jerome Tejc as President of...   \n",
       "62  Mr President, ladies and gentlemen, I would li...   \n",
       "\n",
       "                                translation-tokenized  \\\n",
       "60  I would now ask Mr Jerome Tejc as President of...   \n",
       "62  Mr President , ladies and gentlemen , I would ...   \n",
       "\n",
       "                              space-after-information  \\\n",
       "60  ['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Ye...   \n",
       "62  ['Yes', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes'...   \n",
       "\n",
       "                                       fwd_align_dict  \\\n",
       "60  {2: '2', 3: '1', 4: '4', 5: '5', 6: '6', 7: '7...   \n",
       "62  {1: '2', 2: '3', 3: '4', 4: '5', 5: '7', 6: '8...   \n",
       "\n",
       "                                       bwd_align_dict  \\\n",
       "60  {3: '1', 2: '2', 4: '3, 4', 5: '5', 6: '6', 7:...   \n",
       "62  {1: '1', 2: '3', 3: '4', 4: '5, 6', 5: '7', 6:...   \n",
       "\n",
       "                                           alignments  \\\n",
       "60  {0: 2, 1: 1, 2: 3, 3: 3, 4: 4, 5: 5, 6: 6, 7: ...   \n",
       "62  {0: 0, 2: 1, 3: 2, 4: 3, 5: 3, 6: 4, 7: 5, 8: ...   \n",
       "\n",
       "                                     new_translations  \\\n",
       "60  I would now ask Mr Jeroným Tejc as President o...   \n",
       "62  Mr President , ladies and gentlemen , I would ...   \n",
       "\n",
       "                                    substitution_info     substituted_pairs  \\\n",
       "60  [(Jerome, Jeroným), No substitution: ('Tejc', ...   [(Jerome, Jeroným)]   \n",
       "62  [No substitution: ('Stanislav', 'Stanislav'), ...  [(Grospic, Grospič)]   \n",
       "\n",
       "   substituted_words errors                                     source_indices  \n",
       "60     {6: 'Jerome'}     No  [[Nyní, 1], [bych, 2], [požádal, 3], [pana, 4]...  \n",
       "62   {15: 'Grospic'}     No  [[Vážený, 1], [pane, 2], [předsedající, 3], [,...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = correct_proper_nouns(lang_code, main_path)\n",
    "\n",
    "# See if there were any errors in word substitution\n",
    "print(df[df[\"errors\"]!=\"No\"].shape)\n",
    "\n",
    "# See example of sentences with substituted words\n",
    "df[df[\"substituted_pairs\"]!= 0][:2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linguistic processing of translated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tajak/Parlamint-translation/parlamint_env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-01-25 16:18:35 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json: 193kB [00:00, 66.0MB/s]                    \n",
      "2023-01-25 16:18:36 WARNING: Can not find mwt: default from official model list. Ignoring it.\n",
      "2023-01-25 16:18:36 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| pos       | combined |\n",
      "| lemma     | combined |\n",
      "| ner       | conll03  |\n",
      "========================\n",
      "\n",
      "2023-01-25 16:18:36 INFO: Use device: gpu\n",
      "2023-01-25 16:18:36 INFO: Loading: tokenize\n",
      "2023-01-25 16:18:36 INFO: Loading: pos\n",
      "2023-01-25 16:18:39 INFO: Loading: lemma\n",
      "2023-01-25 16:18:39 INFO: Loading: ner\n",
      "2023-01-25 16:18:40 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParlaMint-CZ_2013-11-27-ps2013-001-02-016-021.conllu processed and saved.\n",
      "Final file /home/tajak/Parlamint-translation/Final-data/ParlaMint-CZ.conllu/ParlaMint-CZ.conllu/2013/ParlaMint-CZ_2013-11-27-ps2013-001-02-016-021.conllu is saved.\n",
      "ParlaMint-CZ_2013-12-10-ps2013-004-01-014-022.conllu processed and saved.\n",
      "Final file /home/tajak/Parlamint-translation/Final-data/ParlaMint-CZ.conllu/ParlaMint-CZ.conllu/2013/ParlaMint-CZ_2013-12-10-ps2013-004-01-014-022.conllu is saved.\n",
      "ParlaMint-CZ_2013-12-06-ps2013-002-02-000-000.conllu processed and saved.\n",
      "Final file /home/tajak/Parlamint-translation/Final-data/ParlaMint-CZ.conllu/ParlaMint-CZ.conllu/2013/ParlaMint-CZ_2013-12-06-ps2013-002-02-000-000.conllu is saved.\n",
      "ParlaMint-CZ_2013-12-06-ps2013-002-02-001-007.conllu processed and saved.\n",
      "Final file /home/tajak/Parlamint-translation/Final-data/ParlaMint-CZ.conllu/ParlaMint-CZ.conllu/2013/ParlaMint-CZ_2013-12-06-ps2013-002-02-001-007.conllu is saved.\n",
      "ParlaMint-CZ_2013-11-27-ps2013-001-02-014-017.conllu processed and saved.\n",
      "Final file /home/tajak/Parlamint-translation/Final-data/ParlaMint-CZ.conllu/ParlaMint-CZ.conllu/2013/ParlaMint-CZ_2013-11-27-ps2013-001-02-014-017.conllu is saved.\n"
     ]
    }
   ],
   "source": [
    "produce_final_conllu(lang_code, main_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parlamint_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e8622831a220209edc4d2d4e58bc2159575ea2f8d9419209393154757ff5f93"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
